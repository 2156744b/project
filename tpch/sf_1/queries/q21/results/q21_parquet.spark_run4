 -- the query
insert into table q21_tmp1_par
select
  l_orderkey, count(distinct l_suppkey), max(l_suppkey) as max_suppkey
from
  lineitem_par
group by l_orderkey;

insert into table q21_tmp2_par
select
  l_orderkey, count(distinct l_suppkey), max(l_suppkey) as max_suppkey
from
  lineitem_par
where
  l_receiptdate > l_commitdate
group by l_orderkey;

insert into table q21_suppliers_who_kept_orders_waiting_par
select
  s_name, count(1) as numwait
from
  (select s_name from
(select s_name, t2.l_orderkey, l_suppkey, count_suppkey, max_suppkey 
 from q21_tmp2_par t2 right outer join
      (select s_name, l_orderkey, l_suppkey from
         (select s_name, t1.l_orderkey, l_suppkey, count_suppkey, max_suppkey
          from
            q21_tmp1_par t1 join
            (select s_name, l_orderkey, l_suppkey
             from 
               orders_par o join
               (select s_name, l_orderkey, l_suppkey
                from
                  nation_par n join supplier_par s
                  on
                    s.s_nationkey = n.n_nationkey
                    and n.n_name = 'CHINA'
                  join lineitem_par l
                  on
                    s.s_suppkey = l.l_suppkey
                where
                  l.l_receiptdate > l.l_commitdate
                ) l1 on o.o_orderkey = l1.l_orderkey and o.o_orderstatus = 'F'
             ) l2 on l2.l_orderkey = t1.l_orderkey
          ) a
          where
           (count_suppkey > 1) or ((count_suppkey=1) and (l_suppkey <> max_suppkey))
       ) l3 on l3.l_orderkey = t2.l_orderkey
    ) b
    where
     (count_suppkey is null) or ((count_suppkey=1) and (l_suppkey = max_suppkey))
  )c
group by s_name
order by numwait desc, s_name
limit 100;
15/08/16 12:50:40 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/16 12:50:40 INFO metastore: Connected to metastore.
15/08/16 12:50:41 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/16 12:50:41 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:41 INFO SparkContext: Running Spark version 1.4.1
15/08/16 12:50:41 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:41 INFO SecurityManager: Changing view acls to: hive
15/08/16 12:50:41 INFO SecurityManager: Changing modify acls to: hive
15/08/16 12:50:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/16 12:50:42 INFO Slf4jLogger: Slf4jLogger started
15/08/16 12:50:42 INFO Remoting: Starting remoting
15/08/16 12:50:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:54978]
15/08/16 12:50:43 INFO Utils: Successfully started service 'sparkDriver' on port 54978.
15/08/16 12:50:43 INFO SparkEnv: Registering MapOutputTracker
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 INFO SparkEnv: Registering BlockManagerMaster
15/08/16 12:50:43 INFO DiskBlockManager: Created local directory at /tmp/spark-3f2fd1ef-07da-4991-a44b-f8b0f5341524/blockmgr-c8900d73-f3eb-417b-93a2-5c47174a33f4
15/08/16 12:50:43 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 INFO HttpFileServer: HTTP File server directory is /tmp/spark-3f2fd1ef-07da-4991-a44b-f8b0f5341524/httpd-043a888e-a4ca-4f00-84e9-480e6c40dbba
15/08/16 12:50:43 INFO HttpServer: Starting HTTP Server
15/08/16 12:50:43 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/16 12:50:43 INFO AbstractConnector: Started SocketConnector@0.0.0.0:56371
15/08/16 12:50:43 INFO Utils: Successfully started service 'HTTP file server' on port 56371.
15/08/16 12:50:43 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/16 12:50:43 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/16 12:50:43 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/16 12:50:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/16 12:50:43 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:43 INFO Executor: Starting executor ID driver on host localhost
15/08/16 12:50:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36543.
15/08/16 12:50:43 INFO NettyBlockTransferService: Server created on 36543
15/08/16 12:50:43 INFO BlockManagerMaster: Trying to register BlockManager
15/08/16 12:50:43 INFO BlockManagerMasterEndpoint: Registering block manager localhost:36543 with 3.1 GB RAM, BlockManagerId(driver, localhost, 36543)
15/08/16 12:50:43 INFO BlockManagerMaster: Registered BlockManager
15/08/16 12:50:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:44 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/16 12:50:44 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/16 12:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/16 12:50:45 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/16 12:50:45 INFO metastore: Connected to metastore.
15/08/16 12:50:46 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/16 12:50:46 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/16 12:50:46 INFO ParseDriver: Parsing command: -- the query
insert into table q21_tmp1_par
select
  l_orderkey, count(distinct l_suppkey), max(l_suppkey) as max_suppkey
from
  lineitem_par
group by l_orderkey
15/08/16 12:50:46 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/16 12:50:49 INFO MemoryStore: ensureFreeSpace(257472) called with curMem=0, maxMem=3333968363
15/08/16 12:50:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 251.4 KB, free 3.1 GB)
15/08/16 12:50:49 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=257472, maxMem=3333968363
15/08/16 12:50:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:50:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:50:49 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/16 12:50:49 INFO Exchange: Using SparkSqlSerializer.
15/08/16 12:50:49 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:50:50 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/16 12:50:50 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/16 12:50:50 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/16 12:50:50 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/16 12:50:50 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/16 12:50:50 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/16 12:50:50 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:50:50 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:50:50 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/16 12:50:50 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:50:50 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/16 12:50:50 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/16 12:50:50 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/16 12:50:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/16 12:50:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/16 12:50:50 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:50:50 INFO MemoryStore: ensureFreeSpace(9824) called with curMem=280265, maxMem=3333968363
15/08/16 12:50:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KB, free 3.1 GB)
15/08/16 12:50:50 INFO MemoryStore: ensureFreeSpace(4891) called with curMem=290089, maxMem=3333968363
15/08/16 12:50:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KB, free 3.1 GB)
15/08/16 12:50:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:36543 (size: 4.8 KB, free: 3.1 GB)
15/08/16 12:50:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/16 12:50:50 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/16 12:50:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
15/08/16 12:50:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1761 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1764 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1761 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1760 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1760 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1763 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1763 bytes)
15/08/16 12:50:50 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1761 bytes)
15/08/16 12:50:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/16 12:50:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/16 12:50:50 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/16 12:50:50 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/16 12:50:50 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/16 12:50:50 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/16 12:50:50 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/16 12:50:50 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 26235204 length: 26235204 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 26505368 length: 26505368 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 26576747 length: 26576747 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 26485016 length: 26485016 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 26210131 length: 26210131 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 26243215 length: 26243215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 26234990 length: 26234990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 26536257 length: 26536257 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:50:51 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:00 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 9531 ms on localhost (1/8)
15/08/16 12:51:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 9533 ms on localhost (2/8)
15/08/16 12:51:00 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 9611 ms on localhost (3/8)
15/08/16 12:51:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9660 ms on localhost (4/8)
15/08/16 12:51:00 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 9643 ms on localhost (5/8)
15/08/16 12:51:00 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 9645 ms on localhost (6/8)
15/08/16 12:51:00 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 9653 ms on localhost (7/8)
15/08/16 12:51:00 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/16 12:51:00 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 9788 ms on localhost (8/8)
15/08/16 12:51:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/16 12:51:00 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 9.843 s
15/08/16 12:51:00 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:00 INFO DAGScheduler: running: Set()
15/08/16 12:51:00 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/16 12:51:00 INFO DAGScheduler: failed: Set()
15/08/16 12:51:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@318fb41f
15/08/16 12:51:00 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/16 12:51:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:51:00 INFO StatsReportListener: task runtime:(count: 8, mean: 9633.000000, stdev: 76.064118, max: 9788.000000, min: 9531.000000)
15/08/16 12:51:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:00 INFO StatsReportListener: 	9.5 s	9.5 s	9.5 s	9.6 s	9.6 s	9.7 s	9.8 s	9.8 s	9.8 s
15/08/16 12:51:00 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 6147368.750000, stdev: 92492.913717, max: 6358936.000000, min: 6087987.000000)
15/08/16 12:51:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:00 INFO StatsReportListener: 	5.8 MB	5.8 MB	5.8 MB	5.8 MB	5.8 MB	5.9 MB	6.1 MB	6.1 MB	6.1 MB
15/08/16 12:51:00 INFO StatsReportListener: task result size:(count: 8, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:00 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 98.963501, stdev: 0.390849, max: 99.274235, min: 98.311130)
15/08/16 12:51:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:00 INFO StatsReportListener: 	98 %	98 %	98 %	99 %	99 %	99 %	99 %	99 %	99 %
15/08/16 12:51:00 INFO StatsReportListener: other time pct: (count: 8, mean: 1.036499, stdev: 0.390849, max: 1.688870, min: 0.725765)
15/08/16 12:51:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:00 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 2 %
15/08/16 12:51:00 INFO MemoryStore: ensureFreeSpace(80504) called with curMem=294980, maxMem=3333968363
15/08/16 12:51:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 78.6 KB, free 3.1 GB)
15/08/16 12:51:00 INFO MemoryStore: ensureFreeSpace(30834) called with curMem=375484, maxMem=3333968363
15/08/16 12:51:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.1 KB, free 3.1 GB)
15/08/16 12:51:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:36543 (size: 30.1 KB, free: 3.1 GB)
15/08/16 12:51:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:00 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at processCmd at CliDriver.java:423)
15/08/16 12:51:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/16 12:51:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 12, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 13, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 14, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 16, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 17, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 18, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 19, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 20, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 21, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 22, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 23, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
15/08/16 12:51:00 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
15/08/16 12:51:00 INFO Executor: Running task 3.0 in stage 1.0 (TID 11)
15/08/16 12:51:00 INFO Executor: Running task 6.0 in stage 1.0 (TID 14)
15/08/16 12:51:00 INFO Executor: Running task 7.0 in stage 1.0 (TID 15)
15/08/16 12:51:00 INFO Executor: Running task 5.0 in stage 1.0 (TID 13)
15/08/16 12:51:00 INFO Executor: Running task 4.0 in stage 1.0 (TID 12)
15/08/16 12:51:00 INFO Executor: Running task 2.0 in stage 1.0 (TID 10)
15/08/16 12:51:00 INFO Executor: Running task 8.0 in stage 1.0 (TID 16)
15/08/16 12:51:00 INFO Executor: Running task 10.0 in stage 1.0 (TID 18)
15/08/16 12:51:00 INFO Executor: Running task 11.0 in stage 1.0 (TID 19)
15/08/16 12:51:00 INFO Executor: Running task 14.0 in stage 1.0 (TID 22)
15/08/16 12:51:00 INFO Executor: Running task 15.0 in stage 1.0 (TID 23)
15/08/16 12:51:00 INFO Executor: Running task 9.0 in stage 1.0 (TID 17)
15/08/16 12:51:00 INFO Executor: Running task 13.0 in stage 1.0 (TID 21)
15/08/16 12:51:00 INFO Executor: Running task 12.0 in stage 1.0 (TID 20)
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
16-Aug-2015 12:50:47 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
16-Aug-2015 12:50:47 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
16-Aug-2015 12:50:47 INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
16-Aug-2015 12:50:47 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749107 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749096 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749050 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749056 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 755812 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 751036 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 748901 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749157 records.
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 749050
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 748901
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 749107
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 749157
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 751036
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 77 ms. row count = 755812
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 749096
16-Aug-2015 12:50:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 77 ms. row count = 749056
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
16-Aug-2015 12:51:01 INFO: parquet.ha15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:01 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:01 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:01 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,154
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,106
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,162
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,998
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,190
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,182
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,970
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,286
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,270
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,106
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,138
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,138
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,862
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,138
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,306
15/08/16 12:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,122
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,242 entries, 16,968B raw, 4,242B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,008B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,970B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,025B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,036B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,998B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,234 entries, 16,936B raw, 4,234B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,011B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,973B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,035B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,997B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,041B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,003B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,279 entries, 17,116B raw, 4,279B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,023B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,985B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,248 entries, 16,992B raw, 4,248B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,284 entries, 17,136B raw, 4,284B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,016B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,978B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 21,964B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,926B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,867B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,252B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,214B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,238 entries, 16,952B raw, 4,238B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,200 entries, 16,800B raw, 4,200B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,234 entries, 16,936B raw, 4,234B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,025B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 22,001B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,963B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,207 entries, 16,828B raw, 4,207B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,234B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,196B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,242 entries, 16,968B raw, 4,242B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,275 entries, 17,100B raw, 4,275B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,251B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,255 entries, 17,020B raw, 4,255B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,246 entries, 16,984B raw, 4,246B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 21,974B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,936B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,173 entries, 16,692B raw, 4,173B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,242 entries, 16,968B raw, 4,242B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:02 INFO ColumnChunkPageWriteStore: written 12,251B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,253 entries, 17,012B raw, 4,253B comp}
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000002
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000004
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000007
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000005
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000013
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000012
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000004_0: Committed
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000007_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000000
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000000_0: Committed
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000005_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000006
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000003
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000003_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000011
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000011_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000015
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000014
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000008
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000008_0: Committed
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000012_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000009
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000009_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000010
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000010_0: Committed
15/08/16 12:51:02 INFO Executor: Finished task 3.0 in stage 1.0 (TID 11). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 8.0 in stage 1.0 (TID 16). 843 bytes result sent to driver
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000002_0: Committed
15/08/16 12:51:02 INFO Executor: Finished task 9.0 in stage 1.0 (TID 17). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 10.0 in stage 1.0 (TID 18). 843 bytes result sent to driver
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000013_0: Committed
15/08/16 12:51:02 INFO Executor: Finished task 7.0 in stage 1.0 (TID 15). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 12.0 in stage 1.0 (TID 20). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 4.0 in stage 1.0 (TID 12). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 843 bytes result sent to driver
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000014_0: Committed
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000015_0: Committed
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000006_0: Committed
15/08/16 12:51:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000001
15/08/16 12:51:02 INFO Executor: Finished task 5.0 in stage 1.0 (TID 13). 843 bytes result sent to driver
15/08/16 12:51:02 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000001_0: Committed
15/08/16 12:51:02 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 24, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Finished task 11.0 in stage 1.0 (TID 19). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 2.0 in stage 1.0 (TID 10). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Running task 16.0 in stage 1.0 (TID 24)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 25, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Finished task 14.0 in stage 1.0 (TID 22). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Running task 17.0 in stage 1.0 (TID 25)
15/08/16 12:51:02 INFO Executor: Finished task 6.0 in stage 1.0 (TID 14). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Finished task 1.0 in stage 1.0 (TID 9). 843 bytes result sent to driver
15/08/16 12:51:02 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 26, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Finished task 13.0 in stage 1.0 (TID 21). 843 bytes result sent to driver
15/08/16 12:51:02 INFO Executor: Running task 18.0 in stage 1.0 (TID 26)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 27, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 19.0 in stage 1.0 (TID 27)
15/08/16 12:51:02 INFO Executor: Finished task 15.0 in stage 1.0 (TID 23). 843 bytes result sent to driver
15/08/16 12:51:02 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 28, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 20.0 in stage 1.0 (TID 28)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 29, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 21.0 in stage 1.0 (TID 29)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 30, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 22.0 in stage 1.0 (TID 30)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 31, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 23.0 in stage 1.0 (TID 31)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 11) in 1791 ms on localhost (1/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 32, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 24.0 in stage 1.0 (TID 32)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 33, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 25.0 in stage 1.0 (TID 33)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 16) in 1788 ms on localhost (2/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 18) in 1788 ms on localhost (3/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 17) in 1789 ms on localhost (4/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 34, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 20) in 1788 ms on localhost (5/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 1792 ms on localhost (6/200)
15/08/16 12:51:02 INFO Executor: Running task 26.0 in stage 1.0 (TID 34)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 12) in 1796 ms on localhost (7/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 1802 ms on localhost (8/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 35, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 27.0 in stage 1.0 (TID 35)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 19) in 1792 ms on localhost (9/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 13) in 1798 ms on localhost (10/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 10) in 1801 ms on localhost (11/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 36, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 28.0 in stage 1.0 (TID 36)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 22) in 1791 ms on localhost (12/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 37, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 29.0 in stage 1.0 (TID 37)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 14) in 1802 ms on localhost (13/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 38, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 30.0 in stage 1.0 (TID 38)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 1809 ms on localhost (14/200)
15/08/16 12:51:02 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 39, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:02 INFO Executor: Running task 31.0 in stage 1.0 (TID 39)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 21) in 1801 ms on localhost (15/200)
15/08/16 12:51:02 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 23) in 1802 ms on localhost (16/200)
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,026
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,226
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,954
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,322
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,366
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,858
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,018
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,290
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,182
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,022B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,984B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 21,998B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,960B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,882B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,264 entries, 17,056B raw, 4,264B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,214 entries, 16,856B raw, 4,214B comp}
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,043B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,005B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,869B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,158
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,288 entries, 17,152B raw, 4,288B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 21,962B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,924B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,040B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,002B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,237B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,196 entries, 16,784B raw, 4,196B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,299 entries, 17,196B raw, 4,299B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,051B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 21,969B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,931B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,233B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,195B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,172 entries, 16,688B raw, 4,172B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,882B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,280 entries, 17,120B raw, 4,280B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,253 entries, 17,012B raw, 4,253B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,212 entries, 16,848B raw, 4,212B comp}
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,030
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,222
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,247 entries, 16,988B raw, 4,247B comp}
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,263 entries, 17,052B raw, 4,263B comp}
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,342
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,006
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 21,977B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,939B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,215 entries, 16,860B raw, 4,215B comp}
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000023
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000023_0: Committed
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,008B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,970B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000027
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000027_0: Committed
15/08/16 12:51:03 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,209 entries, 16,836B raw, 4,209B comp}
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:03 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:03 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,005B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,967B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,293 entries, 17,172B raw, 4,293B comp}
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000018
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000018_0: Committed
15/08/16 12:51:03 INFO Executor: Finished task 27.0 in stage 1.0 (TID 35). 843 bytes result sent to driver
15/08/16 12:51:03 INFO Executor: Finished task 23.0 in stage 1.0 (TID 31). 843 bytes result sent to driver
15/08/16 12:51:03 INFO Executor: Finished task 18.0 in stage 1.0 (TID 26). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 40, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 32.0 in stage 1.0 (TID 40)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 41, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 33.0 in stage 1.0 (TID 41)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 42, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000016
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000016_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 35) in 1019 ms on localhost (17/200)
15/08/16 12:51:03 INFO Executor: Running task 34.0 in stage 1.0 (TID 42)
15/08/16 12:51:03 INFO Executor: Finished task 16.0 in stage 1.0 (TID 24). 843 bytes result sent to driver
15/08/16 12:51:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:03 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 26) in 1039 ms on localhost (18/200)
15/08/16 12:51:03 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 31) in 1031 ms on localhost (19/200)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 43, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,022
15/08/16 12:51:03 INFO Executor: Running task 35.0 in stage 1.0 (TID 43)
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000017
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000017_0: Committed
15/08/16 12:51:03 INFO Executor: Finished task 17.0 in stage 1.0 (TID 25). 843 bytes result sent to driver
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000026
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000026_0: Committed
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000020
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000020_0: Committed
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000022
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000022_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 44, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 36.0 in stage 1.0 (TID 44)
15/08/16 12:51:03 INFO Executor: Finished task 20.0 in stage 1.0 (TID 28). 843 bytes result sent to driver
15/08/16 12:51:03 INFO Executor: Finished task 26.0 in stage 1.0 (TID 34). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 24) in 1051 ms on localhost (20/200)
15/08/16 12:51:03 INFO Executor: Finished task 22.0 in stage 1.0 (TID 30). 843 bytes result sent to driver
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000028
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000028_0: Committed
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000025
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000025_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 25) in 1050 ms on localhost (21/200)
15/08/16 12:51:03 INFO Executor: Finished task 28.0 in stage 1.0 (TID 36). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 28) in 1048 ms on localhost (22/200)
15/08/16 12:51:03 INFO Executor: Finished task 25.0 in stage 1.0 (TID 33). 843 bytes result sent to driver
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000024
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000024_0: Committed
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,021B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 45, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 37.0 in stage 1.0 (TID 45)
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 46, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Finished task 24.0 in stage 1.0 (TID 32). 843 bytes result sent to driver
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,213 entries, 16,852B raw, 4,213B comp}
15/08/16 12:51:03 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 34) in 1041 ms on localhost (23/200)
15/08/16 12:51:03 INFO Executor: Running task 38.0 in stage 1.0 (TID 46)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 47, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 39.0 in stage 1.0 (TID 47)
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000019
15/08/16 12:51:03 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 48, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000019_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 36) in 1042 ms on localhost (24/200)
15/08/16 12:51:03 INFO Executor: Finished task 19.0 in stage 1.0 (TID 27). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 30) in 1055 ms on localhost (25/200)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 49, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 41.0 in stage 1.0 (TID 49)
15/08/16 12:51:03 INFO Executor: Running task 40.0 in stage 1.0 (TID 48)
15/08/16 12:51:03 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 33) in 1054 ms on localhost (26/200)
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000031
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000031_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 32) in 1057 ms on localhost (27/200)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 50, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 42.0 in stage 1.0 (TID 50)
15/08/16 12:51:03 INFO Executor: Finished task 31.0 in stage 1.0 (TID 39). 843 bytes result sent to driver
15/08/16 12:51:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,174
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000029
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000029_0: Committed
15/08/16 12:51:03 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 27) in 1072 ms on localhost (28/200)
15/08/16 12:51:03 INFO Executor: Finished task 29.0 in stage 1.0 (TID 37). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 51, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 43.0 in stage 1.0 (TID 51)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 52, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 44.0 in stage 1.0 (TID 52)
15/08/16 12:51:03 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 53, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 45.0 in stage 1.0 (TID 53)
15/08/16 12:51:03 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 39) in 1053 ms on localhost (29/200)
15/08/16 12:51:03 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 37) in 1057 ms on localhost (30/200)
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 22,045B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,007B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:03 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,251 entries, 17,004B raw, 4,251B comp}
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000030
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000030_0: Committed
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO Executor: Finished task 30.0 in stage 1.0 (TID 38). 843 bytes result sent to driver
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/16 12:51:03 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 54, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 46.0 in stage 1.0 (TID 54)
15/08/16 12:51:03 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 38) in 1278 ms on localhost (31/200)
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000021
15/08/16 12:51:03 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000021_0: Committed
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:03 INFO Executor: Finished task 21.0 in stage 1.0 (TID 29). 843 bytes result sent to driver
15/08/16 12:51:03 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 55, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:03 INFO Executor: Running task 47.0 in stage 1.0 (TID 55)
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:03 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 29) in 1311 ms on localhost (32/200)
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,006
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,114
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,053B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,015B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,110
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,229B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,191B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,236 entries, 16,944B raw, 4,236B comp}
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 21,961B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,923B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,110
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,209 entries, 16,836B raw, 4,209B comp}
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,170
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,934
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,031B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,993B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 21,982B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,944B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,038
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,118
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,235 entries, 16,940B raw, 4,235B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,239B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,201B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,235 entries, 16,940B raw, 4,235B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,018B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,250 entries, 17,000B raw, 4,250B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,015B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,977B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,016B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,978B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,217 entries, 16,868B raw, 4,217B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,191 entries, 16,764B raw, 4,191B comp}
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000032
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,234
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,986
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000032_0: Committed
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO Executor: Finished task 32.0 in stage 1.0 (TID 40). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,237 entries, 16,948B raw, 4,237B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,038B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,000B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 56, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 48.0 in stage 1.0 (TID 56)
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,266 entries, 17,064B raw, 4,266B comp}
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000033
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000033_0: Committed
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000038
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000038_0: Committed
15/08/16 12:51:04 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 40) in 920 ms on localhost (33/200)
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,010B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,972B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO Executor: Finished task 33.0 in stage 1.0 (TID 41). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,204 entries, 16,816B raw, 4,204B comp}
15/08/16 12:51:04 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 57, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,034
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000041
15/08/16 12:51:04 INFO Executor: Finished task 38.0 in stage 1.0 (TID 46). 843 bytes result sent to driver
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000041_0: Committed
15/08/16 12:51:04 INFO Executor: Running task 49.0 in stage 1.0 (TID 57)
15/08/16 12:51:04 INFO Executor: Finished task 41.0 in stage 1.0 (TID 49). 843 bytes result sent to driver
15/08/16 12:51:04 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 58, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 50.0 in stage 1.0 (TID 58)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 41) in 1089 ms on localhost (34/200)
15/08/16 12:51:04 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 59, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,098
15/08/16 12:51:04 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 46) in 1074 ms on localhost (35/200)
15/08/16 12:51:04 INFO Executor: Running task 51.0 in stage 1.0 (TID 59)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 49) in 1064 ms on localhost (36/200)
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,216 entries, 16,864B raw, 4,216B comp}
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,026
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,030
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000037
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000037_0: Committed
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,044B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO Executor: Finished task 37.0 in stage 1.0 (TID 45). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,232 entries, 16,928B raw, 4,232B comp}
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000034
15/08/16 12:51:04 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 60, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000042
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000034_0: Committed
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,017B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,979B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000042_0: Committed
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000036
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000036_0: Committed
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,019B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,981B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 45) in 1093 ms on localhost (37/200)
15/08/16 12:51:04 INFO Executor: Finished task 34.0 in stage 1.0 (TID 42). 843 bytes result sent to driver
15/08/16 12:51:04 INFO Executor: Running task 52.0 in stage 1.0 (TID 60)
15/08/16 12:51:04 INFO Executor: Finished task 42.0 in stage 1.0 (TID 50). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO Executor: Finished task 36.0 in stage 1.0 (TID 44). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 61, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,228B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,190B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,215 entries, 16,860B raw, 4,215B comp}
15/08/16 12:51:04 INFO Executor: Running task 53.0 in stage 1.0 (TID 61)
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,214 entries, 16,856B raw, 4,214B comp}
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000039
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000039_0: Committed
15/08/16 12:51:04 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 62, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 54.0 in stage 1.0 (TID 62)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 42) in 1115 ms on localhost (38/200)
15/08/16 12:51:04 INFO Executor: Finished task 39.0 in stage 1.0 (TID 47). 843 bytes result sent to driver
15/08/16 12:51:04 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 50) in 1081 ms on localhost (39/200)
15/08/16 12:51:04 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 63, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 55.0 in stage 1.0 (TID 63)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 44) in 1104 ms on localhost (40/200)
15/08/16 12:51:04 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 64, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 56.0 in stage 1.0 (TID 64)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 47) in 1096 ms on localhost (41/200)
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000045
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000044
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000044_0: Committed
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000045_0: Committed
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO Executor: Finished task 45.0 in stage 1.0 (TID 53). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 65, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 57.0 in stage 1.0 (TID 65)
15/08/16 12:51:04 INFO Executor: Finished task 44.0 in stage 1.0 (TID 52). 843 bytes result sent to driver
15/08/16 12:51:04 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 66, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 58.0 in stage 1.0 (TID 66)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 53) in 1106 ms on localhost (42/200)
15/08/16 12:51:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000035
15/08/16 12:51:04 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000035_0: Committed
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:04 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 52) in 1110 ms on localhost (43/200)
15/08/16 12:51:04 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000047
15/08/16 12:51:04 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000047_0: Committed
15/08/16 12:51:04 INFO Executor: Finished task 35.0 in stage 1.0 (TID 43). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO Executor: Finished task 47.0 in stage 1.0 (TID 55). 843 bytes result sent to driver
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 67, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 68, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO Executor: Running task 60.0 in stage 1.0 (TID 68)
15/08/16 12:51:04 INFO Executor: Running task 59.0 in stage 1.0 (TID 67)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 43) in 1149 ms on localhost (44/200)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 55) in 877 ms on localhost (45/200)
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,030
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,018B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,215 entries, 16,860B raw, 4,215B comp}
15/08/16 12:51:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,054
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 22,060B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,022B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:04 INFO ColumnChunkPageWriteStore: written 12,235B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,197B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,221 entries, 16,884B raw, 4,221B comp}
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000043
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000043_0: Committed
15/08/16 12:51:04 INFO Executor: Finished task 43.0 in stage 1.0 (TID 51). 843 bytes result sent to driver
15/08/16 12:51:04 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 69, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 61.0 in stage 1.0 (TID 69)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 51) in 1176 ms on localhost (46/200)
15/08/16 12:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000040
15/08/16 12:51:04 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000040_0: Committed
15/08/16 12:51:04 INFO Executor: Finished task 40.0 in stage 1.0 (TID 48). 843 bytes result sent to driver
15/08/16 12:51:04 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 70, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:04 INFO Executor: Running task 62.0 in stage 1.0 (TID 70)
15/08/16 12:51:04 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 48) in 1200 ms on localhost (47/200)
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000046
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000046_0: Committed
15/08/16 12:51:05 INFO Executor: Finished task 46.0 in stage 1.0 (TID 54). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 71, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 63.0 in stage 1.0 (TID 71)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 54) in 1432 ms on localhost (48/200)
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,098
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,043B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,005B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,869B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,232 entries, 16,928B raw, 4,232B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,050
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,990
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,007B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,969B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,220 entries, 16,880B raw, 4,220B comp}
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,016B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,978B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,214
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,090
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,205 entries, 16,820B raw, 4,205B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000048
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,007B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,969B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000048_0: Committed
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,038
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,230 entries, 16,920B raw, 4,230B comp}
15/08/16 12:51:05 INFO Executor: Finished task 48.0 in stage 1.0 (TID 56). 843 bytes result sent to driver
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,028B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,990B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,256B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,218B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,261 entries, 17,044B raw, 4,261B comp}
15/08/16 12:51:05 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 72, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO Executor: Running task 64.0 in stage 1.0 (TID 72)
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,098
15/08/16 12:51:05 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 56) in 1070 ms on localhost (49/200)
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,014
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,030B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,992B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,217 entries, 16,868B raw, 4,217B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 21,990B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,952B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,232B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,194B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,232 entries, 16,928B raw, 4,232B comp}
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000049
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,050
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000049_0: Committed
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,966
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,090B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,052B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO Executor: Finished task 49.0 in stage 1.0 (TID 57). 843 bytes result sent to driver
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,256B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,218B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,211 entries, 16,844B raw, 4,211B comp}
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,014
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000053
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000053_0: Committed
15/08/16 12:51:05 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 73, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 65.0 in stage 1.0 (TID 73)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 57) in 1088 ms on localhost (50/200)
15/08/16 12:51:05 INFO Executor: Finished task 53.0 in stage 1.0 (TID 61). 843 bytes result sent to driver
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,166
15/08/16 12:51:05 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 74, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 66.0 in stage 1.0 (TID 74)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,199 entries, 16,796B raw, 4,199B comp}
15/08/16 12:51:05 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 61) in 899 ms on localhost (51/200)
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000051
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000051_0: Committed
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO Executor: Finished task 51.0 in stage 1.0 (TID 59). 843 bytes result sent to driver
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,220 entries, 16,880B raw, 4,220B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 75, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO Executor: Running task 67.0 in stage 1.0 (TID 75)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 59) in 925 ms on localhost (52/200)
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000056
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000056_0: Committed
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,211 entries, 16,844B raw, 4,211B comp}
15/08/16 12:51:05 INFO Executor: Finished task 56.0 in stage 1.0 (TID 64). 843 bytes result sent to driver
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000057
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000057_0: Committed
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO Executor: Finished task 57.0 in stage 1.0 (TID 65). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 76, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 68.0 in stage 1.0 (TID 76)
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,174
15/08/16 12:51:05 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 77, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:05 INFO Executor: Running task 69.0 in stage 1.0 (TID 77)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 64) in 909 ms on localhost (53/200)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,249 entries, 16,996B raw, 4,249B comp}
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000060
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000060_0: Committed
15/08/16 12:51:05 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 65) in 891 ms on localhost (54/200)
15/08/16 12:51:05 INFO Executor: Finished task 60.0 in stage 1.0 (TID 68). 843 bytes result sent to driver
15/08/16 12:51:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:05 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 78, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 70.0 in stage 1.0 (TID 78)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 68) in 880 ms on localhost (55/200)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,251 entries, 17,004B raw, 4,251B comp}
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000050
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000050_0: Committed
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000054
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000054_0: Committed
15/08/16 12:51:05 INFO Executor: Finished task 50.0 in stage 1.0 (TID 58). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 79, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 71.0 in stage 1.0 (TID 79)
15/08/16 12:51:05 INFO Executor: Finished task 54.0 in stage 1.0 (TID 62). 843 bytes result sent to driver
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 80, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO Executor: Running task 72.0 in stage 1.0 (TID 80)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 58) in 1127 ms on localhost (56/200)
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000059
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000059_0: Committed
15/08/16 12:51:05 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 62) in 937 ms on localhost (57/200)
15/08/16 12:51:05 INFO Executor: Finished task 59.0 in stage 1.0 (TID 67). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 81, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,018
15/08/16 12:51:05 INFO Executor: Running task 73.0 in stage 1.0 (TID 81)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 67) in 900 ms on localhost (58/200)
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000055
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000055_0: Committed
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,110
15/08/16 12:51:05 INFO Executor: Finished task 55.0 in stage 1.0 (TID 63). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 82, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 74.0 in stage 1.0 (TID 82)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,071B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,033B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 63) in 942 ms on localhost (59/200)
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,212 entries, 16,848B raw, 4,212B comp}
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 22,041B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,003B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 2,867B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:05 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,235 entries, 16,940B raw, 4,235B comp}
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000058
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000058_0: Committed
15/08/16 12:51:05 INFO Executor: Finished task 58.0 in stage 1.0 (TID 66). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 83, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 75.0 in stage 1.0 (TID 83)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:05 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 66) in 926 ms on localhost (60/200)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000061
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000061_0: Committed
15/08/16 12:51:05 INFO Executor: Finished task 61.0 in stage 1.0 (TID 69). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 84, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 76.0 in stage 1.0 (TID 84)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 69) in 869 ms on localhost (61/200)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000062
15/08/16 12:51:05 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000062_0: Committed
15/08/16 12:51:05 INFO Executor: Finished task 62.0 in stage 1.0 (TID 70). 843 bytes result sent to driver
15/08/16 12:51:05 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 85, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:05 INFO Executor: Running task 77.0 in stage 1.0 (TID 85)
15/08/16 12:51:05 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 70) in 891 ms on localhost (62/200)
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:05 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:05 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:05 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000052
15/08/16 12:51:06 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000052_0: Committed
15/08/16 12:51:06 INFO Executor: Finished task 52.0 in stage 1.0 (TID 60). 843 bytes result sent to driver
15/08/16 12:51:06 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 86, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:06 INFO Executor: Running task 78.0 in stage 1.0 (TID 86)
15/08/16 12:51:06 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 60) in 2239 ms on localhost (63/200)
15/08/16 12:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,258
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 12,235B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,197B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,272 entries, 17,088B raw, 4,272B comp}
15/08/16 12:51:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:36543 in memory (size: 4.8 KB, free: 3.1 GB)
15/08/16 12:51:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000063
15/08/16 12:51:06 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000063_0: Committed
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO Executor: Finished task 63.0 in stage 1.0 (TID 71). 843 bytes result sent to driver
15/08/16 12:51:06 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 87, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:06 INFO Executor: Running task 79.0 in stage 1.0 (TID 87)
15/08/16 12:51:06 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 71) in 1741 ms on localhost (64/200)
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,042
15/08/16 12:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,958
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 21,996B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,958B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 21,999B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,961B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 12,233B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,195B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,197 entries, 16,788B raw, 4,197B comp}
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,218 entries, 16,872B raw, 4,218B comp}
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:06 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:06 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,018
15/08/16 12:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,950
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,021B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,212 entries, 16,848B raw, 4,212B comp}
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,007B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,969B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,237B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,195 entries, 16,780B raw, 4,195B comp}
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,106
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,986
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,146
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,990
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000065
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000065_0: Committed
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000064
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000064_0: Committed
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,039B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,094
15/08/16 12:51:07 INFO Executor: Finished task 64.0 in stage 1.0 (TID 72). 843 bytes result sent to driver
15/08/16 12:51:07 INFO Executor: Finished task 65.0 in stage 1.0 (TID 73). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,237B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,234 entries, 16,936B raw, 4,234B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 21,970B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,932B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,882B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 88, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,232B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,194B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,204 entries, 16,816B raw, 4,204B comp}
15/08/16 12:51:07 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 89, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,218
15/08/16 12:51:07 INFO Executor: Running task 80.0 in stage 1.0 (TID 88)
15/08/16 12:51:07 INFO Executor: Running task 81.0 in stage 1.0 (TID 89)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 72) in 1817 ms on localhost (65/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,069B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,031B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 73) in 1582 ms on localhost (66/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,231 entries, 16,924B raw, 4,231B comp}
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,012B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,974B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,065B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,027B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,874
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,244 entries, 16,976B raw, 4,244B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,205 entries, 16,820B raw, 4,205B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,262 entries, 17,048B raw, 4,262B comp}
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000068
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000068_0: Committed
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,118
15/08/16 12:51:07 INFO Executor: Finished task 68.0 in stage 1.0 (TID 76). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,010B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,972B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 90, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,176 entries, 16,704B raw, 4,176B comp}
15/08/16 12:51:07 INFO Executor: Running task 82.0 in stage 1.0 (TID 90)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 76) in 1587 ms on localhost (67/200)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000072
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000072_0: Committed
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO Executor: Finished task 72.0 in stage 1.0 (TID 80). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 91, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 83.0 in stage 1.0 (TID 91)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000071
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000071_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 80) in 1570 ms on localhost (68/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,024B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,986B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO Executor: Finished task 71.0 in stage 1.0 (TID 79). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000067
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000067_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 92, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,237 entries, 16,948B raw, 4,237B comp}
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000073
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000073_0: Committed
15/08/16 12:51:07 INFO Executor: Running task 84.0 in stage 1.0 (TID 92)
15/08/16 12:51:07 INFO Executor: Finished task 67.0 in stage 1.0 (TID 75). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000069
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000069_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 93, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000066
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000066_0: Committed
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000070
15/08/16 12:51:07 INFO Executor: Running task 85.0 in stage 1.0 (TID 93)
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000070_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 79) in 1577 ms on localhost (69/200)
15/08/16 12:51:07 INFO Executor: Finished task 69.0 in stage 1.0 (TID 77). 843 bytes result sent to driver
15/08/16 12:51:07 INFO Executor: Finished task 66.0 in stage 1.0 (TID 74). 843 bytes result sent to driver
15/08/16 12:51:07 INFO Executor: Finished task 70.0 in stage 1.0 (TID 78). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 75) in 1609 ms on localhost (70/200)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 94, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO Executor: Finished task 73.0 in stage 1.0 (TID 81). 843 bytes result sent to driver
15/08/16 12:51:07 INFO Executor: Running task 86.0 in stage 1.0 (TID 94)
15/08/16 12:51:07 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 95, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 96, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 88.0 in stage 1.0 (TID 96)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 77) in 1602 ms on localhost (71/200)
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,326
15/08/16 12:51:07 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 74) in 1620 ms on localhost (72/200)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 78) in 1597 ms on localhost (73/200)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 81) in 1579 ms on localhost (74/200)
15/08/16 12:51:07 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 97, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 89.0 in stage 1.0 (TID 97)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO Executor: Running task 87.0 in stage 1.0 (TID 95)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000076
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000076_0: Committed
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,049B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,011B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,289 entries, 17,156B raw, 4,289B comp}
15/08/16 12:51:07 INFO Executor: Finished task 76.0 in stage 1.0 (TID 84). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 98, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 90.0 in stage 1.0 (TID 98)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 84) in 1571 ms on localhost (75/200)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000074
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000074_0: Committed
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO Executor: Finished task 74.0 in stage 1.0 (TID 82). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 99, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 91.0 in stage 1.0 (TID 99)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 82) in 1608 ms on localhost (76/200)
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,246
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,269 entries, 17,076B raw, 4,269B comp}
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000077
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000077_0: Committed
15/08/16 12:51:07 INFO Executor: Finished task 77.0 in stage 1.0 (TID 85). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:07 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO Executor: Running task 92.0 in stage 1.0 (TID 100)
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 85) in 1589 ms on localhost (77/200)
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000075
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000075_0: Committed
15/08/16 12:51:07 INFO Executor: Finished task 75.0 in stage 1.0 (TID 83). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 93.0 in stage 1.0 (TID 101)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 83) in 1651 ms on localhost (78/200)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,314
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,022B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,984B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,286 entries, 17,144B raw, 4,286B comp}
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000078
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000078_0: Committed
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO Executor: Finished task 78.0 in stage 1.0 (TID 86). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 94.0 in stage 1.0 (TID 102)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 86) in 629 ms on localhost (79/200)
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,022
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,024B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,986B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,213 entries, 16,852B raw, 4,213B comp}
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000079
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000079_0: Committed
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,146
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO Executor: Finished task 79.0 in stage 1.0 (TID 87). 843 bytes result sent to driver
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO Executor: Running task 95.0 in stage 1.0 (TID 103)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 87) in 691 ms on localhost (80/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,057B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,019B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,244 entries, 16,976B raw, 4,244B comp}
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,234
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,098
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,070
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,286
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,058
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 21,997B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,959B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,266 entries, 17,064B raw, 4,266B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 21,990B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,952B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 21,952B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,914B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,869B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,231B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,193B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,225 entries, 16,900B raw, 4,225B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,043B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,005B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,279 entries, 17,116B raw, 4,279B comp}
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,818
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,232 entries, 16,928B raw, 4,232B comp}
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,009B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,971B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000088
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000088_0: Committed
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,866B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,830B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,222 entries, 16,888B raw, 4,222B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,162 entries, 16,648B raw, 4,162B comp}
15/08/16 12:51:07 INFO Executor: Finished task 88.0 in stage 1.0 (TID 96). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 96.0 in stage 1.0 (TID 104)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 96) in 704 ms on localhost (81/200)
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,970
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,898
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,090
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000080
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,118
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000080_0: Committed
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000087
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000087_0: Committed
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,011B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,973B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,200 entries, 16,800B raw, 4,200B comp}
15/08/16 12:51:07 INFO Executor: Finished task 80.0 in stage 1.0 (TID 88). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,251B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,182 entries, 16,728B raw, 4,182B comp}
15/08/16 12:51:07 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 97.0 in stage 1.0 (TID 105)
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000083
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO Executor: Finished task 87.0 in stage 1.0 (TID 95). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000083_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 98.0 in stage 1.0 (TID 106)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 88) in 768 ms on localhost (82/200)
15/08/16 12:51:07 INFO Executor: Finished task 83.0 in stage 1.0 (TID 91). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,020B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,982B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 95) in 724 ms on localhost (83/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,252B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,214B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,230 entries, 16,920B raw, 4,230B comp}
15/08/16 12:51:07 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 99.0 in stage 1.0 (TID 107)
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000081
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000081_0: Committed
15/08/16 12:51:07 INFO Executor: Finished task 81.0 in stage 1.0 (TID 89). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 91) in 739 ms on localhost (84/200)
15/08/16 12:51:07 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 100.0 in stage 1.0 (TID 108)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 21,962B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,924B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,866B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,830B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,237 entries, 16,948B raw, 4,237B comp}
15/08/16 12:51:07 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 89) in 778 ms on localhost (85/200)
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000084
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000084_0: Committed
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000082
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000082_0: Committed
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO Executor: Finished task 84.0 in stage 1.0 (TID 92). 843 bytes result sent to driver
15/08/16 12:51:07 INFO Executor: Finished task 82.0 in stage 1.0 (TID 90). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 101.0 in stage 1.0 (TID 109)
15/08/16 12:51:07 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 102.0 in stage 1.0 (TID 110)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 92) in 757 ms on localhost (86/200)
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,390
15/08/16 12:51:07 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 90) in 773 ms on localhost (87/200)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000091
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000091_0: Committed
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,007B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,969B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,974
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,230B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,192B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,305 entries, 17,220B raw, 4,305B comp}
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000085
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000085_0: Committed
15/08/16 12:51:07 INFO Executor: Finished task 91.0 in stage 1.0 (TID 99). 843 bytes result sent to driver
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000086
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000086_0: Committed
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Finished task 86.0 in stage 1.0 (TID 94). 843 bytes result sent to driver
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 99) in 743 ms on localhost (88/200)
15/08/16 12:51:07 INFO Executor: Running task 104.0 in stage 1.0 (TID 112)
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO Executor: Running task 103.0 in stage 1.0 (TID 111)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 94) in 783 ms on localhost (89/200)
15/08/16 12:51:07 INFO Executor: Finished task 85.0 in stage 1.0 (TID 93). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 105.0 in stage 1.0 (TID 113)
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 93) in 790 ms on localhost (90/200)
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000089
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000089_0: Committed
15/08/16 12:51:07 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,201 entries, 16,804B raw, 4,201B comp}
15/08/16 12:51:07 INFO Executor: Finished task 89.0 in stage 1.0 (TID 97). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 106.0 in stage 1.0 (TID 114)
15/08/16 12:51:07 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 97) in 789 ms on localhost (91/200)
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000092
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000092_0: Committed
15/08/16 12:51:07 INFO Executor: Finished task 92.0 in stage 1.0 (TID 100). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 107.0 in stage 1.0 (TID 115)
15/08/16 12:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000090
15/08/16 12:51:07 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000090_0: Committed
15/08/16 12:51:07 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 100) in 754 ms on localhost (92/200)
15/08/16 12:51:07 INFO Executor: Finished task 90.0 in stage 1.0 (TID 98). 843 bytes result sent to driver
15/08/16 12:51:07 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:07 INFO Executor: Running task 108.0 in stage 1.0 (TID 116)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:07 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 98) in 807 ms on localhost (93/200)
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:07 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:07 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:07 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,154
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,254
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 21,999B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,961B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,271 entries, 17,084B raw, 4,271B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,251B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,246 entries, 16,984B raw, 4,246B comp}
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000094
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000094_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 94.0 in stage 1.0 (TID 102). 843 bytes result sent to driver
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000093
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000093_0: Committed
15/08/16 12:51:08 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Finished task 93.0 in stage 1.0 (TID 101). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 110.0 in stage 1.0 (TID 118)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 102) in 802 ms on localhost (94/200)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 101) in 1052 ms on localhost (95/200)
15/08/16 12:51:08 INFO Executor: Running task 109.0 in stage 1.0 (TID 117)
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,254
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 21,998B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,960B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,231B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,193B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,271 entries, 17,084B raw, 4,271B comp}
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,138
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 21,976B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,938B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,883B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,847B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,242 entries, 16,968B raw, 4,242B comp}
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,294
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,039B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,235B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,197B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,281 entries, 17,124B raw, 4,281B comp}
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,862
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000097
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000097_0: Committed
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,902
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO Executor: Finished task 97.0 in stage 1.0 (TID 105). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,018B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,173 entries, 16,692B raw, 4,173B comp}
15/08/16 12:51:08 INFO Executor: Running task 111.0 in stage 1.0 (TID 119)
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,846
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 105) in 817 ms on localhost (96/200)
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000098
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000098_0: Committed
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,034
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,183 entries, 16,732B raw, 4,183B comp}
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO Executor: Finished task 98.0 in stage 1.0 (TID 106). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 112.0 in stage 1.0 (TID 120)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 106) in 829 ms on localhost (97/200)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,057B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,019B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,239B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,201B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,216 entries, 16,864B raw, 4,216B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,169 entries, 16,676B raw, 4,169B comp}
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,954
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,142
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000102
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000102_0: Committed
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000101
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000101_0: Committed
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO Executor: Finished task 102.0 in stage 1.0 (TID 110). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO Executor: Finished task 101.0 in stage 1.0 (TID 109). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000100
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000096
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000100_0: Committed
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000096_0: Committed
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:08 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 113.0 in stage 1.0 (TID 121)
15/08/16 12:51:08 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Finished task 96.0 in stage 1.0 (TID 104). 843 bytes result sent to driver
15/08/16 12:51:08 INFO Executor: Finished task 100.0 in stage 1.0 (TID 108). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 110) in 838 ms on localhost (98/200)
15/08/16 12:51:08 INFO Executor: Running task 114.0 in stage 1.0 (TID 122)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,254B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,216B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,196 entries, 16,784B raw, 4,196B comp}
15/08/16 12:51:08 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 109) in 841 ms on localhost (99/200)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,038B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,000B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO Executor: Running task 115.0 in stage 1.0 (TID 123)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,022
15/08/16 12:51:08 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 116.0 in stage 1.0 (TID 124)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,243 entries, 16,972B raw, 4,243B comp}
15/08/16 12:51:08 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 104) in 889 ms on localhost (100/200)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 108) in 860 ms on localhost (101/200)
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,045B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,007B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,213 entries, 16,852B raw, 4,213B comp}
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,086
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000099
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000099_0: Committed
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000107
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000107_0: Committed
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO Executor: Finished task 99.0 in stage 1.0 (TID 107). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:08 INFO Executor: Finished task 107.0 in stage 1.0 (TID 115). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 117.0 in stage 1.0 (TID 125)
15/08/16 12:51:08 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO Executor: Running task 118.0 in stage 1.0 (TID 126)
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 107) in 906 ms on localhost (102/200)
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 115) in 806 ms on localhost (103/200)
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,229 entries, 16,916B raw, 4,229B comp}
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000104
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000104_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 104.0 in stage 1.0 (TID 112). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 119.0 in stage 1.0 (TID 127)
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,022
15/08/16 12:51:08 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 112) in 865 ms on localhost (104/200)
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000095
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000095_0: Committed
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 21,960B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,922B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO Executor: Finished task 95.0 in stage 1.0 (TID 103). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,239B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,201B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,213 entries, 16,852B raw, 4,213B comp}
15/08/16 12:51:08 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 120.0 in stage 1.0 (TID 128)
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,906
15/08/16 12:51:08 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 103) in 1173 ms on localhost (105/200)
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,206
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,024B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,986B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,234B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,196B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,184 entries, 16,736B raw, 4,184B comp}
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000108
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000108_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 108.0 in stage 1.0 (TID 116). 843 bytes result sent to driver
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 162 ms
15/08/16 12:51:08 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 121.0 in stage 1.0 (TID 129)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,059B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,021B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 116) in 999 ms on localhost (106/200)
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,259 entries, 17,036B raw, 4,259B comp}
15/08/16 12:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,198
15/08/16 12:51:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:08 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:08 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:08 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 22,060B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,022B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:08 INFO ColumnChunkPageWriteStore: written 12,254B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,216B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,257 entries, 17,028B raw, 4,257B comp}
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000105
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000105_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 105.0 in stage 1.0 (TID 113). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 122.0 in stage 1.0 (TID 130)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 113) in 1079 ms on localhost (107/200)
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000106
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000106_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 106.0 in stage 1.0 (TID 114). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:08 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 114) in 1080 ms on localhost (108/200)
15/08/16 12:51:08 INFO Executor: Running task 123.0 in stage 1.0 (TID 131)
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000109
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000109_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 109.0 in stage 1.0 (TID 117). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 124.0 in stage 1.0 (TID 132)
15/08/16 12:51:08 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 117) in 738 ms on localhost (109/200)
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000103
15/08/16 12:51:08 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000103_0: Committed
15/08/16 12:51:08 INFO Executor: Finished task 103.0 in stage 1.0 (TID 111). 843 bytes result sent to driver
15/08/16 12:51:08 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:08 INFO Executor: Running task 125.0 in stage 1.0 (TID 133)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 111) in 1121 ms on localhost (110/200)
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,114
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,003B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,965B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,236 entries, 16,944B raw, 4,236B comp}
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000110
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000110_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 110.0 in stage 1.0 (TID 118). 843 bytes result sent to driver
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO Executor: Running task 126.0 in stage 1.0 (TID 134)
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 118) in 833 ms on localhost (111/200)
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,158
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,930
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,174
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,234
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,028B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,990B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,247 entries, 16,988B raw, 4,247B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 21,946B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,908B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,255B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,217B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,251 entries, 17,004B raw, 4,251B comp}
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,064B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,026B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,266 entries, 17,064B raw, 4,266B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,926
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 21,999B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,961B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,190 entries, 16,760B raw, 4,190B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,039B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,189 entries, 16,756B raw, 4,189B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,058
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,002
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,042B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,208 entries, 16,832B raw, 4,208B comp}
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,122
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000113
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000113_0: Committed
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 21,990B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,952B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,238 entries, 16,952B raw, 4,238B comp}
15/08/16 12:51:09 INFO Executor: Finished task 113.0 in stage 1.0 (TID 121). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO Executor: Running task 127.0 in stage 1.0 (TID 135)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 121) in 664 ms on localhost (112/200)
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,010B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,972B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000116
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000116_0: Committed
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,222 entries, 16,888B raw, 4,222B comp}
15/08/16 12:51:09 INFO Executor: Finished task 116.0 in stage 1.0 (TID 124). 843 bytes result sent to driver
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000112
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000112_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO Executor: Running task 128.0 in stage 1.0 (TID 136)
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000111
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000111_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 124) in 674 ms on localhost (113/200)
15/08/16 12:51:09 INFO Executor: Finished task 112.0 in stage 1.0 (TID 120). 843 bytes result sent to driver
15/08/16 12:51:09 INFO Executor: Finished task 111.0 in stage 1.0 (TID 119). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 129.0 in stage 1.0 (TID 137)
15/08/16 12:51:09 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 130.0 in stage 1.0 (TID 138)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 120) in 723 ms on localhost (114/200)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 119) in 737 ms on localhost (115/200)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000117
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000117_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 117.0 in stage 1.0 (TID 125). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 131.0 in stage 1.0 (TID 139)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 125) in 648 ms on localhost (116/200)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000114
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000114_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 114.0 in stage 1.0 (TID 122). 843 bytes result sent to driver
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000115
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000115_0: Committed
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,062
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 132.0 in stage 1.0 (TID 140)
15/08/16 12:51:09 INFO Executor: Finished task 115.0 in stage 1.0 (TID 123). 843 bytes result sent to driver
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000118
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000118_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 122) in 708 ms on localhost (117/200)
15/08/16 12:51:09 INFO Executor: Finished task 118.0 in stage 1.0 (TID 126). 843 bytes result sent to driver
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,086
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 123) in 713 ms on localhost (118/200)
15/08/16 12:51:09 INFO Executor: Running task 133.0 in stage 1.0 (TID 141)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 126) in 671 ms on localhost (119/200)
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO Executor: Running task 134.0 in stage 1.0 (TID 142)
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,031B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,993B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,223 entries, 16,892B raw, 4,223B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 21,949B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,911B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,229 entries, 16,916B raw, 4,229B comp}
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000120
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000120_0: Committed
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,226
15/08/16 12:51:09 INFO Executor: Finished task 120.0 in stage 1.0 (TID 128). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 135.0 in stage 1.0 (TID 143)
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 128) in 679 ms on localhost (120/200)
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,226
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,057B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,019B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,264 entries, 17,056B raw, 4,264B comp}
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,264 entries, 17,056B raw, 4,264B comp}
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,138
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000119
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000119_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 119.0 in stage 1.0 (TID 127). 843 bytes result sent to driver
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000122
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000122_0: Committed
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,013B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,975B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,242 entries, 16,968B raw, 4,242B comp}
15/08/16 12:51:09 INFO Executor: Finished task 122.0 in stage 1.0 (TID 130). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 127) in 881 ms on localhost (121/200)
15/08/16 12:51:09 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 137.0 in stage 1.0 (TID 145)
15/08/16 12:51:09 INFO Executor: Running task 136.0 in stage 1.0 (TID 144)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 130) in 666 ms on localhost (122/200)
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,206
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,814
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,025B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,259 entries, 17,036B raw, 4,259B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,161 entries, 16,644B raw, 4,161B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,306
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,051B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,284 entries, 17,136B raw, 4,284B comp}
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000124
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000124_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 124.0 in stage 1.0 (TID 132). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 138.0 in stage 1.0 (TID 146)
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:09 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 132) in 687 ms on localhost (123/200)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000123
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000123_0: Committed
15/08/16 12:51:09 INFO Executor: Finished task 123.0 in stage 1.0 (TID 131). 843 bytes result sent to driver
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000125
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000125_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 139.0 in stage 1.0 (TID 147)
15/08/16 12:51:09 INFO Executor: Finished task 125.0 in stage 1.0 (TID 133). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 131) in 705 ms on localhost (124/200)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000126
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000126_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 140.0 in stage 1.0 (TID 148)
15/08/16 12:51:09 INFO Executor: Finished task 126.0 in stage 1.0 (TID 134). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 133) in 685 ms on localhost (125/200)
15/08/16 12:51:09 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 141.0 in stage 1.0 (TID 149)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 134) in 607 ms on localhost (126/200)
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,826
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,966
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 21,983B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,945B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,164 entries, 16,656B raw, 4,164B comp}
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,025B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,199 entries, 16,796B raw, 4,199B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,234
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,278
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000121
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000121_0: Committed
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,266 entries, 17,064B raw, 4,266B comp}
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO Executor: Finished task 121.0 in stage 1.0 (TID 129). 843 bytes result sent to driver
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 129) in 1024 ms on localhost (127/200)
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO Executor: Running task 142.0 in stage 1.0 (TID 150)
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:09 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:09 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:09 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000129
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000129_0: Committed
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,051B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,277 entries, 17,108B raw, 4,277B comp}
15/08/16 12:51:09 INFO Executor: Finished task 129.0 in stage 1.0 (TID 137). 843 bytes result sent to driver
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000128
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000128_0: Committed
15/08/16 12:51:09 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 137) in 596 ms on localhost (128/200)
15/08/16 12:51:09 INFO Executor: Running task 143.0 in stage 1.0 (TID 151)
15/08/16 12:51:09 INFO Executor: Finished task 128.0 in stage 1.0 (TID 136). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 144.0 in stage 1.0 (TID 152)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 136) in 611 ms on localhost (129/200)
15/08/16 12:51:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000127
15/08/16 12:51:09 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000127_0: Committed
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,286
15/08/16 12:51:09 INFO Executor: Finished task 127.0 in stage 1.0 (TID 135). 843 bytes result sent to driver
15/08/16 12:51:09 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:09 INFO Executor: Running task 145.0 in stage 1.0 (TID 153)
15/08/16 12:51:09 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 135) in 628 ms on localhost (130/200)
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,226
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,114
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,279 entries, 17,116B raw, 4,279B comp}
15/08/16 12:51:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,264 entries, 17,056B raw, 4,264B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 2,880B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,844B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:09 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,236 entries, 16,944B raw, 4,236B comp}
15/08/16 12:51:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,306
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000130
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000130_0: Committed
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:10 INFO Executor: Finished task 130.0 in stage 1.0 (TID 138). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO Executor: Running task 146.0 in stage 1.0 (TID 154)
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:10 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 138) in 637 ms on localhost (131/200)
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000134
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000134_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 134.0 in stage 1.0 (TID 142). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 147.0 in stage 1.0 (TID 155)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 142) in 616 ms on localhost (132/200)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,058B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,020B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,284 entries, 17,136B raw, 4,284B comp}
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000132
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000132_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 132.0 in stage 1.0 (TID 140). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 148.0 in stage 1.0 (TID 156)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 140) in 633 ms on localhost (133/200)
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,110
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000133
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000133_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 133.0 in stage 1.0 (TID 141). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 149.0 in stage 1.0 (TID 157)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 141) in 644 ms on localhost (134/200)
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,235 entries, 16,940B raw, 4,235B comp}
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000135
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000135_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 135.0 in stage 1.0 (TID 143). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 150.0 in stage 1.0 (TID 158)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 143) in 627 ms on localhost (135/200)
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,046
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,219 entries, 16,876B raw, 4,219B comp}
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,902
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 21,942B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,904B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,875B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,839B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,252B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,214B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,183 entries, 16,732B raw, 4,183B comp}
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,106
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000131
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000131_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 131.0 in stage 1.0 (TID 139). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 151.0 in stage 1.0 (TID 159)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 139) in 736 ms on localhost (136/200)
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,106
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000136
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000136_0: Committed
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000137
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000137_0: Committed
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,970
15/08/16 12:51:10 INFO Executor: Finished task 136.0 in stage 1.0 (TID 144). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Finished task 137.0 in stage 1.0 (TID 145). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 152.0 in stage 1.0 (TID 160)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,029B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,991B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,254B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,216B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,234 entries, 16,936B raw, 4,234B comp}
15/08/16 12:51:10 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 144) in 681 ms on localhost (137/200)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,025B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO Executor: Running task 153.0 in stage 1.0 (TID 161)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,234 entries, 16,936B raw, 4,234B comp}
15/08/16 12:51:10 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 145) in 680 ms on localhost (138/200)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,044B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,233B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,195B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,200 entries, 16,800B raw, 4,200B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,330
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,036B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,998B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,232B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,194B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,290 entries, 17,160B raw, 4,290B comp}
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000138
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000138_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 138.0 in stage 1.0 (TID 146). 843 bytes result sent to driver
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000141
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000141_0: Committed
15/08/16 12:51:10 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Finished task 141.0 in stage 1.0 (TID 149). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Running task 154.0 in stage 1.0 (TID 162)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 146) in 703 ms on localhost (139/200)
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000139
15/08/16 12:51:10 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 149) in 689 ms on localhost (140/200)
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000139_0: Committed
15/08/16 12:51:10 INFO Executor: Running task 155.0 in stage 1.0 (TID 163)
15/08/16 12:51:10 INFO Executor: Finished task 139.0 in stage 1.0 (TID 147). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 156.0 in stage 1.0 (TID 164)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 147) in 704 ms on localhost (141/200)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000140
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000140_0: Committed
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO Executor: Finished task 140.0 in stage 1.0 (TID 148). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 148) in 738 ms on localhost (142/200)
15/08/16 12:51:10 INFO Executor: Running task 157.0 in stage 1.0 (TID 165)
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,990
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,037B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,999B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,205 entries, 16,820B raw, 4,205B comp}
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,070
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 21,988B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,950B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,225 entries, 16,900B raw, 4,225B comp}
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,990
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,946
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000144
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000144_0: Committed
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,238
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,205 entries, 16,820B raw, 4,205B comp}
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO Executor: Finished task 144.0 in stage 1.0 (TID 152). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 158.0 in stage 1.0 (TID 166)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 152) in 543 ms on localhost (143/200)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,035B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,997B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,865B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,829B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,267 entries, 17,068B raw, 4,267B comp}
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,194 entries, 16,776B raw, 4,194B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,946
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000145
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000145_0: Committed
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,126
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,926
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,118
15/08/16 12:51:10 INFO Executor: Finished task 145.0 in stage 1.0 (TID 153). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,009B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,971B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,254B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,216B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,194 entries, 16,776B raw, 4,194B comp}
15/08/16 12:51:10 INFO Executor: Running task 159.0 in stage 1.0 (TID 167)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 21,988B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,950B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,867B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,189 entries, 16,756B raw, 4,189B comp}
15/08/16 12:51:10 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 153) in 748 ms on localhost (144/200)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000149
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000149_0: Committed
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,239 entries, 16,956B raw, 4,239B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,018B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000142
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000142_0: Committed
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,237 entries, 16,948B raw, 4,237B comp}
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000147
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000147_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 149.0 in stage 1.0 (TID 157). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Finished task 142.0 in stage 1.0 (TID 150). 843 bytes result sent to driver
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:10 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Finished task 147.0 in stage 1.0 (TID 155). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Running task 160.0 in stage 1.0 (TID 168)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 161.0 in stage 1.0 (TID 169)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 162.0 in stage 1.0 (TID 170)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 157) in 694 ms on localhost (145/200)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 155) in 724 ms on localhost (146/200)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 150) in 791 ms on localhost (147/200)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000146
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000146_0: Committed
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000150
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000150_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 146.0 in stage 1.0 (TID 154). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Finished task 150.0 in stage 1.0 (TID 158). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 163.0 in stage 1.0 (TID 171)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 154) in 767 ms on localhost (148/200)
15/08/16 12:51:10 INFO Executor: Running task 164.0 in stage 1.0 (TID 172)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 158) in 710 ms on localhost (149/200)
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000148
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000148_0: Committed
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000143
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000143_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 148.0 in stage 1.0 (TID 156). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Finished task 143.0 in stage 1.0 (TID 151). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Running task 165.0 in stage 1.0 (TID 173)
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,110
15/08/16 12:51:10 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 156) in 760 ms on localhost (150/200)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 166.0 in stage 1.0 (TID 174)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 151) in 828 ms on localhost (151/200)
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,010B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,972B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,235 entries, 16,940B raw, 4,235B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,966
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 21,956B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,918B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,199 entries, 16,796B raw, 4,199B comp}
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000151
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000151_0: Committed
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO Executor: Finished task 151.0 in stage 1.0 (TID 159). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 167.0 in stage 1.0 (TID 175)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 159) in 742 ms on localhost (152/200)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000153
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000153_0: Committed
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO Executor: Finished task 153.0 in stage 1.0 (TID 161). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO Executor: Running task 168.0 in stage 1.0 (TID 176)
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 161) in 566 ms on localhost (153/200)
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,190
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,040B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,002B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,214
15/08/16 12:51:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,255 entries, 17,020B raw, 4,255B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,946
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,261 entries, 17,044B raw, 4,261B comp}
15/08/16 12:51:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,082
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 22,023B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,985B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,882B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,251B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,194 entries, 16,776B raw, 4,194B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 21,999B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,961B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:10 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,228 entries, 16,912B raw, 4,228B comp}
15/08/16 12:51:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:10 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:10 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:10 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000156
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000156_0: Committed
15/08/16 12:51:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000152
15/08/16 12:51:10 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000152_0: Committed
15/08/16 12:51:10 INFO Executor: Finished task 156.0 in stage 1.0 (TID 164). 843 bytes result sent to driver
15/08/16 12:51:10 INFO Executor: Finished task 152.0 in stage 1.0 (TID 160). 843 bytes result sent to driver
15/08/16 12:51:10 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 169.0 in stage 1.0 (TID 177)
15/08/16 12:51:10 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:10 INFO Executor: Running task 170.0 in stage 1.0 (TID 178)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 164) in 570 ms on localhost (154/200)
15/08/16 12:51:10 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 160) in 646 ms on localhost (155/200)
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000154
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000154_0: Committed
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000155
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000155_0: Committed
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,042
15/08/16 12:51:11 INFO Executor: Finished task 154.0 in stage 1.0 (TID 162). 843 bytes result sent to driver
15/08/16 12:51:11 INFO Executor: Finished task 155.0 in stage 1.0 (TID 163). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 171.0 in stage 1.0 (TID 179)
15/08/16 12:51:11 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 162) in 1260 ms on localhost (156/200)
15/08/16 12:51:11 INFO Executor: Running task 172.0 in stage 1.0 (TID 180)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 163) in 1259 ms on localhost (157/200)
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,868B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,832B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,218 entries, 16,872B raw, 4,218B comp}
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,062
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000157
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000157_0: Committed
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:11 INFO Executor: Finished task 157.0 in stage 1.0 (TID 165). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 173.0 in stage 1.0 (TID 181)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 165) in 1270 ms on localhost (158/200)
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 21,955B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,917B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,078
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,223 entries, 16,892B raw, 4,223B comp}
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,021B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,206
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,006B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,882B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,227 entries, 16,908B raw, 4,227B comp}
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000161
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000161_0: Committed
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,259 entries, 17,036B raw, 4,259B comp}
15/08/16 12:51:11 INFO Executor: Finished task 161.0 in stage 1.0 (TID 169). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 174.0 in stage 1.0 (TID 182)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 169) in 992 ms on localhost (159/200)
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,098
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000159
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000158
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000159_0: Committed
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000158_0: Committed
15/08/16 12:51:11 INFO Executor: Finished task 159.0 in stage 1.0 (TID 167). 843 bytes result sent to driver
15/08/16 12:51:11 INFO Executor: Finished task 158.0 in stage 1.0 (TID 166). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 175.0 in stage 1.0 (TID 183)
15/08/16 12:51:11 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 176.0 in stage 1.0 (TID 184)
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 167) in 1046 ms on localhost (160/200)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 166) in 1254 ms on localhost (161/200)
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,041B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,003B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,869B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,239B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,201B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,232 entries, 16,928B raw, 4,232B comp}
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,946
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,022B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,984B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,194 entries, 16,776B raw, 4,194B comp}
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000162
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000162_0: Committed
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:11 INFO Executor: Finished task 162.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,186
15/08/16 12:51:11 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 170) in 1086 ms on localhost (162/200)
15/08/16 12:51:11 INFO Executor: Running task 177.0 in stage 1.0 (TID 185)
15/08/16 12:51:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,048B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,010B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,254 entries, 17,016B raw, 4,254B comp}
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000166
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000166_0: Committed
15/08/16 12:51:11 INFO Executor: Finished task 166.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 178.0 in stage 1.0 (TID 186)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 174) in 1049 ms on localhost (163/200)
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,054
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,006
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,233B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,195B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,209 entries, 16,836B raw, 4,209B comp}
15/08/16 12:51:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,078
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,221 entries, 16,884B raw, 4,221B comp}
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000160
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000160_0: Committed
15/08/16 12:51:11 INFO Executor: Finished task 160.0 in stage 1.0 (TID 168). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 179.0 in stage 1.0 (TID 187)
15/08/16 12:51:11 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 168) in 1137 ms on localhost (164/200)
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 22,030B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,992B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:11 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,227 entries, 16,908B raw, 4,227B comp}
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000164
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000164_0: Committed
15/08/16 12:51:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000165
15/08/16 12:51:11 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000165_0: Committed
15/08/16 12:51:11 INFO Executor: Finished task 164.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/16 12:51:11 INFO Executor: Finished task 165.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/16 12:51:11 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 180.0 in stage 1.0 (TID 188)
15/08/16 12:51:11 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:11 INFO Executor: Running task 181.0 in stage 1.0 (TID 189)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 173) in 1274 ms on localhost (165/200)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 172) in 1292 ms on localhost (166/200)
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000163
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000163_0: Committed
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO Executor: Finished task 163.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 182.0 in stage 1.0 (TID 190)
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 171) in 1349 ms on localhost (167/200)
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,430
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,010
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,011B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,973B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,030B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,992B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,236B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,210 entries, 16,840B raw, 4,210B comp}
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,242
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,881B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,315 entries, 17,260B raw, 4,315B comp}
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,033B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,994
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,250B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,268 entries, 17,072B raw, 4,268B comp}
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000167
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000167_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 167.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 183.0 in stage 1.0 (TID 191)
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000170
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000170_0: Committed
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,036B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,998B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 175) in 1359 ms on localhost (168/200)
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,206 entries, 16,824B raw, 4,206B comp}
15/08/16 12:51:12 INFO Executor: Finished task 170.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 184.0 in stage 1.0 (TID 192)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 178) in 1269 ms on localhost (169/200)
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,058
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,958
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,326
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 21,979B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,941B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,244B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,222 entries, 16,888B raw, 4,222B comp}
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,008B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,970B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,232B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,194B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,197 entries, 16,788B raw, 4,197B comp}
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000171
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000171_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 171.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 179) in 637 ms on localhost (170/200)
15/08/16 12:51:12 INFO Executor: Running task 185.0 in stage 1.0 (TID 193)
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000168
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000168_0: Committed
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
15/08/16 12:51:12 INFO Executor: Finished task 168.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,289 entries, 17,156B raw, 4,289B comp}
15/08/16 12:51:12 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 186.0 in stage 1.0 (TID 194)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 176) in 1409 ms on localhost (171/200)
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,278
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,278
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,277 entries, 17,108B raw, 4,277B comp}
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,253B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,215B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,277 entries, 17,108B raw, 4,277B comp}
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000172
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000172_0: Committed
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO Executor: Finished task 172.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,022
15/08/16 12:51:12 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 180) in 941 ms on localhost (172/200)
15/08/16 12:51:12 INFO Executor: Running task 187.0 in stage 1.0 (TID 195)
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,050
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 21,976B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,938B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,239B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,201B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,213 entries, 16,852B raw, 4,213B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,220 entries, 16,880B raw, 4,220B comp}
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,198
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,030B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,992B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,257 entries, 17,028B raw, 4,257B comp}
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000174
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000174_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 174.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000177
15/08/16 12:51:12 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000177_0: Committed
15/08/16 12:51:12 INFO Executor: Running task 188.0 in stage 1.0 (TID 196)
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:12 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 182) in 900 ms on localhost (173/200)
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000179
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000179_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 177.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 189.0 in stage 1.0 (TID 197)
15/08/16 12:51:12 INFO Executor: Finished task 179.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 190.0 in stage 1.0 (TID 198)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 185) in 813 ms on localhost (174/200)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 187) in 761 ms on localhost (175/200)
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,010
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000176
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000176_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 176.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 191.0 in stage 1.0 (TID 199)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 184) in 886 ms on localhost (176/200)
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 21,997B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,959B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,210 entries, 16,840B raw, 4,210B comp}
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,166
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,062B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,024B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,256B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,218B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,249 entries, 16,996B raw, 4,249B comp}
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000173
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000173_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 173.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000169
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000169_0: Committed
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000178
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000178_0: Committed
15/08/16 12:51:12 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 192.0 in stage 1.0 (TID 200)
15/08/16 12:51:12 INFO Executor: Finished task 178.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/16 12:51:12 INFO Executor: Finished task 169.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 181) in 1012 ms on localhost (177/200)
15/08/16 12:51:12 INFO Executor: Running task 193.0 in stage 1.0 (TID 201)
15/08/16 12:51:12 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 194.0 in stage 1.0 (TID 202)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 186) in 866 ms on localhost (178/200)
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,342
15/08/16 12:51:12 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 177) in 1757 ms on localhost (179/200)
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,037B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,999B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,254B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,216B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,293 entries, 17,172B raw, 4,293B comp}
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000180
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000180_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 180.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 195.0 in stage 1.0 (TID 203)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 188) in 805 ms on localhost (180/200)
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000182
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000182_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 182.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 196.0 in stage 1.0 (TID 204)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 190) in 673 ms on localhost (181/200)
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,158
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,047B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,009B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,237B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,247 entries, 16,988B raw, 4,247B comp}
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,906
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:12 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:12 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:12 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,039B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,242B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,184 entries, 16,736B raw, 4,184B comp}
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000181
15/08/16 12:51:12 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000181_0: Committed
15/08/16 12:51:12 INFO Executor: Finished task 181.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/16 12:51:12 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:12 INFO Executor: Running task 197.0 in stage 1.0 (TID 205)
15/08/16 12:51:12 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 189) in 911 ms on localhost (182/200)
15/08/16 12:51:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,034
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 22,027B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,989B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 2,870B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,834B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:12 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,216 entries, 16,864B raw, 4,216B comp}
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,870
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,942
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000184
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000184_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 184.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,044B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:13 INFO Executor: Running task 198.0 in stage 1.0 (TID 206)
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,249B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,175 entries, 16,700B raw, 4,175B comp}
15/08/16 12:51:13 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 192) in 824 ms on localhost (183/200)
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 21,986B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,948B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,866B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,830B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,238B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,193 entries, 16,772B raw, 4,193B comp}
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000175
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000183
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000175_0: Committed
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000183_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 183.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/16 12:51:13 INFO Executor: Finished task 175.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:13 INFO Executor: Running task 199.0 in stage 1.0 (TID 207)
15/08/16 12:51:13 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 191) in 847 ms on localhost (184/200)
15/08/16 12:51:13 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 183) in 1294 ms on localhost (185/200)
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000186
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000186_0: Committed
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000185
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000185_0: Committed
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO Executor: Finished task 185.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:13 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 193) in 850 ms on localhost (186/200)
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO Executor: Finished task 186.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 194) in 844 ms on localhost (187/200)
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,710
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,883B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,847B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,135 entries, 16,540B raw, 4,135B comp}
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000187
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000187_0: Committed
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,338
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO Executor: Finished task 187.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 195) in 600 ms on localhost (188/200)
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,056B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,018B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,252B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,214B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,292 entries, 17,168B raw, 4,292B comp}
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,234
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,082
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,994
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,010B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,972B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,879B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,843B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,266 entries, 17,064B raw, 4,266B comp}
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000188
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000188_0: Committed
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,050B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,012B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,876B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,840B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,243B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,205B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,206 entries, 16,824B raw, 4,206B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,051B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 22,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO Executor: Finished task 188.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,241B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,203B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,228 entries, 16,912B raw, 4,228B comp}
15/08/16 12:51:13 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 196) in 582 ms on localhost (189/200)
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,978
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,154
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 21,958B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,920B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,032B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,994B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,248B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,246 entries, 16,984B raw, 4,246B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,871B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,835B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,202 entries, 16,808B raw, 4,202B comp}
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000190
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000190_0: Committed
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000191
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000191_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 190.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO Executor: Finished task 191.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 198) in 599 ms on localhost (190/200)
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000189
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000189_0: Committed
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 199) in 586 ms on localhost (191/200)
15/08/16 12:51:13 INFO Executor: Finished task 189.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 197) in 604 ms on localhost (192/200)
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000193
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000193_0: Committed
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,146
15/08/16 12:51:13 INFO Executor: Finished task 193.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000194
15/08/16 12:51:13 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 201) in 549 ms on localhost (193/200)
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000194_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 194.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 202) in 554 ms on localhost (194/200)
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,018B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,872B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,245B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,244 entries, 16,976B raw, 4,244B comp}
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,900,878
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,122
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,011B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,973B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000192
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000192_0: Committed
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,869B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,247B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,209B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,177 entries, 16,708B raw, 4,177B comp}
15/08/16 12:51:13 INFO Executor: Finished task 192.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,026B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,874B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,838B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 200) in 589 ms on localhost (195/200)
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,257B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,219B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,238 entries, 16,952B raw, 4,238B comp}
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000195
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000195_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 195.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000196
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000196_0: Committed
15/08/16 12:51:13 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 203) in 586 ms on localhost (196/200)
15/08/16 12:51:13 INFO Executor: Finished task 196.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 204) in 543 ms on localhost (197/200)
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,118
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,034B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,996B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,877B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,240B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,237 entries, 16,948B raw, 4,237B comp}
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:13 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:13 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:13 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000197
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000197_0: Committed
15/08/16 12:51:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:13 INFO Executor: Finished task 197.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 205) in 532 ms on localhost (198/200)
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,174
15/08/16 12:51:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,901,010
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 22,002B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,964B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,873B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,837B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 21,996B for [l_orderkey] INT32: 7,500 values, 30,007B raw, 21,958B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,230B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,192B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,251 entries, 17,004B raw, 4,251B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 2,878B for [count_suppkey] INT32: 7,500 values, 2,837B raw, 2,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:13 INFO ColumnChunkPageWriteStore: written 12,246B for [max_suppkey] INT32: 7,500 values, 12,217B raw, 12,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 4,210 entries, 16,840B raw, 4,210B comp}
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000199
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000199_0: Committed
15/08/16 12:51:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_temporary/0/task_201508161251_0001_m_000198
15/08/16 12:51:13 INFO SparkHadoopMapRedUtil: attempt_201508161251_0001_m_000198_0: Committed
15/08/16 12:51:13 INFO Executor: Finished task 199.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/16 12:51:13 INFO Executor: Finished task 198.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/16 12:51:13 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 207) in 376 ms on localhost (199/200)
15/08/16 12:51:13 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 206) in 395 ms on localhost (200/200)
15/08/16 12:51:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/16 12:51:13 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 12.767 s
15/08/16 12:51:13 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2857c39d
15/08/16 12:51:13 INFO StatsReportListener: task runtime:(count: 200, mean: 1013.645000, stdev: 380.244367, max: 2239.000000, min: 376.000000)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	376.0 ms	586.0 ms	627.0 ms	724.0 ms	900.0 ms	1.2 s	1.7 s	1.8 s	2.2 s
15/08/16 12:51:13 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.110000, stdev: 0.328481, max: 2.000000, min: 0.000000)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/16 12:51:13 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:51:13 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 23.150409 s
15/08/16 12:51:13 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/16 12:51:13 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 89.049482, stdev: 9.443373, max: 98.112020, min: 43.262411)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	43 %	71 %	76 %	85 %	93 %	95 %	97 %	97 %	98 %
15/08/16 12:51:13 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.013215, stdev: 0.043413, max: 0.271370, min: 0.000000)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/16 12:51:13 INFO StatsReportListener: other time pct: (count: 200, mean: 10.937303, stdev: 9.446056, max: 56.737589, min: 1.865672)
15/08/16 12:51:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:13 INFO StatsReportListener: 	 2 %	 3 %	 3 %	 5 %	 7 %	15 %	24 %	30 %	57 %
15/08/16 12:51:14 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:15 INFO DefaultWriterContainer: Job job_201508161250_0000 committed.
15/08/16 12:51:15 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:15 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/_common_metadata
15/08/16 12:51:15 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:51:15 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/16 12:51:15 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/16 12:51:15 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:51:15 INFO DAGScheduler: Missing parents: List()
15/08/16 12:51:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(2824) called with curMem=391603, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.8 KB, free 3.1 GB)
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(1682) called with curMem=394427, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1682.0 B, free 3.1 GB)
15/08/16 12:51:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:36543 (size: 1682.0 B, free: 3.1 GB)
15/08/16 12:51:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423)
15/08/16 12:51:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/16 12:51:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 208, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/16 12:51:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 208)
15/08/16 12:51:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 208). 606 bytes result sent to driver
15/08/16 12:51:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 208) in 29 ms on localhost (1/1)
15/08/16 12:51:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/16 12:51:15 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.029 s
15/08/16 12:51:15 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.045788 s
15/08/16 12:51:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7b8371aa
15/08/16 12:51:15 INFO StatsReportListener: task runtime:(count: 1, mean: 29.000000, stdev: 0.000000, max: 29.000000, min: 29.000000)
15/08/16 12:51:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:15 INFO StatsReportListener: 	29.0 ms	29.0 ms	29.0 ms	29.0 ms	29.0 ms	29.0 ms	29.0 ms	29.0 ms	29.0 ms
15/08/16 12:51:15 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/16 12:51:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:15 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/16 12:51:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 34.482759, stdev: 0.000000, max: 34.482759, min: 34.482759)
15/08/16 12:51:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:15 INFO StatsReportListener: 	34 %	34 %	34 %	34 %	34 %	34 %	34 %	34 %	34 %
15/08/16 12:51:15 INFO StatsReportListener: other time pct: (count: 1, mean: 65.517241, stdev: 0.000000, max: 65.517241, min: 65.517241)
15/08/16 12:51:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:15 INFO StatsReportListener: 	66 %	66 %	66 %	66 %	66 %	66 %	66 %	66 %	66 %
Time taken: 29.187 seconds
15/08/16 12:51:15 INFO CliDriver: Time taken: 29.187 seconds
15/08/16 12:51:15 INFO ParseDriver: Parsing command: insert into table q21_tmp2_par
select
  l_orderkey, count(distinct l_suppkey), max(l_suppkey) as max_suppkey
from
  lineitem_par
where
  l_receiptdate > l_commitdate
group by l_orderkey
15/08/16 12:51:15 INFO ParseDriver: Parse Completed
15/08/16 12:51:15 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=396109, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=653637, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:15 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/16 12:51:15 INFO Exchange: Using SparkSqlSerializer.
15/08/16 12:51:15 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:51:15 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/16 12:51:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:15 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:15 INFO DAGScheduler: Registering RDD 15 (processCmd at CliDriver.java:423)
15/08/16 12:51:15 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/16 12:51:15 INFO DAGScheduler: Final stage: ResultStage 4(processCmd at CliDriver.java:423)
15/08/16 12:51:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15/08/16 12:51:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15/08/16 12:51:15 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[15] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(10648) called with curMem=676430, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.4 KB, free 3.1 GB)
15/08/16 12:51:15 INFO MemoryStore: ensureFreeSpace(5209) called with curMem=687078, maxMem=3333968363
15/08/16 12:51:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.1 KB, free 3.1 GB)
15/08/16 12:51:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:36543 (size: 5.1 KB, free: 3.1 GB)
15/08/16 12:51:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:15 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[15] at processCmd at CliDriver.java:423)
15/08/16 12:51:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks
15/08/16 12:51:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 209, localhost, ANY, 1792 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 210, localhost, ANY, 1795 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 211, localhost, ANY, 1792 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 212, localhost, ANY, 1792 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 213, localhost, ANY, 1792 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 214, localhost, ANY, 1794 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 215, localhost, ANY, 1794 bytes)
15/08/16 12:51:15 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 216, localhost, ANY, 1793 bytes)
15/08/16 12:51:15 INFO Executor: Running task 1.0 in stage 3.0 (TID 210)
15/08/16 12:51:15 INFO Executor: Running task 2.0 in stage 3.0 (TID 211)
15/08/16 12:51:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 209)
15/08/16 12:51:15 INFO Executor: Running task 5.0 in stage 3.0 (TID 214)
15/08/16 12:51:15 INFO Executor: Running task 7.0 in stage 3.0 (TID 216)
15/08/16 12:51:15 INFO Executor: Running task 3.0 in stage 3.0 (TID 212)
15/08/16 12:51:15 INFO Executor: Running task 6.0 in stage 3.0 (TID 215)
15/08/16 12:51:15 INFO Executor: Running task 4.0 in stage 3.0 (TID 213)
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 26485016 length: 26485016 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 26235204 length: 26235204 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 26536257 length: 26536257 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 26243215 length: 26243215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 26234990 length: 26234990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 26505368 length: 26505368 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 26210131 length: 26210131 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 26576747 length: 26576747 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749056 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 755812 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749157 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749096 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749050 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 748901 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 751036 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749107 records.
15/08/16 12:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 755812
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 749096
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 749157
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 749050
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 748901
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 751036
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 749107
15/08/16 12:51:15 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 749056
15/08/16 12:51:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:36543 in memory (size: 1682.0 B, free: 3.1 GB)
15/08/16 12:51:22 INFO Executor: Finished task 3.0 in stage 3.0 (TID 212). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 212) in 6907 ms on localhost (1/8)
15/08/16 12:51:22 INFO Executor: Finished task 1.0 in stage 3.0 (TID 210). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 210) in 6980 ms on localhost (2/8)
15/08/16 12:51:22 INFO Executor: Finished task 2.0 in stage 3.0 (TID 211). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 211) in 6999 ms on localhost (3/8)
15/08/16 12:51:22 INFO Executor: Finished task 6.0 in stage 3.0 (TID 215). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 215) in 7071 ms on localhost (4/8)
15/08/16 12:51:22 INFO Executor: Finished task 5.0 in stage 3.0 (TID 214). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 214) in 7077 ms on localhost (5/8)
15/08/16 12:51:22 INFO Executor: Finished task 4.0 in stage 3.0 (TID 213). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 213) in 7086 ms on localhost (6/8)
15/08/16 12:51:22 INFO Executor: Finished task 7.0 in stage 3.0 (TID 216). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 216) in 7087 ms on localhost (7/8)
15/08/16 12:51:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 209). 2125 bytes result sent to driver
15/08/16 12:51:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 209) in 7133 ms on localhost (8/8)
15/08/16 12:51:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/16 12:51:22 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 7.133 s
15/08/16 12:51:22 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:22 INFO DAGScheduler: running: Set()
15/08/16 12:51:22 INFO DAGScheduler: waiting: Set(ResultStage 4)
15/08/16 12:51:22 INFO DAGScheduler: failed: Set()
15/08/16 12:51:22 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@40047522
15/08/16 12:51:22 INFO StatsReportListener: task runtime:(count: 8, mean: 7042.500000, stdev: 69.123079, max: 7133.000000, min: 6907.000000)
15/08/16 12:51:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:22 INFO StatsReportListener: 	6.9 s	6.9 s	6.9 s	7.0 s	7.1 s	7.1 s	7.1 s	7.1 s	7.1 s
15/08/16 12:51:22 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 4756579.750000, stdev: 81451.151472, max: 4939722.000000, min: 4702931.000000)
15/08/16 12:51:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:22 INFO StatsReportListener: 	4.5 MB	4.5 MB	4.5 MB	4.5 MB	4.5 MB	4.6 MB	4.7 MB	4.7 MB	4.7 MB
15/08/16 12:51:22 INFO StatsReportListener: task result size:(count: 8, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:22 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:22 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 99.689327, stdev: 0.034785, max: 99.759581, min: 99.628518)
15/08/16 12:51:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:22 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/16 12:51:22 INFO StatsReportListener: other time pct: (count: 8, mean: 0.310673, stdev: 0.034785, max: 0.371482, min: 0.240419)
15/08/16 12:51:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:22 INFO DAGScheduler: Missing parents for ResultStage 4: List()
15/08/16 12:51:22 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/16 12:51:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:51:22 INFO MemoryStore: ensureFreeSpace(80872) called with curMem=687781, maxMem=3333968363
15/08/16 12:51:22 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 79.0 KB, free 3.1 GB)
15/08/16 12:51:22 INFO MemoryStore: ensureFreeSpace(31068) called with curMem=768653, maxMem=3333968363
15/08/16 12:51:22 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KB, free 3.1 GB)
15/08/16 12:51:22 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:36543 (size: 30.3 KB, free: 3.1 GB)
15/08/16 12:51:22 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:22 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at processCmd at CliDriver.java:423)
15/08/16 12:51:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks
15/08/16 12:51:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:22 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:22 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:22 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 217)
15/08/16 12:51:23 INFO Executor: Running task 1.0 in stage 4.0 (TID 218)
15/08/16 12:51:23 INFO Executor: Running task 2.0 in stage 4.0 (TID 219)
15/08/16 12:51:23 INFO Executor: Running task 4.0 in stage 4.0 (TID 221)
15/08/16 12:51:23 INFO Executor: Running task 15.0 in stage 4.0 (TID 232)
15/08/16 12:51:23 INFO Executor: Running task 11.0 in stage 4.0 (TID 228)
15/08/16 12:51:23 INFO Executor: Running task 10.0 in stage 4.0 (TID 227)
15/08/16 12:51:23 INFO Executor: Running task 3.0 in stage 4.0 (TID 220)
15/08/16 12:51:23 INFO Executor: Running task 9.0 in stage 4.0 (TID 226)
15/08/16 12:51:23 INFO Executor: Running task 7.0 in stage 4.0 (TID 224)
15/08/16 12:51:23 INFO Executor: Running task 5.0 in stage 4.0 (TID 222)
15/08/16 12:51:23 INFO Executor: Running task 6.0 in stage 4.0 (TID 223)
15/08/16 12:51:23 INFO Executor: Running task 8.0 in stage 4.0 (TID 225)
15/08/16 12:51:23 INFO Executor: Running task 14.0 in stage 4.0 (TID 231)
15/08/16 12:51:23 INFO Executor: Running task 12.0 in stage 4.0 (TID 229)
15/08/16 12:51:23 INFO Executor: Running task 13.0 in stage 4.0 (TID 230)
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,586
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,250
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,326
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,634
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,096B for [l_orderkey] INT32: 6,857 values, 27,435B raw, 20,058B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,486B for [count_suppkey] INT32: 6,857 values, 2,596B raw, 2,450B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 19,998B for [l_orderkey] INT32: 6,829 values, 27,323B raw, 19,960B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,354
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,239B for [max_suppkey] INT32: 6,857 values, 27,435B raw, 16,201B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,118B for [l_orderkey] INT32: 6,876 values, 27,511B raw, 20,080B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,509B for [count_suppkey] INT32: 6,829 values, 2,584B raw, 2,473B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,890
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,074B for [l_orderkey] INT32: 6,851 values, 27,411B raw, 20,036B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,358B for [count_suppkey] INT32: 6,851 values, 2,593B raw, 2,322B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,190B for [max_suppkey] INT32: 6,829 values, 27,323B raw, 16,152B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,958
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,222B for [max_suppkey] INT32: 6,851 values, 27,411B raw, 16,184B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,138
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,500B for [count_suppkey] INT32: 6,876 values, 2,602B raw, 2,464B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,319B for [max_suppkey] INT32: 6,876 values, 27,511B raw, 16,281B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 19,988B for [l_orderkey] INT32: 6,832 values, 27,335B raw, 19,950B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,434B for [count_suppkey] INT32: 6,832 values, 2,584B raw, 2,398B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,208B for [max_suppkey] INT32: 6,832 values, 27,335B raw, 16,170B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,294
15/08/16 12:51:23 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,137B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,099B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,898
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 19,983B for [l_orderkey] INT32: 6,815 values, 27,267B raw, 19,945B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,455B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,419B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,123B for [l_orderkey] INT32: 6,865 values, 27,467B raw, 20,085B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,389B for [count_suppkey] INT32: 6,815 values, 2,578B raw, 2,353B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,512B for [count_suppkey] INT32: 6,865 values, 2,599B raw, 2,476B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,177B for [max_suppkey] INT32: 6,815 values, 27,267B raw, 16,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,234B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,196B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,036B for [l_orderkey] INT32: 6,843 values, 27,379B raw, 19,998B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,298B for [max_suppkey] INT32: 6,865 values, 27,467B raw, 16,260B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,503B for [count_suppkey] INT32: 6,843 values, 2,590B raw, 2,467B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,236B for [max_suppkey] INT32: 6,843 values, 27,379B raw, 16,198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,794
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,538
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,122B for [l_orderkey] INT32: 6,869 values, 27,483B raw, 20,084B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,576B for [count_suppkey] INT32: 6,869 values, 2,599B raw, 2,540B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,303B for [max_suppkey] INT32: 6,869 values, 27,483B raw, 16,265B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,490
15/08/16 12:51:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,105B for [l_orderkey] INT32: 6,859 values, 27,443B raw, 20,067B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,458B for [count_suppkey] INT32: 6,859 values, 2,596B raw, 2,422B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,257B for [max_suppkey] INT32: 6,859 values, 27,443B raw, 16,219B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,271B for [l_orderkey] INT32: 6,915 values, 27,667B raw, 20,233B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,550B for [count_suppkey] INT32: 6,915 values, 2,617B raw, 2,514B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,359B for [max_suppkey] INT32: 6,915 values, 27,667B raw, 16,321B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000015
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000015_0: Committed
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,177B for [l_orderkey] INT32: 6,896 values, 27,591B raw, 20,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,502B for [count_suppkey] INT32: 6,896 values, 2,608B raw, 2,466B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,322B for [max_suppkey] INT32: 6,896 values, 27,591B raw, 16,284B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000000
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000000_0: Committed
15/08/16 12:51:23 INFO Executor: Finished task 15.0 in stage 4.0 (TID 232). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 217). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 16.0 in stage 4.0 (TID 233)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 17.0 in stage 4.0 (TID 234)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 232) in 551 ms on localhost (1/200)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 217) in 563 ms on localhost (2/200)
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,990
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000001
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000001_0: Committed
15/08/16 12:51:23 INFO Executor: Finished task 1.0 in stage 4.0 (TID 218). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 18.0 in stage 4.0 (TID 235)
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,010
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000007
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000007_0: Committed
15/08/16 12:51:23 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 218) in 571 ms on localhost (3/200)
15/08/16 12:51:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,142
15/08/16 12:51:23 INFO Executor: Finished task 7.0 in stage 4.0 (TID 224). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 19.0 in stage 4.0 (TID 236)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 224) in 575 ms on localhost (4/200)
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000002
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000010
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000002_0: Committed
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000010_0: Committed
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,151B for [l_orderkey] INT32: 6,876 values, 27,511B raw, 20,113B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000004
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,532B for [count_suppkey] INT32: 6,876 values, 2,602B raw, 2,496B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000004_0: Committed
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,280B for [max_suppkey] INT32: 6,876 values, 27,511B raw, 16,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO Executor: Finished task 10.0 in stage 4.0 (TID 227). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Finished task 2.0 in stage 4.0 (TID 219). 843 bytes result sent to driver
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000005
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000012
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000012_0: Committed
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000005_0: Committed
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000008
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000008_0: Committed
15/08/16 12:51:23 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 20.0 in stage 4.0 (TID 237)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Finished task 4.0 in stage 4.0 (TID 221). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Running task 21.0 in stage 4.0 (TID 238)
15/08/16 12:51:23 INFO Executor: Finished task 5.0 in stage 4.0 (TID 222). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Finished task 8.0 in stage 4.0 (TID 225). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Finished task 12.0 in stage 4.0 (TID 229). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 22.0 in stage 4.0 (TID 239)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 227) in 581 ms on localhost (5/200)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 219) in 589 ms on localhost (6/200)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 23.0 in stage 4.0 (TID 240)
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000006
15/08/16 12:51:23 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 221) in 590 ms on localhost (7/200)
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000006_0: Committed
15/08/16 12:51:23 INFO Executor: Finished task 6.0 in stage 4.0 (TID 223). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 222) in 586 ms on localhost (8/200)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 24.0 in stage 4.0 (TID 241)
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 19,935B for [l_orderkey] INT32: 6,822 values, 27,295B raw, 19,897B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,462B for [count_suppkey] INT32: 6,822 values, 2,581B raw, 2,426B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO Executor: Running task 25.0 in stage 4.0 (TID 242)
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,161B for [max_suppkey] INT32: 6,822 values, 27,295B raw, 16,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 26.0 in stage 4.0 (TID 243)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 225) in 588 ms on localhost (9/200)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 229) in 586 ms on localhost (10/200)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 223) in 590 ms on localhost (11/200)
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 20,188B for [l_orderkey] INT32: 6,890 values, 27,567B raw, 20,150B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 2,416B for [count_suppkey] INT32: 6,890 values, 2,608B raw, 2,380B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:23 INFO ColumnChunkPageWriteStore: written 16,305B for [max_suppkey] INT32: 6,890 values, 27,567B raw, 16,267B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000013
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000013_0: Committed
15/08/16 12:51:23 INFO Executor: Finished task 13.0 in stage 4.0 (TID 230). 843 bytes result sent to driver
15/08/16 12:51:23 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 27.0 in stage 4.0 (TID 244)
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 230) in 601 ms on localhost (12/200)
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000009
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000009_0: Committed
15/08/16 12:51:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000003
15/08/16 12:51:23 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000003_0: Committed
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO Executor: Finished task 3.0 in stage 4.0 (TID 220). 843 bytes result sent to driver
15/08/16 12:51:23 INFO Executor: Finished task 9.0 in stage 4.0 (TID 226). 843 bytes result sent to driver
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO Executor: Running task 28.0 in stage 4.0 (TID 245)
15/08/16 12:51:23 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:23 INFO Executor: Running task 29.0 in stage 4.0 (TID 246)
15/08/16 12:51:23 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 220) in 617 ms on localhost (13/200)
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 226) in 609 ms on localhost (14/200)
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000011
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000011_0: Committed
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000014
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000014_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 11.0 in stage 4.0 (TID 228). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Finished task 14.0 in stage 4.0 (TID 231). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 30.0 in stage 4.0 (TID 247)
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO Executor: Running task 31.0 in stage 4.0 (TID 248)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 228) in 1007 ms on localhost (15/200)
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 231) in 1009 ms on localhost (16/200)
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,298
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,274
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,177B for [l_orderkey] INT32: 6,895 values, 27,587B raw, 20,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,537B for [count_suppkey] INT32: 6,895 values, 2,608B raw, 2,501B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,341B for [max_suppkey] INT32: 6,895 values, 27,587B raw, 16,303B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,910
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,092B for [l_orderkey] INT32: 6,874 values, 27,503B raw, 20,054B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,399B for [count_suppkey] INT32: 6,874 values, 2,602B raw, 2,363B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,302B for [max_suppkey] INT32: 6,874 values, 27,503B raw, 16,264B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,149B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,514B for [count_suppkey] INT32: 6,872 values, 2,598B raw, 2,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,750
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,275B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,237B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,766
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,190
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,206
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,706
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000019
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000019_0: Committed
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,061B for [l_orderkey] INT32: 6,857 values, 27,435B raw, 20,023B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,471B for [count_suppkey] INT32: 6,857 values, 2,596B raw, 2,435B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,270B for [max_suppkey] INT32: 6,857 values, 27,435B raw, 16,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,982
15/08/16 12:51:24 INFO Executor: Finished task 19.0 in stage 4.0 (TID 236). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 32.0 in stage 4.0 (TID 249)
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,066
15/08/16 12:51:24 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 236) in 535 ms on localhost (17/200)
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000017
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000017_0: Committed
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,890,998
15/08/16 12:51:24 INFO Executor: Finished task 17.0 in stage 4.0 (TID 234). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,131B for [l_orderkey] INT32: 6,869 values, 27,483B raw, 20,093B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,183B for [l_orderkey] INT32: 6,882 values, 27,535B raw, 20,145B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,480B for [count_suppkey] INT32: 6,869 values, 2,599B raw, 2,444B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,514B for [count_suppkey] INT32: 6,882 values, 2,605B raw, 2,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,254B for [max_suppkey] INT32: 6,869 values, 27,483B raw, 16,216B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,285B for [max_suppkey] INT32: 6,882 values, 27,535B raw, 16,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO Executor: Running task 33.0 in stage 4.0 (TID 250)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 234) in 566 ms on localhost (18/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,233B for [l_orderkey] INT32: 6,903 values, 27,619B raw, 20,195B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,528B for [count_suppkey] INT32: 6,903 values, 2,611B raw, 2,492B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,296B for [max_suppkey] INT32: 6,903 values, 27,619B raw, 16,258B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,041B for [l_orderkey] INT32: 6,836 values, 27,351B raw, 20,003B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,374B for [count_suppkey] INT32: 6,836 values, 2,586B raw, 2,338B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,228B for [max_suppkey] INT32: 6,836 values, 27,351B raw, 16,190B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,306
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,026
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000016
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000016_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 16.0 in stage 4.0 (TID 233). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 34.0 in stage 4.0 (TID 251)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,151B for [l_orderkey] INT32: 6,880 values, 27,527B raw, 20,113B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,531B for [count_suppkey] INT32: 6,880 values, 2,602B raw, 2,495B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,318B for [max_suppkey] INT32: 6,880 values, 27,527B raw, 16,280B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 233) in 593 ms on localhost (19/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 19,990B for [l_orderkey] INT32: 6,819 values, 27,283B raw, 19,952B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,111B for [l_orderkey] INT32: 6,869 values, 27,483B raw, 20,073B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,552B for [count_suppkey] INT32: 6,819 values, 2,581B raw, 2,516B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,189B for [max_suppkey] INT32: 6,819 values, 27,283B raw, 16,151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,538B for [count_suppkey] INT32: 6,869 values, 2,599B raw, 2,502B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,253B for [max_suppkey] INT32: 6,869 values, 27,483B raw, 16,215B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000018
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000018_0: Committed
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000022
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000022_0: Committed
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000024
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000024_0: Committed
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,217B for [l_orderkey] INT32: 6,899 values, 27,603B raw, 20,179B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO Executor: Finished task 18.0 in stage 4.0 (TID 235). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,474B for [count_suppkey] INT32: 6,899 values, 2,611B raw, 2,438B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO Executor: Finished task 22.0 in stage 4.0 (TID 239). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,335B for [max_suppkey] INT32: 6,899 values, 27,603B raw, 16,297B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Finished task 24.0 in stage 4.0 (TID 241). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Running task 35.0 in stage 4.0 (TID 252)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000020
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000020_0: Committed
15/08/16 12:51:24 INFO Executor: Running task 36.0 in stage 4.0 (TID 253)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 37.0 in stage 4.0 (TID 254)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 235) in 599 ms on localhost (20/200)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 239) in 576 ms on localhost (21/200)
15/08/16 12:51:24 INFO Executor: Finished task 20.0 in stage 4.0 (TID 237). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 241) in 573 ms on localhost (22/200)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000023
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000023_0: Committed
15/08/16 12:51:24 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 38.0 in stage 4.0 (TID 255)
15/08/16 12:51:24 INFO Executor: Finished task 23.0 in stage 4.0 (TID 240). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,099B for [l_orderkey] INT32: 6,879 values, 27,523B raw, 20,061B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 237) in 582 ms on localhost (23/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,469B for [count_suppkey] INT32: 6,879 values, 2,602B raw, 2,433B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,293B for [max_suppkey] INT32: 6,879 values, 27,523B raw, 16,255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO Executor: Running task 39.0 in stage 4.0 (TID 256)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000026
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000026_0: Committed
15/08/16 12:51:24 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 240) in 581 ms on localhost (24/200)
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000028
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000028_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 26.0 in stage 4.0 (TID 243). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Finished task 28.0 in stage 4.0 (TID 245). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Running task 40.0 in stage 4.0 (TID 257)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 41.0 in stage 4.0 (TID 258)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 243) in 581 ms on localhost (25/200)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 245) in 562 ms on localhost (26/200)
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000021
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000021_0: Committed
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,950
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:24 INFO Executor: Finished task 21.0 in stage 4.0 (TID 238). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000029
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000029_0: Committed
15/08/16 12:51:24 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 238) in 607 ms on localhost (27/200)
15/08/16 12:51:24 INFO Executor: Finished task 29.0 in stage 4.0 (TID 246). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Running task 42.0 in stage 4.0 (TID 259)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 43.0 in stage 4.0 (TID 260)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 246) in 678 ms on localhost (28/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,095B for [l_orderkey] INT32: 6,859 values, 27,443B raw, 20,057B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,434B for [count_suppkey] INT32: 6,859 values, 2,596B raw, 2,398B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000025
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000025_0: Committed
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,298B for [max_suppkey] INT32: 6,859 values, 27,443B raw, 16,260B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO Executor: Finished task 25.0 in stage 4.0 (TID 242). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 44.0 in stage 4.0 (TID 261)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 242) in 719 ms on localhost (29/200)
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000027
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000027_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 27.0 in stage 4.0 (TID 244). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 45.0 in stage 4.0 (TID 262)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 244) in 739 ms on localhost (30/200)
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,286
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,138B for [l_orderkey] INT32: 6,883 values, 27,539B raw, 20,100B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,453B for [count_suppkey] INT32: 6,883 values, 2,605B raw, 2,417B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,334B for [max_suppkey] INT32: 6,883 values, 27,539B raw, 16,296B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,202
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,133B for [l_orderkey] INT32: 6,884 values, 27,543B raw, 20,095B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,462B for [count_suppkey] INT32: 6,884 values, 2,605B raw, 2,426B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,277B for [max_suppkey] INT32: 6,884 values, 27,543B raw, 16,239B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000031
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000031_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 31.0 in stage 4.0 (TID 248). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 46.0 in stage 4.0 (TID 263)
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 248) in 481 ms on localhost (31/200)
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,614
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000030
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000030_0: Committed
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO Executor: Finished task 30.0 in stage 4.0 (TID 247). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO Executor: Running task 47.0 in stage 4.0 (TID 264)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,097B for [l_orderkey] INT32: 6,857 values, 27,435B raw, 20,059B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,355B for [count_suppkey] INT32: 6,857 values, 2,596B raw, 2,319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,442
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,239B for [max_suppkey] INT32: 6,857 values, 27,435B raw, 16,201B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 247) in 624 ms on localhost (32/200)
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,216B for [l_orderkey] INT32: 6,898 values, 27,599B raw, 20,178B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,472B for [count_suppkey] INT32: 6,898 values, 2,611B raw, 2,436B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,361B for [max_suppkey] INT32: 6,898 values, 27,599B raw, 16,323B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,882
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,426
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,026
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,117B for [l_orderkey] INT32: 6,863 values, 27,459B raw, 20,079B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,530B for [count_suppkey] INT32: 6,863 values, 2,596B raw, 2,494B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,264B for [max_suppkey] INT32: 6,863 values, 27,459B raw, 16,226B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,185B for [l_orderkey] INT32: 6,896 values, 27,591B raw, 20,147B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,529B for [count_suppkey] INT32: 6,896 values, 2,608B raw, 2,493B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,333B for [max_suppkey] INT32: 6,896 values, 27,591B raw, 16,295B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,141B for [l_orderkey] INT32: 6,877 values, 27,515B raw, 20,103B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,786
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,508B for [count_suppkey] INT32: 6,877 values, 2,602B raw, 2,472B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,273B for [max_suppkey] INT32: 6,877 values, 27,515B raw, 16,235B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,874
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000039
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000039_0: Committed
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,237B for [l_orderkey] INT32: 6,917 values, 27,675B raw, 20,199B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO Executor: Finished task 39.0 in stage 4.0 (TID 256). 843 bytes result sent to driver
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,626
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,546B for [count_suppkey] INT32: 6,917 values, 2,616B raw, 2,510B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,398B for [max_suppkey] INT32: 6,917 values, 27,675B raw, 16,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000040
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000040_0: Committed
15/08/16 12:51:24 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 48.0 in stage 4.0 (TID 265)
15/08/16 12:51:24 INFO Executor: Finished task 40.0 in stage 4.0 (TID 257). 843 bytes result sent to driver
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,318
15/08/16 12:51:24 INFO Executor: Running task 49.0 in stage 4.0 (TID 266)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 256) in 518 ms on localhost (33/200)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 257) in 515 ms on localhost (34/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,118B for [l_orderkey] INT32: 6,867 values, 27,475B raw, 20,080B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,388B for [count_suppkey] INT32: 6,867 values, 2,599B raw, 2,352B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,229B for [max_suppkey] INT32: 6,867 values, 27,475B raw, 16,191B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000044
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000044_0: Committed
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO Executor: Finished task 44.0 in stage 4.0 (TID 261). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,049B for [l_orderkey] INT32: 6,859 values, 27,443B raw, 20,011B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 50.0 in stage 4.0 (TID 267)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,540B for [count_suppkey] INT32: 6,859 values, 2,596B raw, 2,504B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,276B for [max_suppkey] INT32: 6,859 values, 27,443B raw, 16,238B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,185B for [l_orderkey] INT32: 6,891 values, 27,571B raw, 20,147B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 261) in 387 ms on localhost (35/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,481B for [count_suppkey] INT32: 6,891 values, 2,608B raw, 2,445B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,324B for [max_suppkey] INT32: 6,891 values, 27,571B raw, 16,286B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000036
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000036_0: Committed
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000037
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000037_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 37.0 in stage 4.0 (TID 254). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Finished task 36.0 in stage 4.0 (TID 253). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 51.0 in stage 4.0 (TID 268)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 52.0 in stage 4.0 (TID 269)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 254) in 545 ms on localhost (36/200)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 253) in 547 ms on localhost (37/200)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000034
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000034_0: Committed
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO Executor: Finished task 34.0 in stage 4.0 (TID 251). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 53.0 in stage 4.0 (TID 270)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000033
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000033_0: Committed
15/08/16 12:51:24 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 251) in 566 ms on localhost (38/200)
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,206
15/08/16 12:51:24 INFO Executor: Finished task 33.0 in stage 4.0 (TID 250). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 54.0 in stage 4.0 (TID 271)
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 250) in 594 ms on localhost (39/200)
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000038
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000038_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 38.0 in stage 4.0 (TID 255). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 55.0 in stage 4.0 (TID 272)
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 255) in 558 ms on localhost (40/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,110B for [l_orderkey] INT32: 6,885 values, 27,547B raw, 20,072B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,494B for [count_suppkey] INT32: 6,885 values, 2,605B raw, 2,458B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,308B for [max_suppkey] INT32: 6,885 values, 27,547B raw, 16,270B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,650
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,066
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,151B for [l_orderkey] INT32: 6,882 values, 27,535B raw, 20,113B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,263B for [l_orderkey] INT32: 6,904 values, 27,623B raw, 20,225B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,498B for [count_suppkey] INT32: 6,882 values, 2,605B raw, 2,462B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,407B for [count_suppkey] INT32: 6,904 values, 2,611B raw, 2,371B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,338B for [max_suppkey] INT32: 6,882 values, 27,535B raw, 16,300B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,399B for [max_suppkey] INT32: 6,904 values, 27,623B raw, 16,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,722
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,110B for [l_orderkey] INT32: 6,858 values, 27,439B raw, 20,072B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,449B for [count_suppkey] INT32: 6,858 values, 2,596B raw, 2,413B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,231B for [max_suppkey] INT32: 6,858 values, 27,439B raw, 16,193B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000041
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000041_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 41.0 in stage 4.0 (TID 258). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 56.0 in stage 4.0 (TID 273)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 258) in 592 ms on localhost (41/200)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000032
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000032_0: Committed
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000035
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000035_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 32.0 in stage 4.0 (TID 249). 843 bytes result sent to driver
15/08/16 12:51:24 INFO Executor: Finished task 35.0 in stage 4.0 (TID 252). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,582
15/08/16 12:51:24 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 249) in 663 ms on localhost (42/200)
15/08/16 12:51:24 INFO Executor: Running task 57.0 in stage 4.0 (TID 274)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 252) in 615 ms on localhost (43/200)
15/08/16 12:51:24 INFO Executor: Running task 58.0 in stage 4.0 (TID 275)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000045
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000045_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 45.0 in stage 4.0 (TID 262). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 59.0 in stage 4.0 (TID 276)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,078B for [l_orderkey] INT32: 6,854 values, 27,423B raw, 20,040B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,425B for [count_suppkey] INT32: 6,854 values, 2,593B raw, 2,389B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 262) in 437 ms on localhost (44/200)
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,219B for [max_suppkey] INT32: 6,854 values, 27,423B raw, 16,181B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,454
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000042
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000042_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 42.0 in stage 4.0 (TID 259). 843 bytes result sent to driver
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 20,021B for [l_orderkey] INT32: 6,846 values, 27,391B raw, 19,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 2,460B for [count_suppkey] INT32: 6,846 values, 2,590B raw, 2,424B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:24 INFO ColumnChunkPageWriteStore: written 16,212B for [max_suppkey] INT32: 6,846 values, 27,391B raw, 16,174B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:24 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 60.0 in stage 4.0 (TID 277)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 259) in 752 ms on localhost (45/200)
15/08/16 12:51:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000046
15/08/16 12:51:24 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000046_0: Committed
15/08/16 12:51:24 INFO Executor: Finished task 46.0 in stage 4.0 (TID 263). 843 bytes result sent to driver
15/08/16 12:51:24 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:24 INFO Executor: Running task 61.0 in stage 4.0 (TID 278)
15/08/16 12:51:24 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 263) in 468 ms on localhost (46/200)
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:24 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,362
15/08/16 12:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:24 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,050B for [l_orderkey] INT32: 6,843 values, 27,379B raw, 20,012B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,555B for [count_suppkey] INT32: 6,843 values, 2,590B raw, 2,519B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,206B for [max_suppkey] INT32: 6,843 values, 27,379B raw, 16,168B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,554
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,022B for [l_orderkey] INT32: 6,844 values, 27,383B raw, 19,984B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,496B for [count_suppkey] INT32: 6,844 values, 2,590B raw, 2,460B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000047
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000047_0: Committed
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,219B for [max_suppkey] INT32: 6,844 values, 27,383B raw, 16,181B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO Executor: Finished task 47.0 in stage 4.0 (TID 264). 843 bytes result sent to driver
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO Executor: Running task 62.0 in stage 4.0 (TID 279)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 264) in 413 ms on localhost (47/200)
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,642
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,690
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,264B for [l_orderkey] INT32: 6,912 values, 27,655B raw, 20,226B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,471B for [count_suppkey] INT32: 6,912 values, 2,614B raw, 2,435B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,406B for [max_suppkey] INT32: 6,912 values, 27,655B raw, 16,368B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,115B for [l_orderkey] INT32: 6,860 values, 27,447B raw, 20,077B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,493B for [count_suppkey] INT32: 6,860 values, 2,596B raw, 2,457B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,256B for [max_suppkey] INT32: 6,860 values, 27,447B raw, 16,218B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000049
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000049_0: Committed
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,026
15/08/16 12:51:25 INFO Executor: Finished task 49.0 in stage 4.0 (TID 266). 843 bytes result sent to driver
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 63.0 in stage 4.0 (TID 280)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 266) in 384 ms on localhost (48/200)
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,172B for [l_orderkey] INT32: 6,876 values, 27,511B raw, 20,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,437B for [count_suppkey] INT32: 6,876 values, 2,602B raw, 2,401B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,297B for [max_suppkey] INT32: 6,876 values, 27,511B raw, 16,259B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,310
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000050
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000050_0: Committed
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000048
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000048_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 50.0 in stage 4.0 (TID 267). 843 bytes result sent to driver
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO Executor: Finished task 48.0 in stage 4.0 (TID 265). 843 bytes result sent to driver
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO Executor: Running task 64.0 in stage 4.0 (TID 281)
15/08/16 12:51:25 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 65.0 in stage 4.0 (TID 282)
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,746
15/08/16 12:51:25 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 265) in 402 ms on localhost (49/200)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 267) in 392 ms on localhost (50/200)
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,006B for [l_orderkey] INT32: 6,836 values, 27,351B raw, 19,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,535B for [count_suppkey] INT32: 6,836 values, 2,587B raw, 2,499B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,181B for [max_suppkey] INT32: 6,836 values, 27,351B raw, 16,143B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,071B for [l_orderkey] INT32: 6,860 values, 27,447B raw, 20,033B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,543B for [count_suppkey] INT32: 6,860 values, 2,596B raw, 2,507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,215B for [max_suppkey] INT32: 6,860 values, 27,447B raw, 16,177B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,118
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:36543 in memory (size: 5.1 KB, free: 3.1 GB)
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000054
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000054_0: Committed
15/08/16 12:51:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:36543 in memory (size: 30.1 KB, free: 3.1 GB)
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,145B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,107B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,501B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,465B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,286B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,248B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,734
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO Executor: Finished task 54.0 in stage 4.0 (TID 271). 843 bytes result sent to driver
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000043
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000043_0: Committed
15/08/16 12:51:25 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 271) in 1030 ms on localhost (51/200)
15/08/16 12:51:25 INFO ContextCleaner: Cleaned shuffle 0
15/08/16 12:51:25 INFO Executor: Running task 66.0 in stage 4.0 (TID 283)
15/08/16 12:51:25 INFO Executor: Finished task 43.0 in stage 4.0 (TID 260). 843 bytes result sent to driver
15/08/16 12:51:25 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 67.0 in stage 4.0 (TID 284)
15/08/16 12:51:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:36543 in memory (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 260) in 1556 ms on localhost (52/200)
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000051
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000051_0: Committed
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,223B for [l_orderkey] INT32: 6,908 values, 27,639B raw, 20,185B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,533B for [count_suppkey] INT32: 6,908 values, 2,614B raw, 2,497B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,372B for [max_suppkey] INT32: 6,908 values, 27,639B raw, 16,334B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO Executor: Finished task 51.0 in stage 4.0 (TID 268). 843 bytes result sent to driver
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,090
15/08/16 12:51:25 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 68.0 in stage 4.0 (TID 285)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 268) in 1051 ms on localhost (53/200)
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,150B for [l_orderkey] INT32: 6,877 values, 27,515B raw, 20,112B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,401B for [count_suppkey] INT32: 6,877 values, 2,602B raw, 2,365B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,279B for [max_suppkey] INT32: 6,877 values, 27,515B raw, 16,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000053
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000053_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 53.0 in stage 4.0 (TID 270). 843 bytes result sent to driver
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000055
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000055_0: Committed
15/08/16 12:51:25 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,454
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,430
15/08/16 12:51:25 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 270) in 1061 ms on localhost (54/200)
15/08/16 12:51:25 INFO Executor: Finished task 55.0 in stage 4.0 (TID 272). 843 bytes result sent to driver
15/08/16 12:51:25 INFO Executor: Running task 69.0 in stage 4.0 (TID 286)
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,910
15/08/16 12:51:25 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 70.0 in stage 4.0 (TID 287)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 272) in 1055 ms on localhost (55/200)
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000052
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000052_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 52.0 in stage 4.0 (TID 269). 843 bytes result sent to driver
15/08/16 12:51:25 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 71.0 in stage 4.0 (TID 288)
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,038B for [l_orderkey] INT32: 6,849 values, 27,403B raw, 20,000B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,227B for [l_orderkey] INT32: 6,908 values, 27,639B raw, 20,189B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 269) in 1075 ms on localhost (56/200)
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,492B for [count_suppkey] INT32: 6,908 values, 2,614B raw, 2,456B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,149B for [l_orderkey] INT32: 6,867 values, 27,475B raw, 20,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,358B for [max_suppkey] INT32: 6,908 values, 27,639B raw, 16,320B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,381B for [count_suppkey] INT32: 6,867 values, 2,599B raw, 2,345B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,523B for [count_suppkey] INT32: 6,849 values, 2,593B raw, 2,487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,253B for [max_suppkey] INT32: 6,867 values, 27,475B raw, 16,215B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,238B for [max_suppkey] INT32: 6,849 values, 27,403B raw, 16,200B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000056
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000056_0: Committed
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO Executor: Finished task 56.0 in stage 4.0 (TID 273). 843 bytes result sent to driver
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 72.0 in stage 4.0 (TID 289)
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 273) in 1028 ms on localhost (57/200)
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000058
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000058_0: Committed
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000059
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000059_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 58.0 in stage 4.0 (TID 275). 843 bytes result sent to driver
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000057
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000057_0: Committed
15/08/16 12:51:25 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Finished task 59.0 in stage 4.0 (TID 276). 843 bytes result sent to driver
15/08/16 12:51:25 INFO Executor: Running task 73.0 in stage 4.0 (TID 290)
15/08/16 12:51:25 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 74.0 in stage 4.0 (TID 291)
15/08/16 12:51:25 INFO Executor: Finished task 57.0 in stage 4.0 (TID 274). 843 bytes result sent to driver
15/08/16 12:51:25 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 275) in 1030 ms on localhost (58/200)
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 276) in 1024 ms on localhost (59/200)
15/08/16 12:51:25 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:25 INFO Executor: Running task 75.0 in stage 4.0 (TID 292)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 274) in 1034 ms on localhost (60/200)
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,358
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,229B for [l_orderkey] INT32: 6,902 values, 27,615B raw, 20,191B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,504B for [count_suppkey] INT32: 6,902 values, 2,611B raw, 2,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,311B for [max_suppkey] INT32: 6,902 values, 27,615B raw, 16,273B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,034
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000060
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000060_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 60.0 in stage 4.0 (TID 277). 843 bytes result sent to driver
15/08/16 12:51:25 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 76.0 in stage 4.0 (TID 293)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 277) in 931 ms on localhost (61/200)
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 20,209B for [l_orderkey] INT32: 6,892 values, 27,575B raw, 20,171B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 2,462B for [count_suppkey] INT32: 6,892 values, 2,608B raw, 2,426B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:25 INFO ColumnChunkPageWriteStore: written 16,305B for [max_suppkey] INT32: 6,892 values, 27,575B raw, 16,267B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000061
15/08/16 12:51:25 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000061_0: Committed
15/08/16 12:51:25 INFO Executor: Finished task 61.0 in stage 4.0 (TID 278). 843 bytes result sent to driver
15/08/16 12:51:25 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:25 INFO Executor: Running task 77.0 in stage 4.0 (TID 294)
15/08/16 12:51:25 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 278) in 938 ms on localhost (62/200)
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:25 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:25 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,070
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,172B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,537B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,501B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,309B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,271B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,618
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000062
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000062_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 62.0 in stage 4.0 (TID 279). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,072B for [l_orderkey] INT32: 6,844 values, 27,383B raw, 20,034B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,504B for [count_suppkey] INT32: 6,844 values, 2,590B raw, 2,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,220B for [max_suppkey] INT32: 6,844 values, 27,383B raw, 16,182B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 78.0 in stage 4.0 (TID 295)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 279) in 1037 ms on localhost (63/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,702
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,362
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,026
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,130
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,231B for [l_orderkey] INT32: 6,907 values, 27,635B raw, 20,193B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,039B for [l_orderkey] INT32: 6,852 values, 27,415B raw, 20,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,466B for [count_suppkey] INT32: 6,907 values, 2,614B raw, 2,430B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,344B for [max_suppkey] INT32: 6,907 values, 27,635B raw, 16,306B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,517B for [count_suppkey] INT32: 6,852 values, 2,593B raw, 2,481B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,267B for [max_suppkey] INT32: 6,852 values, 27,415B raw, 16,229B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,123B for [l_orderkey] INT32: 6,880 values, 27,527B raw, 20,085B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,451B for [count_suppkey] INT32: 6,880 values, 2,602B raw, 2,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,316B for [max_suppkey] INT32: 6,880 values, 27,527B raw, 16,278B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,166B for [l_orderkey] INT32: 6,894 values, 27,583B raw, 20,128B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,542B for [count_suppkey] INT32: 6,894 values, 2,608B raw, 2,506B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,319B for [max_suppkey] INT32: 6,894 values, 27,583B raw, 16,281B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000063
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000063_0: Committed
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,070
15/08/16 12:51:26 INFO Executor: Finished task 63.0 in stage 4.0 (TID 280). 843 bytes result sent to driver
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,422
15/08/16 12:51:26 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 79.0 in stage 4.0 (TID 296)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 280) in 1051 ms on localhost (64/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,124B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,086B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,539B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,503B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,305B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,267B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,098B for [l_orderkey] INT32: 6,849 values, 27,403B raw, 20,060B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,498B for [count_suppkey] INT32: 6,849 values, 2,593B raw, 2,462B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,192B for [max_suppkey] INT32: 6,849 values, 27,403B raw, 16,154B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,922
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000065
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000065_0: Committed
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000064
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000064_0: Committed
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO Executor: Finished task 65.0 in stage 4.0 (TID 282). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000066
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000066_0: Committed
15/08/16 12:51:26 INFO Executor: Running task 80.0 in stage 4.0 (TID 297)
15/08/16 12:51:26 INFO Executor: Finished task 64.0 in stage 4.0 (TID 281). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Finished task 66.0 in stage 4.0 (TID 283). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 282) in 1058 ms on localhost (65/200)
15/08/16 12:51:26 INFO Executor: Running task 81.0 in stage 4.0 (TID 298)
15/08/16 12:51:26 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 82.0 in stage 4.0 (TID 299)
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,110B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,072B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,519B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,483B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,273B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,235B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 281) in 1059 ms on localhost (66/200)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 283) in 400 ms on localhost (67/200)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000068
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000068_0: Committed
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000070
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000070_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 68.0 in stage 4.0 (TID 285). 843 bytes result sent to driver
15/08/16 12:51:26 INFO Executor: Finished task 70.0 in stage 4.0 (TID 287). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 83.0 in stage 4.0 (TID 300)
15/08/16 12:51:26 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 84.0 in stage 4.0 (TID 301)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 285) in 398 ms on localhost (68/200)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 287) in 378 ms on localhost (69/200)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,162
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000069
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000069_0: Committed
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,426
15/08/16 12:51:26 INFO Executor: Finished task 69.0 in stage 4.0 (TID 286). 843 bytes result sent to driver
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,082
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000071
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000071_0: Committed
15/08/16 12:51:26 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 85.0 in stage 4.0 (TID 302)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,222
15/08/16 12:51:26 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 286) in 394 ms on localhost (70/200)
15/08/16 12:51:26 INFO Executor: Finished task 71.0 in stage 4.0 (TID 288). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 86.0 in stage 4.0 (TID 303)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 288) in 387 ms on localhost (71/200)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,121B for [l_orderkey] INT32: 6,886 values, 27,551B raw, 20,083B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,204B for [l_orderkey] INT32: 6,892 values, 27,575B raw, 20,166B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,395B for [count_suppkey] INT32: 6,892 values, 2,608B raw, 2,359B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,348B for [max_suppkey] INT32: 6,892 values, 27,575B raw, 16,310B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,482B for [count_suppkey] INT32: 6,886 values, 2,605B raw, 2,446B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,315B for [max_suppkey] INT32: 6,886 values, 27,551B raw, 16,277B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,175B for [l_orderkey] INT32: 6,880 values, 27,527B raw, 20,137B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,383B for [count_suppkey] INT32: 6,880 values, 2,602B raw, 2,347B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,316B for [max_suppkey] INT32: 6,880 values, 27,527B raw, 16,278B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,063B for [l_orderkey] INT32: 6,846 values, 27,391B raw, 20,025B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,397B for [count_suppkey] INT32: 6,846 values, 2,590B raw, 2,361B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,221B for [max_suppkey] INT32: 6,846 values, 27,391B raw, 16,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,078
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 161 ms
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000072
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000072_0: Committed
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,143B for [l_orderkey] INT32: 6,877 values, 27,515B raw, 20,105B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,490B for [count_suppkey] INT32: 6,877 values, 2,602B raw, 2,454B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,289B for [max_suppkey] INT32: 6,877 values, 27,515B raw, 16,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO Executor: Finished task 72.0 in stage 4.0 (TID 289). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000074
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000067
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000074_0: Committed
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000067_0: Committed
15/08/16 12:51:26 INFO Executor: Running task 87.0 in stage 4.0 (TID 304)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 289) in 580 ms on localhost (72/200)
15/08/16 12:51:26 INFO Executor: Finished task 67.0 in stage 4.0 (TID 284). 843 bytes result sent to driver
15/08/16 12:51:26 INFO Executor: Finished task 74.0 in stage 4.0 (TID 291). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 88.0 in stage 4.0 (TID 305)
15/08/16 12:51:26 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000073
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000073_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 73.0 in stage 4.0 (TID 290). 843 bytes result sent to driver
15/08/16 12:51:26 INFO Executor: Running task 89.0 in stage 4.0 (TID 306)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 284) in 626 ms on localhost (73/200)
15/08/16 12:51:26 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 90.0 in stage 4.0 (TID 307)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 291) in 572 ms on localhost (74/200)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 290) in 573 ms on localhost (75/200)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,506
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,245B for [l_orderkey] INT32: 6,911 values, 27,651B raw, 20,207B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,570B for [count_suppkey] INT32: 6,911 values, 2,614B raw, 2,534B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,380B for [max_suppkey] INT32: 6,911 values, 27,651B raw, 16,342B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000075
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000075_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 75.0 in stage 4.0 (TID 292). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 91.0 in stage 4.0 (TID 308)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000076
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000076_0: Committed
15/08/16 12:51:26 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 292) in 597 ms on localhost (76/200)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO Executor: Finished task 76.0 in stage 4.0 (TID 293). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 92.0 in stage 4.0 (TID 309)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:26 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 293) in 539 ms on localhost (77/200)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,462
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,208B for [l_orderkey] INT32: 6,887 values, 27,555B raw, 20,170B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,516B for [count_suppkey] INT32: 6,887 values, 2,605B raw, 2,480B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,327B for [max_suppkey] INT32: 6,887 values, 27,555B raw, 16,289B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000077
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000077_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 77.0 in stage 4.0 (TID 294). 843 bytes result sent to driver
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO Executor: Running task 93.0 in stage 4.0 (TID 310)
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 294) in 578 ms on localhost (78/200)
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,454
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,079B for [l_orderkey] INT32: 6,841 values, 27,371B raw, 20,041B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,440B for [count_suppkey] INT32: 6,841 values, 2,590B raw, 2,404B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,176B for [max_suppkey] INT32: 6,841 values, 27,371B raw, 16,138B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000078
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000078_0: Committed
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO Executor: Finished task 78.0 in stage 4.0 (TID 295). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 94.0 in stage 4.0 (TID 311)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 295) in 464 ms on localhost (79/200)
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,566
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,870
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,074B for [l_orderkey] INT32: 6,854 values, 27,423B raw, 20,036B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,464B for [count_suppkey] INT32: 6,854 values, 2,593B raw, 2,428B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,230B for [max_suppkey] INT32: 6,854 values, 27,423B raw, 16,192B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,282
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,118
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,462
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,284B for [l_orderkey] INT32: 6,923 values, 27,699B raw, 20,246B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,497B for [count_suppkey] INT32: 6,923 values, 2,620B raw, 2,461B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,369B for [max_suppkey] INT32: 6,923 values, 27,699B raw, 16,331B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,070B for [l_orderkey] INT32: 6,870 values, 27,487B raw, 20,032B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,490B for [count_suppkey] INT32: 6,870 values, 2,599B raw, 2,454B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,291B for [max_suppkey] INT32: 6,870 values, 27,487B raw, 16,253B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,051B for [l_orderkey] INT32: 6,847 values, 27,395B raw, 20,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,213B for [l_orderkey] INT32: 6,899 values, 27,603B raw, 20,175B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,504B for [count_suppkey] INT32: 6,847 values, 2,590B raw, 2,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,412B for [count_suppkey] INT32: 6,899 values, 2,611B raw, 2,376B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,221B for [max_suppkey] INT32: 6,847 values, 27,395B raw, 16,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,329B for [max_suppkey] INT32: 6,899 values, 27,603B raw, 16,291B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,134
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,974
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000080
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000080_0: Committed
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000079
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000079_0: Committed
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO Executor: Finished task 80.0 in stage 4.0 (TID 297). 843 bytes result sent to driver
15/08/16 12:51:26 INFO Executor: Finished task 79.0 in stage 4.0 (TID 296). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,170B for [l_orderkey] INT32: 6,891 values, 27,571B raw, 20,132B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000081
15/08/16 12:51:26 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000081_0: Committed
15/08/16 12:51:26 INFO Executor: Running task 95.0 in stage 4.0 (TID 312)
15/08/16 12:51:26 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 96.0 in stage 4.0 (TID 313)
15/08/16 12:51:26 INFO Executor: Finished task 81.0 in stage 4.0 (TID 298). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,430B for [count_suppkey] INT32: 6,891 values, 2,608B raw, 2,394B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,303B for [max_suppkey] INT32: 6,891 values, 27,571B raw, 16,265B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO Executor: Running task 97.0 in stage 4.0 (TID 314)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 297) in 641 ms on localhost (80/200)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 296) in 664 ms on localhost (81/200)
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000085
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000085_0: Committed
15/08/16 12:51:26 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 298) in 641 ms on localhost (82/200)
15/08/16 12:51:26 INFO Executor: Finished task 85.0 in stage 4.0 (TID 302). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 98.0 in stage 4.0 (TID 315)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 302) in 621 ms on localhost (83/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,168B for [l_orderkey] INT32: 6,886 values, 27,551B raw, 20,130B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,462B for [count_suppkey] INT32: 6,886 values, 2,605B raw, 2,426B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000082
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000082_0: Committed
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,266B for [max_suppkey] INT32: 6,886 values, 27,551B raw, 16,228B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO Executor: Finished task 82.0 in stage 4.0 (TID 299). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 99.0 in stage 4.0 (TID 316)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 299) in 649 ms on localhost (84/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,614
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000086
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000086_0: Committed
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,209B for [l_orderkey] INT32: 6,905 values, 27,627B raw, 20,171B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO Executor: Finished task 86.0 in stage 4.0 (TID 303). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,517B for [count_suppkey] INT32: 6,905 values, 2,614B raw, 2,481B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,388B for [max_suppkey] INT32: 6,905 values, 27,627B raw, 16,350B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 100.0 in stage 4.0 (TID 317)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 303) in 644 ms on localhost (85/200)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,050
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000084
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000084_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 84.0 in stage 4.0 (TID 301). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 101.0 in stage 4.0 (TID 318)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,390
15/08/16 12:51:26 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 301) in 675 ms on localhost (86/200)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000083
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000083_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 83.0 in stage 4.0 (TID 300). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 102.0 in stage 4.0 (TID 319)
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,102B for [l_orderkey] INT32: 6,875 values, 27,507B raw, 20,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,521B for [count_suppkey] INT32: 6,875 values, 2,602B raw, 2,485B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 300) in 681 ms on localhost (87/200)
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,261B for [max_suppkey] INT32: 6,875 values, 27,507B raw, 16,223B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 19,974B for [l_orderkey] INT32: 6,839 values, 27,363B raw, 19,936B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,485B for [count_suppkey] INT32: 6,839 values, 2,587B raw, 2,449B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,204B for [max_suppkey] INT32: 6,839 values, 27,363B raw, 16,166B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,494
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000087
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000087_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 87.0 in stage 4.0 (TID 304). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 103.0 in stage 4.0 (TID 320)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:26 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 304) in 489 ms on localhost (88/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,255B for [l_orderkey] INT32: 6,900 values, 27,607B raw, 20,217B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,508B for [count_suppkey] INT32: 6,900 values, 2,611B raw, 2,472B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,372B for [max_suppkey] INT32: 6,900 values, 27,607B raw, 16,334B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,646
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,893,050
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000089
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000089_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 89.0 in stage 4.0 (TID 306). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,106B for [l_orderkey] INT32: 6,862 values, 27,455B raw, 20,068B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,259B for [l_orderkey] INT32: 6,919 values, 27,683B raw, 20,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,336B for [count_suppkey] INT32: 6,862 values, 2,596B raw, 2,300B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,492B for [count_suppkey] INT32: 6,919 values, 2,617B raw, 2,456B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,267B for [max_suppkey] INT32: 6,862 values, 27,455B raw, 16,229B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,465B for [max_suppkey] INT32: 6,919 values, 27,683B raw, 16,427B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 104.0 in stage 4.0 (TID 321)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 306) in 508 ms on localhost (89/200)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000088
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000088_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 88.0 in stage 4.0 (TID 305). 843 bytes result sent to driver
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 105.0 in stage 4.0 (TID 322)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,330
15/08/16 12:51:26 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 305) in 516 ms on localhost (90/200)
15/08/16 12:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:26 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:26 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 20,193B for [l_orderkey] INT32: 6,901 values, 27,611B raw, 20,155B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,483B for [count_suppkey] INT32: 6,901 values, 2,611B raw, 2,447B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,336B for [max_suppkey] INT32: 6,901 values, 27,611B raw, 16,298B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000092
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000092_0: Committed
15/08/16 12:51:26 INFO Executor: Finished task 92.0 in stage 4.0 (TID 309). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 106.0 in stage 4.0 (TID 323)
15/08/16 12:51:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000090
15/08/16 12:51:26 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000090_0: Committed
15/08/16 12:51:26 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 309) in 505 ms on localhost (91/200)
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:26 INFO Executor: Finished task 90.0 in stage 4.0 (TID 307). 843 bytes result sent to driver
15/08/16 12:51:26 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:26 INFO Executor: Running task 107.0 in stage 4.0 (TID 324)
15/08/16 12:51:26 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 307) in 539 ms on localhost (92/200)
15/08/16 12:51:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,098
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 19,988B for [l_orderkey] INT32: 6,826 values, 27,311B raw, 19,950B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 2,502B for [count_suppkey] INT32: 6,826 values, 2,584B raw, 2,466B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:26 INFO ColumnChunkPageWriteStore: written 16,184B for [max_suppkey] INT32: 6,826 values, 27,311B raw, 16,146B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000091
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000091_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 91.0 in stage 4.0 (TID 308). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 308) in 685 ms on localhost (93/200)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO Executor: Running task 108.0 in stage 4.0 (TID 325)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000093
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000093_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 93.0 in stage 4.0 (TID 310). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 109.0 in stage 4.0 (TID 326)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 310) in 632 ms on localhost (94/200)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,642
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,074B for [l_orderkey] INT32: 6,851 values, 27,411B raw, 20,036B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,500B for [count_suppkey] INT32: 6,851 values, 2,593B raw, 2,464B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,026
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,242B for [max_suppkey] INT32: 6,851 values, 27,411B raw, 16,204B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,192B for [l_orderkey] INT32: 6,887 values, 27,555B raw, 20,154B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,401B for [count_suppkey] INT32: 6,887 values, 2,605B raw, 2,365B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,261B for [max_suppkey] INT32: 6,887 values, 27,555B raw, 16,223B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000094
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000094_0: Committed
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO Executor: Finished task 94.0 in stage 4.0 (TID 311). 843 bytes result sent to driver
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,262
15/08/16 12:51:27 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 110.0 in stage 4.0 (TID 327)
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 311) in 667 ms on localhost (95/200)
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,166B for [l_orderkey] INT32: 6,881 values, 27,531B raw, 20,128B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,465B for [count_suppkey] INT32: 6,881 values, 2,605B raw, 2,429B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,321B for [max_suppkey] INT32: 6,881 values, 27,531B raw, 16,283B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,102
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,574
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,167B for [l_orderkey] INT32: 6,883 values, 27,539B raw, 20,129B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,401B for [count_suppkey] INT32: 6,883 values, 2,604B raw, 2,365B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,311B for [max_suppkey] INT32: 6,883 values, 27,539B raw, 16,273B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,235B for [l_orderkey] INT32: 6,898 values, 27,599B raw, 20,197B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,455B for [count_suppkey] INT32: 6,898 values, 2,611B raw, 2,419B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,380B for [max_suppkey] INT32: 6,898 values, 27,599B raw, 16,342B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,574
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,446
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000098
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000098_0: Committed
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:27 INFO Executor: Finished task 98.0 in stage 4.0 (TID 315). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 111.0 in stage 4.0 (TID 328)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,206B for [l_orderkey] INT32: 6,907 values, 27,635B raw, 20,168B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,427B for [count_suppkey] INT32: 6,907 values, 2,614B raw, 2,391B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,370B for [max_suppkey] INT32: 6,907 values, 27,635B raw, 16,332B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 315) in 470 ms on localhost (96/200)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000095
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000095_0: Committed
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,650
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,210B for [l_orderkey] INT32: 6,896 values, 27,591B raw, 20,172B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,559B for [count_suppkey] INT32: 6,896 values, 2,608B raw, 2,523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO Executor: Finished task 95.0 in stage 4.0 (TID 312). 843 bytes result sent to driver
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000099
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000099_0: Committed
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,335B for [max_suppkey] INT32: 6,896 values, 27,591B raw, 16,297B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 112.0 in stage 4.0 (TID 329)
15/08/16 12:51:27 INFO Executor: Finished task 99.0 in stage 4.0 (TID 316). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 312) in 487 ms on localhost (97/200)
15/08/16 12:51:27 INFO Executor: Running task 113.0 in stage 4.0 (TID 330)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 316) in 477 ms on localhost (98/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,202B for [l_orderkey] INT32: 6,913 values, 27,659B raw, 20,164B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,558B for [count_suppkey] INT32: 6,913 values, 2,617B raw, 2,522B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,339B for [max_suppkey] INT32: 6,913 values, 27,659B raw, 16,301B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,798
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000100
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000100_0: Committed
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO Executor: Finished task 100.0 in stage 4.0 (TID 317). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,144B for [l_orderkey] INT32: 6,869 values, 27,483B raw, 20,106B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,546B for [count_suppkey] INT32: 6,869 values, 2,599B raw, 2,510B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,221B for [max_suppkey] INT32: 6,869 values, 27,483B raw, 16,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 114.0 in stage 4.0 (TID 331)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:27 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 317) in 475 ms on localhost (99/200)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000101
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000101_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 101.0 in stage 4.0 (TID 318). 843 bytes result sent to driver
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 115.0 in stage 4.0 (TID 332)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 318) in 464 ms on localhost (100/200)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000097
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000097_0: Committed
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO Executor: Finished task 97.0 in stage 4.0 (TID 314). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 314) in 515 ms on localhost (101/200)
15/08/16 12:51:27 INFO Executor: Running task 116.0 in stage 4.0 (TID 333)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000102
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000102_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 102.0 in stage 4.0 (TID 319). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 117.0 in stage 4.0 (TID 334)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 319) in 471 ms on localhost (102/200)
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,454
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,990
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,048B for [l_orderkey] INT32: 6,839 values, 27,363B raw, 20,010B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,173B for [l_orderkey] INT32: 6,880 values, 27,527B raw, 20,135B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,488B for [count_suppkey] INT32: 6,880 values, 2,602B raw, 2,452B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,285B for [max_suppkey] INT32: 6,880 values, 27,527B raw, 16,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,558B for [count_suppkey] INT32: 6,839 values, 2,587B raw, 2,522B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,902
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,219B for [max_suppkey] INT32: 6,839 values, 27,363B raw, 16,181B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,223B for [l_orderkey] INT32: 6,923 values, 27,699B raw, 20,185B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,571B for [count_suppkey] INT32: 6,923 values, 2,620B raw, 2,535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,413B for [max_suppkey] INT32: 6,923 values, 27,699B raw, 16,375B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000104
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000104_0: Committed
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000103
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000103_0: Committed
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO Executor: Finished task 104.0 in stage 4.0 (TID 321). 843 bytes result sent to driver
15/08/16 12:51:27 INFO Executor: Finished task 103.0 in stage 4.0 (TID 320). 843 bytes result sent to driver
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 118.0 in stage 4.0 (TID 335)
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 119.0 in stage 4.0 (TID 336)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 321) in 576 ms on localhost (103/200)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 320) in 603 ms on localhost (104/200)
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,550
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000105
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000105_0: Committed
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO Executor: Finished task 105.0 in stage 4.0 (TID 322). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 120.0 in stage 4.0 (TID 337)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 322) in 582 ms on localhost (105/200)
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,266B for [l_orderkey] INT32: 6,918 values, 27,679B raw, 20,228B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,510B for [count_suppkey] INT32: 6,918 values, 2,617B raw, 2,474B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,522
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,345B for [max_suppkey] INT32: 6,918 values, 27,679B raw, 16,307B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,241B for [l_orderkey] INT32: 6,903 values, 27,619B raw, 20,203B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,374
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,506B for [count_suppkey] INT32: 6,903 values, 2,611B raw, 2,470B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,371B for [max_suppkey] INT32: 6,903 values, 27,619B raw, 16,333B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,638
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000107
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000107_0: Committed
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,186B for [l_orderkey] INT32: 6,885 values, 27,547B raw, 20,148B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,512B for [count_suppkey] INT32: 6,885 values, 2,605B raw, 2,476B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,324B for [max_suppkey] INT32: 6,885 values, 27,547B raw, 16,286B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO Executor: Finished task 107.0 in stage 4.0 (TID 324). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000108
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000108_0: Committed
15/08/16 12:51:27 INFO Executor: Running task 121.0 in stage 4.0 (TID 338)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 324) in 591 ms on localhost (106/200)
15/08/16 12:51:27 INFO Executor: Finished task 108.0 in stage 4.0 (TID 325). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 122.0 in stage 4.0 (TID 339)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 325) in 423 ms on localhost (107/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,078B for [l_orderkey] INT32: 6,858 values, 27,439B raw, 20,040B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,495B for [count_suppkey] INT32: 6,858 values, 2,596B raw, 2,459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,258B for [max_suppkey] INT32: 6,858 values, 27,439B raw, 16,220B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000109
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000109_0: Committed
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000106
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000106_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 106.0 in stage 4.0 (TID 323). 843 bytes result sent to driver
15/08/16 12:51:27 INFO Executor: Finished task 109.0 in stage 4.0 (TID 326). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 323) in 627 ms on localhost (108/200)
15/08/16 12:51:27 INFO Executor: Running task 124.0 in stage 4.0 (TID 341)
15/08/16 12:51:27 INFO Executor: Running task 123.0 in stage 4.0 (TID 340)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 326) in 439 ms on localhost (109/200)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,890
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,102B for [l_orderkey] INT32: 6,861 values, 27,451B raw, 20,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,383B for [count_suppkey] INT32: 6,861 values, 2,596B raw, 2,347B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,277B for [max_suppkey] INT32: 6,861 values, 27,451B raw, 16,239B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,618
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,638
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,091B for [l_orderkey] INT32: 6,861 values, 27,451B raw, 20,053B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,479B for [count_suppkey] INT32: 6,861 values, 2,596B raw, 2,443B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,218B for [max_suppkey] INT32: 6,861 values, 27,451B raw, 16,180B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000096
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000096_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 96.0 in stage 4.0 (TID 313). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 125.0 in stage 4.0 (TID 342)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,169B for [l_orderkey] INT32: 6,908 values, 27,639B raw, 20,131B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,481B for [count_suppkey] INT32: 6,908 values, 2,614B raw, 2,445B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,340B for [max_suppkey] INT32: 6,908 values, 27,639B raw, 16,302B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,566
15/08/16 12:51:27 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 313) in 998 ms on localhost (110/200)
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,258
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000110
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000110_0: Committed
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO Executor: Finished task 110.0 in stage 4.0 (TID 327). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 126.0 in stage 4.0 (TID 343)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,233B for [l_orderkey] INT32: 6,914 values, 27,663B raw, 20,195B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 327) in 584 ms on localhost (111/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,419B for [count_suppkey] INT32: 6,914 values, 2,617B raw, 2,383B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,413B for [max_suppkey] INT32: 6,914 values, 27,663B raw, 16,375B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,161B for [l_orderkey] INT32: 6,891 values, 27,571B raw, 20,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,580B for [count_suppkey] INT32: 6,891 values, 2,608B raw, 2,544B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,320B for [max_suppkey] INT32: 6,891 values, 27,571B raw, 16,282B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,478
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,223B for [l_orderkey] INT32: 6,910 values, 27,647B raw, 20,185B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,495B for [count_suppkey] INT32: 6,910 values, 2,614B raw, 2,459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,366B for [max_suppkey] INT32: 6,910 values, 27,647B raw, 16,328B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000114
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000114_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 114.0 in stage 4.0 (TID 331). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000112
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000112_0: Committed
15/08/16 12:51:27 INFO Executor: Running task 127.0 in stage 4.0 (TID 344)
15/08/16 12:51:27 INFO Executor: Finished task 112.0 in stage 4.0 (TID 329). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 331) in 528 ms on localhost (112/200)
15/08/16 12:51:27 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 128.0 in stage 4.0 (TID 345)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 329) in 550 ms on localhost (113/200)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000113
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000113_0: Committed
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,618
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,046
15/08/16 12:51:27 INFO Executor: Finished task 113.0 in stage 4.0 (TID 330). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 129.0 in stage 4.0 (TID 346)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 330) in 558 ms on localhost (114/200)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000115
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000115_0: Committed
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,247B for [l_orderkey] INT32: 6,908 values, 27,639B raw, 20,209B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,392B for [count_suppkey] INT32: 6,908 values, 2,614B raw, 2,356B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,341B for [max_suppkey] INT32: 6,908 values, 27,639B raw, 16,303B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO Executor: Finished task 115.0 in stage 4.0 (TID 332). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 130.0 in stage 4.0 (TID 347)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 332) in 541 ms on localhost (115/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,120B for [l_orderkey] INT32: 6,865 values, 27,467B raw, 20,082B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,529B for [count_suppkey] INT32: 6,865 values, 2,599B raw, 2,493B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,290B for [max_suppkey] INT32: 6,865 values, 27,467B raw, 16,252B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000117
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000117_0: Committed
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000116
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000116_0: Committed
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO Executor: Finished task 117.0 in stage 4.0 (TID 334). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO Executor: Finished task 116.0 in stage 4.0 (TID 333). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 131.0 in stage 4.0 (TID 348)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 334) in 552 ms on localhost (116/200)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 333) in 558 ms on localhost (117/200)
15/08/16 12:51:27 INFO Executor: Running task 132.0 in stage 4.0 (TID 349)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,374
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,177B for [l_orderkey] INT32: 6,886 values, 27,551B raw, 20,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,552B for [count_suppkey] INT32: 6,886 values, 2,605B raw, 2,516B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,374B for [max_suppkey] INT32: 6,886 values, 27,551B raw, 16,336B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,586
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,986
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000119
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000119_0: Committed
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,229B for [l_orderkey] INT32: 6,899 values, 27,603B raw, 20,191B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,396B for [count_suppkey] INT32: 6,899 values, 2,611B raw, 2,360B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,376B for [max_suppkey] INT32: 6,899 values, 27,603B raw, 16,338B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO Executor: Finished task 119.0 in stage 4.0 (TID 336). 843 bytes result sent to driver
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 133.0 in stage 4.0 (TID 350)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 336) in 475 ms on localhost (118/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,152B for [l_orderkey] INT32: 6,880 values, 27,527B raw, 20,114B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,502B for [count_suppkey] INT32: 6,880 values, 2,602B raw, 2,466B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,269B for [max_suppkey] INT32: 6,880 values, 27,527B raw, 16,231B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,194
15/08/16 12:51:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:27 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:27 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000118
15/08/16 12:51:27 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000118_0: Committed
15/08/16 12:51:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,087B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,049B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO Executor: Finished task 118.0 in stage 4.0 (TID 335). 843 bytes result sent to driver
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,495B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,342B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,304B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,893,018
15/08/16 12:51:27 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:27 INFO Executor: Running task 134.0 in stage 4.0 (TID 351)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000120
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000120_0: Committed
15/08/16 12:51:27 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 335) in 502 ms on localhost (119/200)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:27 INFO Executor: Finished task 120.0 in stage 4.0 (TID 337). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 135.0 in stage 4.0 (TID 352)
15/08/16 12:51:27 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 337) in 492 ms on localhost (120/200)
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 20,309B for [l_orderkey] INT32: 6,928 values, 27,719B raw, 20,271B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,572B for [count_suppkey] INT32: 6,928 values, 2,620B raw, 2,536B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,429B for [max_suppkey] INT32: 6,928 values, 27,719B raw, 16,391B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,194
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000121
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000121_0: Committed
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 19,987B for [l_orderkey] INT32: 6,821 values, 27,291B raw, 19,949B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 2,506B for [count_suppkey] INT32: 6,821 values, 2,581B raw, 2,470B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:27 INFO ColumnChunkPageWriteStore: written 16,146B for [max_suppkey] INT32: 6,821 values, 27,291B raw, 16,108B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:27 INFO Executor: Finished task 121.0 in stage 4.0 (TID 338). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO Executor: Running task 136.0 in stage 4.0 (TID 353)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:27 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 338) in 487 ms on localhost (121/200)
15/08/16 12:51:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000124
15/08/16 12:51:27 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000124_0: Committed
15/08/16 12:51:27 INFO Executor: Finished task 124.0 in stage 4.0 (TID 341). 843 bytes result sent to driver
15/08/16 12:51:27 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:27 INFO Executor: Running task 137.0 in stage 4.0 (TID 354)
15/08/16 12:51:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:27 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 341) in 460 ms on localhost (122/200)
15/08/16 12:51:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,914
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,144B for [l_orderkey] INT32: 6,873 values, 27,499B raw, 20,106B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,507B for [count_suppkey] INT32: 6,873 values, 2,602B raw, 2,471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000122
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,290B for [max_suppkey] INT32: 6,873 values, 27,499B raw, 16,252B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000122_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 122.0 in stage 4.0 (TID 339). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 339) in 509 ms on localhost (123/200)
15/08/16 12:51:28 INFO Executor: Running task 138.0 in stage 4.0 (TID 355)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000123
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000123_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 123.0 in stage 4.0 (TID 340). 843 bytes result sent to driver
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 139.0 in stage 4.0 (TID 356)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 340) in 499 ms on localhost (124/200)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,562
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,082B for [l_orderkey] INT32: 6,857 values, 27,435B raw, 20,044B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,549B for [count_suppkey] INT32: 6,857 values, 2,596B raw, 2,513B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000111
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000111_0: Committed
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,253B for [max_suppkey] INT32: 6,857 values, 27,435B raw, 16,215B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO Executor: Finished task 111.0 in stage 4.0 (TID 328). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 140.0 in stage 4.0 (TID 357)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 328) in 950 ms on localhost (125/200)
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000125
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000125_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 125.0 in stage 4.0 (TID 342). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 141.0 in stage 4.0 (TID 358)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 342) in 609 ms on localhost (126/200)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,962
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,318
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,870
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,103B for [l_orderkey] INT32: 6,856 values, 27,431B raw, 20,065B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,215B for [l_orderkey] INT32: 6,900 values, 27,607B raw, 20,177B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,361B for [count_suppkey] INT32: 6,856 values, 2,593B raw, 2,325B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,414B for [count_suppkey] INT32: 6,900 values, 2,611B raw, 2,378B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,296B for [max_suppkey] INT32: 6,856 values, 27,431B raw, 16,258B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,127B for [l_orderkey] INT32: 6,865 values, 27,467B raw, 20,089B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,504B for [count_suppkey] INT32: 6,865 values, 2,599B raw, 2,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,266B for [max_suppkey] INT32: 6,865 values, 27,467B raw, 16,228B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,302B for [max_suppkey] INT32: 6,900 values, 27,607B raw, 16,264B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,710
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,442
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000126
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000126_0: Committed
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000128
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000128_0: Committed
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000130
15/08/16 12:51:28 INFO Executor: Finished task 126.0 in stage 4.0 (TID 343). 843 bytes result sent to driver
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000130_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 128.0 in stage 4.0 (TID 345). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,094B for [l_orderkey] INT32: 6,855 values, 27,427B raw, 20,056B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,446B for [count_suppkey] INT32: 6,855 values, 2,592B raw, 2,410B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 142.0 in stage 4.0 (TID 359)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 143.0 in stage 4.0 (TID 360)
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,255B for [max_suppkey] INT32: 6,855 values, 27,427B raw, 16,217B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO Executor: Finished task 130.0 in stage 4.0 (TID 347). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 343) in 539 ms on localhost (127/200)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 144.0 in stage 4.0 (TID 361)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 345) in 513 ms on localhost (128/200)
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,223B for [l_orderkey] INT32: 6,907 values, 27,635B raw, 20,185B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,578B for [count_suppkey] INT32: 6,907 values, 2,614B raw, 2,542B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,350B for [max_suppkey] INT32: 6,907 values, 27,635B raw, 16,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 347) in 499 ms on localhost (129/200)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,054
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000127
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000127_0: Committed
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO Executor: Finished task 127.0 in stage 4.0 (TID 344). 843 bytes result sent to driver
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,159B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,121B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000129
15/08/16 12:51:28 INFO Executor: Running task 145.0 in stage 4.0 (TID 362)
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000129_0: Committed
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,522B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,486B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,291B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,253B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO Executor: Finished task 129.0 in stage 4.0 (TID 346). 843 bytes result sent to driver
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 344) in 548 ms on localhost (130/200)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO Executor: Running task 146.0 in stage 4.0 (TID 363)
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 346) in 541 ms on localhost (131/200)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000131
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000131_0: Committed
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,338
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO Executor: Finished task 131.0 in stage 4.0 (TID 348). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 147.0 in stage 4.0 (TID 364)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,510
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 348) in 539 ms on localhost (132/200)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,200B for [l_orderkey] INT32: 6,884 values, 27,543B raw, 20,162B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,549B for [count_suppkey] INT32: 6,884 values, 2,605B raw, 2,513B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,327B for [max_suppkey] INT32: 6,884 values, 27,543B raw, 16,289B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,188B for [l_orderkey] INT32: 6,896 values, 27,591B raw, 20,150B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,488B for [count_suppkey] INT32: 6,896 values, 2,608B raw, 2,452B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,352B for [max_suppkey] INT32: 6,896 values, 27,591B raw, 16,314B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,030
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000134
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000134_0: Committed
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000135
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000135_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 134.0 in stage 4.0 (TID 351). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,167B for [l_orderkey] INT32: 6,882 values, 27,535B raw, 20,129B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Finished task 135.0 in stage 4.0 (TID 352). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,543B for [count_suppkey] INT32: 6,882 values, 2,605B raw, 2,507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO Executor: Running task 148.0 in stage 4.0 (TID 365)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,770
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,263B for [max_suppkey] INT32: 6,882 values, 27,535B raw, 16,225B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 149.0 in stage 4.0 (TID 366)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 351) in 475 ms on localhost (133/200)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 352) in 472 ms on localhost (134/200)
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,350
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,231B for [l_orderkey] INT32: 6,914 values, 27,663B raw, 20,193B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,491B for [count_suppkey] INT32: 6,914 values, 2,617B raw, 2,455B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,416B for [max_suppkey] INT32: 6,914 values, 27,663B raw, 16,378B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 19,959B for [l_orderkey] INT32: 6,841 values, 27,371B raw, 19,921B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,375B for [count_suppkey] INT32: 6,841 values, 2,590B raw, 2,339B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,192B for [max_suppkey] INT32: 6,841 values, 27,371B raw, 16,154B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,174
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000136
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000136_0: Committed
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,193B for [l_orderkey] INT32: 6,891 values, 27,571B raw, 20,155B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:28 INFO Executor: Finished task 136.0 in stage 4.0 (TID 353). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,572B for [count_suppkey] INT32: 6,891 values, 2,608B raw, 2,536B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,322B for [max_suppkey] INT32: 6,891 values, 27,571B raw, 16,284B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 150.0 in stage 4.0 (TID 367)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 353) in 569 ms on localhost (135/200)
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000137
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000137_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 137.0 in stage 4.0 (TID 354). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 151.0 in stage 4.0 (TID 368)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 354) in 571 ms on localhost (136/200)
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000133
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000133_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 133.0 in stage 4.0 (TID 350). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 152.0 in stage 4.0 (TID 369)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 350) in 649 ms on localhost (137/200)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,302
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,085B for [l_orderkey] INT32: 6,847 values, 27,395B raw, 20,047B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,437B for [count_suppkey] INT32: 6,847 values, 2,590B raw, 2,401B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,232B for [max_suppkey] INT32: 6,847 values, 27,395B raw, 16,194B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000141
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000141_0: Committed
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,702
15/08/16 12:51:28 INFO Executor: Finished task 141.0 in stage 4.0 (TID 358). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 370, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 153.0 in stage 4.0 (TID 370)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 358) in 425 ms on localhost (138/200)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,590
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,938
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,084B for [l_orderkey] INT32: 6,850 values, 27,407B raw, 20,046B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,368B for [count_suppkey] INT32: 6,850 values, 2,593B raw, 2,332B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,191B for [max_suppkey] INT32: 6,850 values, 27,407B raw, 16,153B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,143B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,105B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,381B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,345B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,279B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,090B for [l_orderkey] INT32: 6,859 values, 27,443B raw, 20,052B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,511B for [count_suppkey] INT32: 6,859 values, 2,596B raw, 2,475B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,264B for [max_suppkey] INT32: 6,859 values, 27,443B raw, 16,226B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000138
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000138_0: Committed
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000140
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000140_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 138.0 in stage 4.0 (TID 355). 843 bytes result sent to driver
15/08/16 12:51:28 INFO Executor: Finished task 140.0 in stage 4.0 (TID 357). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 371, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 154.0 in stage 4.0 (TID 371)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 372, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000139
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000139_0: Committed
15/08/16 12:51:28 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 355) in 675 ms on localhost (139/200)
15/08/16 12:51:28 INFO Executor: Finished task 139.0 in stage 4.0 (TID 356). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 357) in 490 ms on localhost (140/200)
15/08/16 12:51:28 INFO Executor: Running task 155.0 in stage 4.0 (TID 372)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 373, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 156.0 in stage 4.0 (TID 373)
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 356) in 659 ms on localhost (141/200)
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,062
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,172B for [l_orderkey] INT32: 6,882 values, 27,535B raw, 20,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,535B for [count_suppkey] INT32: 6,882 values, 2,605B raw, 2,499B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,350B for [max_suppkey] INT32: 6,882 values, 27,535B raw, 16,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,738
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,010
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000146
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000146_0: Committed
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,127B for [l_orderkey] INT32: 6,882 values, 27,535B raw, 20,089B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,505B for [count_suppkey] INT32: 6,882 values, 2,605B raw, 2,469B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO Executor: Finished task 146.0 in stage 4.0 (TID 363). 843 bytes result sent to driver
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,297B for [max_suppkey] INT32: 6,882 values, 27,535B raw, 16,259B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,102B for [l_orderkey] INT32: 6,867 values, 27,475B raw, 20,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,532B for [count_suppkey] INT32: 6,867 values, 2,599B raw, 2,496B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,226B for [max_suppkey] INT32: 6,867 values, 27,475B raw, 16,188B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 374, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 157.0 in stage 4.0 (TID 374)
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,242
15/08/16 12:51:28 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 363) in 408 ms on localhost (142/200)
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,178B for [l_orderkey] INT32: 6,892 values, 27,575B raw, 20,140B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,425B for [count_suppkey] INT32: 6,892 values, 2,608B raw, 2,389B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000145
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,304B for [max_suppkey] INT32: 6,892 values, 27,575B raw, 16,266B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000142
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000145_0: Committed
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000142_0: Committed
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,194
15/08/16 12:51:28 INFO Executor: Finished task 142.0 in stage 4.0 (TID 359). 843 bytes result sent to driver
15/08/16 12:51:28 INFO Executor: Finished task 145.0 in stage 4.0 (TID 362). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 375, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 158.0 in stage 4.0 (TID 375)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 376, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 159.0 in stage 4.0 (TID 376)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 359) in 478 ms on localhost (143/200)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 362) in 453 ms on localhost (144/200)
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,172B for [l_orderkey] INT32: 6,885 values, 27,547B raw, 20,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,533B for [count_suppkey] INT32: 6,885 values, 2,605B raw, 2,497B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,319B for [max_suppkey] INT32: 6,885 values, 27,547B raw, 16,281B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,906
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,293B for [l_orderkey] INT32: 6,928 values, 27,719B raw, 20,255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,548B for [count_suppkey] INT32: 6,928 values, 2,620B raw, 2,512B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,400B for [max_suppkey] INT32: 6,928 values, 27,719B raw, 16,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,386
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000144
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000144_0: Committed
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO Executor: Finished task 144.0 in stage 4.0 (TID 361). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 377, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 160.0 in stage 4.0 (TID 377)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 361) in 641 ms on localhost (145/200)
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 20,210B for [l_orderkey] INT32: 6,893 values, 27,579B raw, 20,172B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 2,479B for [count_suppkey] INT32: 6,893 values, 2,608B raw, 2,443B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000132
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000132_0: Committed
15/08/16 12:51:28 INFO ColumnChunkPageWriteStore: written 16,339B for [max_suppkey] INT32: 6,893 values, 27,579B raw, 16,301B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000143
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000143_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 143.0 in stage 4.0 (TID 360). 843 bytes result sent to driver
15/08/16 12:51:28 INFO Executor: Finished task 132.0 in stage 4.0 (TID 349). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 378, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 379, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 162.0 in stage 4.0 (TID 379)
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO Executor: Running task 161.0 in stage 4.0 (TID 378)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 360) in 652 ms on localhost (146/200)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 349) in 1123 ms on localhost (147/200)
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000150
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000150_0: Committed
15/08/16 12:51:28 INFO Executor: Finished task 150.0 in stage 4.0 (TID 367). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 380, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 163.0 in stage 4.0 (TID 380)
15/08/16 12:51:28 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 367) in 432 ms on localhost (148/200)
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000147
15/08/16 12:51:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:28 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000147_0: Committed
15/08/16 12:51:28 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:28 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:28 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:28 INFO Executor: Finished task 147.0 in stage 4.0 (TID 364). 843 bytes result sent to driver
15/08/16 12:51:28 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 381, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:28 INFO Executor: Running task 164.0 in stage 4.0 (TID 381)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,858
15/08/16 12:51:29 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 364) in 620 ms on localhost (149/200)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,806
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,259B for [l_orderkey] INT32: 6,917 values, 27,675B raw, 20,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,465B for [count_suppkey] INT32: 6,917 values, 2,617B raw, 2,429B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,414B for [max_suppkey] INT32: 6,917 values, 27,675B raw, 16,376B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,094B for [l_orderkey] INT32: 6,863 values, 27,459B raw, 20,056B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,511B for [count_suppkey] INT32: 6,863 values, 2,596B raw, 2,475B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,270B for [max_suppkey] INT32: 6,863 values, 27,459B raw, 16,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,854
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000148
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000148_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 148.0 in stage 4.0 (TID 365). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 382, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 165.0 in stage 4.0 (TID 382)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 365) in 659 ms on localhost (150/200)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,169B for [l_orderkey] INT32: 6,879 values, 27,523B raw, 20,131B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,545B for [count_suppkey] INT32: 6,879 values, 2,602B raw, 2,509B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,272B for [max_suppkey] INT32: 6,879 values, 27,523B raw, 16,234B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,034
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000149
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000149_0: Committed
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,149B for [l_orderkey] INT32: 6,876 values, 27,511B raw, 20,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,486B for [count_suppkey] INT32: 6,876 values, 2,602B raw, 2,450B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,257B for [max_suppkey] INT32: 6,876 values, 27,511B raw, 16,219B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO Executor: Finished task 149.0 in stage 4.0 (TID 366). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 383, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 166.0 in stage 4.0 (TID 383)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 366) in 686 ms on localhost (151/200)
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,418
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000151
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000151_0: Committed
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,162B for [l_orderkey] INT32: 6,897 values, 27,595B raw, 20,124B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,559B for [count_suppkey] INT32: 6,897 values, 2,611B raw, 2,523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,361B for [max_suppkey] INT32: 6,897 values, 27,595B raw, 16,323B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO Executor: Finished task 151.0 in stage 4.0 (TID 368). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 384, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 167.0 in stage 4.0 (TID 384)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 368) in 581 ms on localhost (152/200)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,942
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,146B for [l_orderkey] INT32: 6,879 values, 27,523B raw, 20,108B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,443B for [count_suppkey] INT32: 6,879 values, 2,602B raw, 2,407B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,259B for [max_suppkey] INT32: 6,879 values, 27,523B raw, 16,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,742
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000153
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000153_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 153.0 in stage 4.0 (TID 370). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,074B for [l_orderkey] INT32: 6,859 values, 27,443B raw, 20,036B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,494B for [count_suppkey] INT32: 6,859 values, 2,596B raw, 2,458B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,234B for [max_suppkey] INT32: 6,859 values, 27,443B raw, 16,196B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 385, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 168.0 in stage 4.0 (TID 385)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 370) in 534 ms on localhost (153/200)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,646
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000155
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000155_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 155.0 in stage 4.0 (TID 372). 843 bytes result sent to driver
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 386, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO Executor: Running task 169.0 in stage 4.0 (TID 386)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 372) in 511 ms on localhost (154/200)
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,215B for [l_orderkey] INT32: 6,903 values, 27,619B raw, 20,177B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,530B for [count_suppkey] INT32: 6,903 values, 2,611B raw, 2,494B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,368B for [max_suppkey] INT32: 6,903 values, 27,619B raw, 16,330B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000154
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000154_0: Committed
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO Executor: Finished task 154.0 in stage 4.0 (TID 371). 843 bytes result sent to driver
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 387, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 170.0 in stage 4.0 (TID 387)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 371) in 534 ms on localhost (155/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000156
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000156_0: Committed
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO Executor: Finished task 156.0 in stage 4.0 (TID 373). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:29 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 388, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 171.0 in stage 4.0 (TID 388)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 373) in 537 ms on localhost (156/200)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,893,090
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,310B for [l_orderkey] INT32: 6,930 values, 27,727B raw, 20,272B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,506B for [count_suppkey] INT32: 6,930 values, 2,623B raw, 2,470B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,449B for [max_suppkey] INT32: 6,930 values, 27,727B raw, 16,411B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,582
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,206
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,105B for [l_orderkey] INT32: 6,855 values, 27,427B raw, 20,067B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,527B for [count_suppkey] INT32: 6,855 values, 2,593B raw, 2,491B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,226B for [max_suppkey] INT32: 6,855 values, 27,427B raw, 16,188B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,146B for [l_orderkey] INT32: 6,879 values, 27,523B raw, 20,108B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,566B for [count_suppkey] INT32: 6,879 values, 2,602B raw, 2,530B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,289B for [max_suppkey] INT32: 6,879 values, 27,523B raw, 16,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000157
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000157_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 157.0 in stage 4.0 (TID 374). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 389, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 172.0 in stage 4.0 (TID 389)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,910
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,590
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000158
15/08/16 12:51:29 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 374) in 688 ms on localhost (157/200)
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000158_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 158.0 in stage 4.0 (TID 375). 843 bytes result sent to driver
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000159
15/08/16 12:51:29 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 390, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000159_0: Committed
15/08/16 12:51:29 INFO Executor: Running task 173.0 in stage 4.0 (TID 390)
15/08/16 12:51:29 INFO Executor: Finished task 159.0 in stage 4.0 (TID 376). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 391, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 174.0 in stage 4.0 (TID 391)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,142B for [l_orderkey] INT32: 6,870 values, 27,487B raw, 20,104B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,019B for [l_orderkey] INT32: 6,853 values, 27,419B raw, 19,981B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,390B for [count_suppkey] INT32: 6,870 values, 2,599B raw, 2,354B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,536B for [count_suppkey] INT32: 6,853 values, 2,593B raw, 2,500B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,270B for [max_suppkey] INT32: 6,870 values, 27,487B raw, 16,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,231B for [max_suppkey] INT32: 6,853 values, 27,419B raw, 16,193B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 375) in 663 ms on localhost (158/200)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 376) in 663 ms on localhost (159/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000152
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000152_0: Committed
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,474
15/08/16 12:51:29 INFO Executor: Finished task 152.0 in stage 4.0 (TID 369). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 392, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 175.0 in stage 4.0 (TID 392)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 369) in 894 ms on localhost (160/200)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,254
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,191B for [l_orderkey] INT32: 6,894 values, 27,583B raw, 20,153B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,441B for [count_suppkey] INT32: 6,894 values, 2,608B raw, 2,405B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,142B for [l_orderkey] INT32: 6,883 values, 27,539B raw, 20,104B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,333B for [max_suppkey] INT32: 6,894 values, 27,583B raw, 16,295B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,480B for [count_suppkey] INT32: 6,883 values, 2,605B raw, 2,444B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,316B for [max_suppkey] INT32: 6,883 values, 27,539B raw, 16,278B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000161
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000161_0: Committed
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO Executor: Finished task 161.0 in stage 4.0 (TID 378). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 393, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 176.0 in stage 4.0 (TID 393)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 378) in 522 ms on localhost (161/200)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000162
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000162_0: Committed
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000164
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000164_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 162.0 in stage 4.0 (TID 379). 843 bytes result sent to driver
15/08/16 12:51:29 INFO Executor: Finished task 164.0 in stage 4.0 (TID 381). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 394, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 177.0 in stage 4.0 (TID 394)
15/08/16 12:51:29 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 395, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 178.0 in stage 4.0 (TID 395)
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 379) in 531 ms on localhost (162/200)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 381) in 501 ms on localhost (163/200)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,106
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,149B for [l_orderkey] INT32: 6,881 values, 27,531B raw, 20,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,496B for [count_suppkey] INT32: 6,881 values, 2,605B raw, 2,460B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,341B for [max_suppkey] INT32: 6,881 values, 27,531B raw, 16,303B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,818
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,104B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,066B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,446B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,410B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,297B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,259B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000163
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000163_0: Committed
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO Executor: Finished task 163.0 in stage 4.0 (TID 380). 843 bytes result sent to driver
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 396, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 179.0 in stage 4.0 (TID 396)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 380) in 609 ms on localhost (164/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000166
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000166_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 166.0 in stage 4.0 (TID 383). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 397, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 180.0 in stage 4.0 (TID 397)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,958
15/08/16 12:51:29 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 383) in 487 ms on localhost (165/200)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,289B for [l_orderkey] INT32: 6,925 values, 27,707B raw, 20,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,567B for [count_suppkey] INT32: 6,925 values, 2,620B raw, 2,531B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,382B for [max_suppkey] INT32: 6,925 values, 27,707B raw, 16,344B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,894
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,202
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,165B for [l_orderkey] INT32: 6,877 values, 27,515B raw, 20,127B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,519B for [count_suppkey] INT32: 6,877 values, 2,602B raw, 2,483B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,282B for [max_suppkey] INT32: 6,877 values, 27,515B raw, 16,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,033B for [l_orderkey] INT32: 6,833 values, 27,339B raw, 19,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,481B for [count_suppkey] INT32: 6,833 values, 2,587B raw, 2,445B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,161B for [max_suppkey] INT32: 6,833 values, 27,339B raw, 16,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000165
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000165_0: Committed
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO Executor: Finished task 165.0 in stage 4.0 (TID 382). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 398, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 181.0 in stage 4.0 (TID 398)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 382) in 560 ms on localhost (166/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000168
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000168_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 168.0 in stage 4.0 (TID 385). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 399, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 182.0 in stage 4.0 (TID 399)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 385) in 469 ms on localhost (167/200)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,678
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,041B for [l_orderkey] INT32: 6,858 values, 27,439B raw, 20,003B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,492B for [count_suppkey] INT32: 6,858 values, 2,596B raw, 2,456B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,225B for [max_suppkey] INT32: 6,858 values, 27,439B raw, 16,187B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,746
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000169
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000169_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 169.0 in stage 4.0 (TID 386). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 400, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,076B for [l_orderkey] INT32: 6,854 values, 27,423B raw, 20,038B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 386) in 609 ms on localhost (168/200)
15/08/16 12:51:29 INFO Executor: Running task 183.0 in stage 4.0 (TID 400)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,380B for [count_suppkey] INT32: 6,854 values, 2,593B raw, 2,344B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,249B for [max_suppkey] INT32: 6,854 values, 27,423B raw, 16,211B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,354
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,113B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,075B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,533B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,497B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,329B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,291B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000171
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000171_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 171.0 in stage 4.0 (TID 388). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 401, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 184.0 in stage 4.0 (TID 401)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 388) in 621 ms on localhost (169/200)
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000170
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000170_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 170.0 in stage 4.0 (TID 387). 843 bytes result sent to driver
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,798
15/08/16 12:51:29 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 402, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 185.0 in stage 4.0 (TID 402)
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,066
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 387) in 646 ms on localhost (170/200)
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,838
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,159B for [l_orderkey] INT32: 6,873 values, 27,499B raw, 20,121B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,097B for [l_orderkey] INT32: 6,860 values, 27,447B raw, 20,059B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,467B for [count_suppkey] INT32: 6,873 values, 2,602B raw, 2,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,430B for [count_suppkey] INT32: 6,860 values, 2,596B raw, 2,394B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,346B for [max_suppkey] INT32: 6,873 values, 27,499B raw, 16,308B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,313B for [max_suppkey] INT32: 6,860 values, 27,447B raw, 16,275B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,139B for [l_orderkey] INT32: 6,870 values, 27,487B raw, 20,101B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,397B for [count_suppkey] INT32: 6,870 values, 2,599B raw, 2,361B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,247B for [max_suppkey] INT32: 6,870 values, 27,487B raw, 16,209B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,434
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000160
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000160_0: Committed
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,044B for [l_orderkey] INT32: 6,845 values, 27,387B raw, 20,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,545B for [count_suppkey] INT32: 6,845 values, 2,590B raw, 2,509B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,197B for [max_suppkey] INT32: 6,845 values, 27,387B raw, 16,159B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000175
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000175_0: Committed
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000173
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000173_0: Committed
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO Executor: Finished task 175.0 in stage 4.0 (TID 392). 843 bytes result sent to driver
15/08/16 12:51:29 INFO Executor: Finished task 173.0 in stage 4.0 (TID 390). 843 bytes result sent to driver
15/08/16 12:51:29 INFO Executor: Finished task 160.0 in stage 4.0 (TID 377). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 403, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,893,158
15/08/16 12:51:29 INFO Executor: Running task 186.0 in stage 4.0 (TID 403)
15/08/16 12:51:29 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 404, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 187.0 in stage 4.0 (TID 404)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,734
15/08/16 12:51:29 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 405, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 188.0 in stage 4.0 (TID 405)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 392) in 447 ms on localhost (171/200)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 390) in 458 ms on localhost (172/200)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 377) in 952 ms on localhost (173/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000172
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000172_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 172.0 in stage 4.0 (TID 389). 843 bytes result sent to driver
15/08/16 12:51:29 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 406, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 189.0 in stage 4.0 (TID 406)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,037B for [l_orderkey] INT32: 6,852 values, 27,415B raw, 19,999B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,483B for [count_suppkey] INT32: 6,852 values, 2,593B raw, 2,447B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,268B for [max_suppkey] INT32: 6,852 values, 27,415B raw, 16,230B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 389) in 476 ms on localhost (174/200)
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,324B for [l_orderkey] INT32: 6,941 values, 27,771B raw, 20,286B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,487B for [count_suppkey] INT32: 6,941 values, 2,626B raw, 2,451B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,746
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,463B for [max_suppkey] INT32: 6,941 values, 27,771B raw, 16,425B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000178
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000178_0: Committed
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:29 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:29 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:29 INFO Executor: Finished task 178.0 in stage 4.0 (TID 395). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 20,081B for [l_orderkey] INT32: 6,863 values, 27,459B raw, 20,043B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 2,454B for [count_suppkey] INT32: 6,863 values, 2,596B raw, 2,418B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:29 INFO ColumnChunkPageWriteStore: written 16,230B for [max_suppkey] INT32: 6,863 values, 27,459B raw, 16,192B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:29 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 407, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 190.0 in stage 4.0 (TID 407)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 395) in 448 ms on localhost (175/200)
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000177
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000177_0: Committed
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:29 INFO Executor: Finished task 177.0 in stage 4.0 (TID 394). 843 bytes result sent to driver
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000176
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000176_0: Committed
15/08/16 12:51:29 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 408, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 191.0 in stage 4.0 (TID 408)
15/08/16 12:51:29 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 394) in 468 ms on localhost (176/200)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO Executor: Finished task 176.0 in stage 4.0 (TID 393). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:51:29 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 409, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:51:29 INFO Executor: Running task 192.0 in stage 4.0 (TID 409)
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:29 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 393) in 483 ms on localhost (177/200)
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,730
15/08/16 12:51:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,114
15/08/16 12:51:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000174
15/08/16 12:51:29 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000174_0: Committed
15/08/16 12:51:29 INFO Executor: Finished task 174.0 in stage 4.0 (TID 391). 843 bytes result sent to driver
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:29 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 410, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:29 INFO Executor: Running task 193.0 in stage 4.0 (TID 410)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 391) in 548 ms on localhost (178/200)
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,257B for [l_orderkey] INT32: 6,915 values, 27,667B raw, 20,219B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,405B for [count_suppkey] INT32: 6,915 values, 2,617B raw, 2,369B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,406B for [max_suppkey] INT32: 6,915 values, 27,667B raw, 16,368B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,143B for [l_orderkey] INT32: 6,877 values, 27,515B raw, 20,105B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,506B for [count_suppkey] INT32: 6,877 values, 2,602B raw, 2,470B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,276B for [max_suppkey] INT32: 6,877 values, 27,515B raw, 16,238B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000180
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000180_0: Committed
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000167
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000167_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 167.0 in stage 4.0 (TID 384). 843 bytes result sent to driver
15/08/16 12:51:30 INFO Executor: Finished task 180.0 in stage 4.0 (TID 397). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 411, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO Executor: Running task 194.0 in stage 4.0 (TID 411)
15/08/16 12:51:30 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 412, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO Executor: Running task 195.0 in stage 4.0 (TID 412)
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 397) in 492 ms on localhost (179/200)
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 384) in 945 ms on localhost (180/200)
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,378
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,037B for [l_orderkey] INT32: 6,841 values, 27,371B raw, 19,999B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,504B for [count_suppkey] INT32: 6,841 values, 2,590B raw, 2,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,186B for [max_suppkey] INT32: 6,841 values, 27,371B raw, 16,148B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,966
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000182
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000182_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 182.0 in stage 4.0 (TID 399). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 413, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO Executor: Running task 196.0 in stage 4.0 (TID 413)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 399) in 496 ms on localhost (181/200)
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,164B for [l_orderkey] INT32: 6,878 values, 27,519B raw, 20,126B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,497B for [count_suppkey] INT32: 6,878 values, 2,602B raw, 2,461B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,272B for [max_suppkey] INT32: 6,878 values, 27,519B raw, 16,234B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000181
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000181_0: Committed
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO Executor: Finished task 181.0 in stage 4.0 (TID 398). 843 bytes result sent to driver
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 414, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO Executor: Running task 197.0 in stage 4.0 (TID 414)
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 398) in 660 ms on localhost (182/200)
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,682
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,099B for [l_orderkey] INT32: 6,863 values, 27,459B raw, 20,061B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,786
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,521B for [count_suppkey] INT32: 6,863 values, 2,596B raw, 2,485B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,240B for [max_suppkey] INT32: 6,863 values, 27,459B raw, 16,202B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,173B for [l_orderkey] INT32: 6,879 values, 27,523B raw, 20,135B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,565B for [count_suppkey] INT32: 6,879 values, 2,602B raw, 2,529B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,259B for [max_suppkey] INT32: 6,879 values, 27,523B raw, 16,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,746
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,075B for [l_orderkey] INT32: 6,863 values, 27,459B raw, 20,037B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,387B for [count_suppkey] INT32: 6,863 values, 2,596B raw, 2,351B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,250B for [max_suppkey] INT32: 6,863 values, 27,459B raw, 16,212B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000183
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000183_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 183.0 in stage 4.0 (TID 400). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 415, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO Executor: Running task 198.0 in stage 4.0 (TID 415)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 400) in 530 ms on localhost (183/200)
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000184
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000184_0: Committed
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO Executor: Finished task 184.0 in stage 4.0 (TID 401). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 416, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:51:30 INFO Executor: Running task 199.0 in stage 4.0 (TID 416)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 401) in 498 ms on localhost (184/200)
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000185
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000185_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 185.0 in stage 4.0 (TID 402). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 402) in 500 ms on localhost (185/200)
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,938
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,118B for [l_orderkey] INT32: 6,868 values, 27,479B raw, 20,080B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,528B for [count_suppkey] INT32: 6,868 values, 2,599B raw, 2,492B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,269B for [max_suppkey] INT32: 6,868 values, 27,479B raw, 16,231B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,830
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,470
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,148B for [l_orderkey] INT32: 6,867 values, 27,475B raw, 20,110B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,242
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,575B for [count_suppkey] INT32: 6,867 values, 2,599B raw, 2,539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000192
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,247B for [max_suppkey] INT32: 6,867 values, 27,475B raw, 16,209B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,226B for [l_orderkey] INT32: 6,907 values, 27,635B raw, 20,188B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000192_0: Committed
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,546B for [count_suppkey] INT32: 6,907 values, 2,614B raw, 2,510B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,327B for [max_suppkey] INT32: 6,907 values, 27,635B raw, 16,289B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO Executor: Finished task 192.0 in stage 4.0 (TID 409). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,232B for [l_orderkey] INT32: 6,894 values, 27,583B raw, 20,194B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 409) in 458 ms on localhost (186/200)
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,471B for [count_suppkey] INT32: 6,894 values, 2,608B raw, 2,435B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,299B for [max_suppkey] INT32: 6,894 values, 27,583B raw, 16,261B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,926
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000179
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000179_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 179.0 in stage 4.0 (TID 396). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 396) in 857 ms on localhost (187/200)
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000189
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000189_0: Committed
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,702
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000186
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000186_0: Committed
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO Executor: Finished task 189.0 in stage 4.0 (TID 406). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,201B for [l_orderkey] INT32: 6,892 values, 27,575B raw, 20,163B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO Executor: Finished task 186.0 in stage 4.0 (TID 403). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,515B for [count_suppkey] INT32: 6,892 values, 2,608B raw, 2,479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,292B for [max_suppkey] INT32: 6,892 values, 27,575B raw, 16,254B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 406) in 533 ms on localhost (188/200)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 403) in 543 ms on localhost (189/200)
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000191
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000191_0: Committed
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,089B for [l_orderkey] INT32: 6,858 values, 27,439B raw, 20,051B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,522B for [count_suppkey] INT32: 6,858 values, 2,596B raw, 2,486B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,248B for [max_suppkey] INT32: 6,858 values, 27,439B raw, 16,210B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO Executor: Finished task 191.0 in stage 4.0 (TID 408). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 408) in 495 ms on localhost (190/200)
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,150
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000187
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000187_0: Committed
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000190
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000190_0: Committed
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO Executor: Finished task 187.0 in stage 4.0 (TID 404). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,105B for [l_orderkey] INT32: 6,865 values, 27,467B raw, 20,067B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,436B for [count_suppkey] INT32: 6,865 values, 2,599B raw, 2,400B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,297B for [max_suppkey] INT32: 6,865 values, 27,467B raw, 16,259B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 404) in 573 ms on localhost (191/200)
15/08/16 12:51:30 INFO Executor: Finished task 190.0 in stage 4.0 (TID 407). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 407) in 538 ms on localhost (192/200)
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,798
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000188
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000188_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 188.0 in stage 4.0 (TID 405). 843 bytes result sent to driver
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,076B for [l_orderkey] INT32: 6,872 values, 27,495B raw, 20,038B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 405) in 596 ms on localhost (193/200)
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,495B for [count_suppkey] INT32: 6,872 values, 2,599B raw, 2,459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,270B for [max_suppkey] INT32: 6,872 values, 27,495B raw, 16,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000193
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000193_0: Committed
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,893,018
15/08/16 12:51:30 INFO Executor: Finished task 193.0 in stage 4.0 (TID 410). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 410) in 531 ms on localhost (194/200)
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,304B for [l_orderkey] INT32: 6,931 values, 27,731B raw, 20,266B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,526B for [count_suppkey] INT32: 6,931 values, 2,623B raw, 2,490B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,390B for [max_suppkey] INT32: 6,931 values, 27,731B raw, 16,352B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,222
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000194
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000194_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 194.0 in stage 4.0 (TID 411). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 411) in 478 ms on localhost (195/200)
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,002B for [l_orderkey] INT32: 6,833 values, 27,339B raw, 19,964B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,437B for [count_suppkey] INT32: 6,833 values, 2,587B raw, 2,401B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,212B for [max_suppkey] INT32: 6,833 values, 27,339B raw, 16,174B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,278
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,149B for [l_orderkey] INT32: 6,897 values, 27,595B raw, 20,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,522B for [count_suppkey] INT32: 6,897 values, 2,611B raw, 2,486B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,290B for [max_suppkey] INT32: 6,897 values, 27,595B raw, 16,252B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000196
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000196_0: Committed
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO Executor: Finished task 196.0 in stage 4.0 (TID 413). 843 bytes result sent to driver
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 413) in 460 ms on localhost (196/200)
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000195
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000195_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 195.0 in stage 4.0 (TID 412). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 412) in 536 ms on localhost (197/200)
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,446
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:51:30 INFO CodecConfig: Compression: GZIP
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:51:30 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:51:30 INFO ParquetOutputFormat: Validation is off
15/08/16 12:51:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,169B for [l_orderkey] INT32: 6,889 values, 27,563B raw, 20,131B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,500B for [count_suppkey] INT32: 6,889 values, 2,608B raw, 2,464B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,340B for [max_suppkey] INT32: 6,889 values, 27,563B raw, 16,302B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000197
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000197_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 197.0 in stage 4.0 (TID 414). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 414) in 387 ms on localhost (198/200)
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,891,282
15/08/16 12:51:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 33,892,522
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,048B for [l_orderkey] INT32: 6,845 values, 27,387B raw, 20,010B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,481B for [count_suppkey] INT32: 6,845 values, 2,590B raw, 2,445B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,162B for [max_suppkey] INT32: 6,845 values, 27,387B raw, 16,124B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 20,212B for [l_orderkey] INT32: 6,897 values, 27,595B raw, 20,174B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 2,599B for [count_suppkey] INT32: 6,897 values, 2,611B raw, 2,563B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 28B raw, 7B comp}
15/08/16 12:51:30 INFO ColumnChunkPageWriteStore: written 16,337B for [max_suppkey] INT32: 6,897 values, 27,595B raw, 16,299B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000199
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000199_0: Committed
15/08/16 12:51:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508161251_0004_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_temporary/0/task_201508161251_0004_m_000198
15/08/16 12:51:30 INFO SparkHadoopMapRedUtil: attempt_201508161251_0004_m_000198_0: Committed
15/08/16 12:51:30 INFO Executor: Finished task 199.0 in stage 4.0 (TID 416). 843 bytes result sent to driver
15/08/16 12:51:30 INFO Executor: Finished task 198.0 in stage 4.0 (TID 415). 843 bytes result sent to driver
15/08/16 12:51:30 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 416) in 359 ms on localhost (199/200)
15/08/16 12:51:30 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 415) in 368 ms on localhost (200/200)
15/08/16 12:51:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/16 12:51:30 INFO DAGScheduler: ResultStage 4 (processCmd at CliDriver.java:423) finished in 7.700 s
15/08/16 12:51:30 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@eb4dd16
15/08/16 12:51:30 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 14.949494 s
15/08/16 12:51:30 INFO StatsReportListener: task runtime:(count: 200, mean: 605.295000, stdev: 183.040318, max: 1556.000000, min: 359.000000)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	359.0 ms	400.0 ms	448.0 ms	495.0 ms	566.0 ms	641.0 ms	950.0 ms	1.0 s	1.6 s
15/08/16 12:51:30 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.085000, stdev: 0.278882, max: 1.000000, min: 0.000000)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms
15/08/16 12:51:30 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:51:30 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/16 12:51:30 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 89.336997, stdev: 9.647840, max: 97.470817, min: 35.599622)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	36 %	70 %	74 %	90 %	92 %	94 %	95 %	97 %	97 %
15/08/16 12:51:30 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.014989, stdev: 0.050701, max: 0.255102, min: 0.000000)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/16 12:51:30 INFO StatsReportListener: other time pct: (count: 200, mean: 10.648014, stdev: 9.653141, max: 64.400378, min: 2.529183)
15/08/16 12:51:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:30 INFO StatsReportListener: 	 3 %	 4 %	 5 %	 6 %	 8 %	10 %	27 %	30 %	64 %
15/08/16 12:51:31 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:31 INFO DefaultWriterContainer: Job job_201508161251_0000 committed.
15/08/16 12:51:31 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:31 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/_common_metadata
15/08/16 12:51:32 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/16 12:51:32 INFO DAGScheduler: Final stage: ResultStage 5(processCmd at CliDriver.java:423)
15/08/16 12:51:32 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:51:32 INFO DAGScheduler: Missing parents: List()
15/08/16 12:51:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(2824) called with curMem=392261, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 2.8 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(1676) called with curMem=395085, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1676.0 B, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:36543 (size: 1676.0 B, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at processCmd at CliDriver.java:423)
15/08/16 12:51:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/08/16 12:51:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 417, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/16 12:51:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 417)
15/08/16 12:51:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 417). 606 bytes result sent to driver
15/08/16 12:51:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 417) in 8 ms on localhost (1/1)
15/08/16 12:51:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/16 12:51:32 INFO DAGScheduler: ResultStage 5 (processCmd at CliDriver.java:423) finished in 0.010 s
15/08/16 12:51:32 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2d04faf2
15/08/16 12:51:32 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 0.020191 s
15/08/16 12:51:32 INFO StatsReportListener: task runtime:(count: 1, mean: 8.000000, stdev: 0.000000, max: 8.000000, min: 8.000000)
15/08/16 12:51:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:32 INFO StatsReportListener: 	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms
Time taken: 16.704 seconds
15/08/16 12:51:32 INFO CliDriver: Time taken: 16.704 seconds
15/08/16 12:51:32 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/16 12:51:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:32 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/16 12:51:32 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:51:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:32 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/16 12:51:32 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/16 12:51:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:32 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/16 12:51:32 INFO ParseDriver: Parsing command: insert into table q21_suppliers_who_kept_orders_waiting_par
select
  s_name, count(1) as numwait
from
  (select s_name from
(select s_name, t2.l_orderkey, l_suppkey, count_suppkey, max_suppkey 
 from q21_tmp2_par t2 right outer join
      (select s_name, l_orderkey, l_suppkey from
         (select s_name, t1.l_orderkey, l_suppkey, count_suppkey, max_suppkey
          from
            q21_tmp1_par t1 join
            (select s_name, l_orderkey, l_suppkey
             from 
               orders_par o join
               (select s_name, l_orderkey, l_suppkey
                from
                  nation_par n join supplier_par s
                  on
                    s.s_nationkey = n.n_nationkey
                    and n.n_name = 'CHINA'
                  join lineitem_par l
                  on
                    s.s_suppkey = l.l_suppkey
                where
                  l.l_receiptdate > l.l_commitdate
                ) l1 on o.o_orderkey = l1.l_orderkey and o.o_orderstatus = 'F'
             ) l2 on l2.l_orderkey = t1.l_orderkey
          ) a
          where
           (count_suppkey > 1) or ((count_suppkey=1) and (l_suppkey <> max_suppkey))
       ) l3 on l3.l_orderkey = t2.l_orderkey
    ) b
    where
     (count_suppkey is null) or ((count_suppkey=1) and (l_suppkey = max_suppkey))
  )c
group by s_name
order by numwait desc, s_name
limit 100
15/08/16 12:51:32 INFO ParseDriver: Parse Completed
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: reading another 8 footers
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: reading another 1 footers
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO ParquetFileReader: reading another 1 footers
15/08/16 12:51:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=396761, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=654289, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 8 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=677082, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=934610, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 9 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=957403, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1214931, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 10 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=1237724, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1495252, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 11 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=1518045, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1775573, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 12 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=1798366, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/16 12:51:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=2055894, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:36543 (size: 22.3 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/16 12:51:32 INFO DAGScheduler: Got job 4 (run at ThreadPoolExecutor.java:1145) with 1 output partitions (allowLocal=false)
15/08/16 12:51:32 INFO DAGScheduler: Final stage: ResultStage 6(run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:32 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:51:32 INFO SparkContext: Created broadcast 13 from processCmd at CliDriver.java:423
15/08/16 12:51:32 INFO DAGScheduler: Missing parents: List()
15/08/16 12:51:32 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[32] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(4432) called with curMem=2078687, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.3 KB, free 3.1 GB)
15/08/16 12:51:32 INFO MemoryStore: ensureFreeSpace(2505) called with curMem=2083119, maxMem=3333968363
15/08/16 12:51:32 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.4 KB, free 3.1 GB)
15/08/16 12:51:32 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:36543 (size: 2.4 KB, free: 3.1 GB)
15/08/16 12:51:32 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[32] at run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:32 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/16 12:51:32 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 418, localhost, ANY, 1725 bytes)
15/08/16 12:51:32 INFO Executor: Running task 0.0 in stage 6.0 (TID 418)
15/08/16 12:51:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 809789 length: 809789 hosts: [] requestedSchema: message root {
  optional binary s_name (UTF8);
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:32 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:32 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:32 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10000 records.
15/08/16 12:51:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 10000
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:33 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 418). 315156 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 418) in 158 ms on localhost (1/1)
15/08/16 12:51:33 INFO DAGScheduler: ResultStage 6 (run at ThreadPoolExecutor.java:1145) finished in 0.158 s
15/08/16 12:51:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/16 12:51:33 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2ecc8bdb
15/08/16 12:51:33 INFO StatsReportListener: task runtime:(count: 1, mean: 158.000000, stdev: 0.000000, max: 158.000000, min: 158.000000)
15/08/16 12:51:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:33 INFO StatsReportListener: 	158.0 ms	158.0 ms	158.0 ms	158.0 ms	158.0 ms	158.0 ms	158.0 ms	158.0 ms	158.0 ms
15/08/16 12:51:33 INFO DAGScheduler: Job 4 finished: run at ThreadPoolExecutor.java:1145, took 0.178832 s
15/08/16 12:51:33 INFO StatsReportListener: task result size:(count: 1, mean: 315156.000000, stdev: 0.000000, max: 315156.000000, min: 315156.000000)
15/08/16 12:51:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:33 INFO StatsReportListener: 	307.8 KB	307.8 KB	307.8 KB	307.8 KB	307.8 KB	307.8 KB	307.8 KB	307.8 KB	307.8 KB
15/08/16 12:51:33 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 41.772152, stdev: 0.000000, max: 41.772152, min: 41.772152)
15/08/16 12:51:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:33 INFO StatsReportListener: 	42 %	42 %	42 %	42 %	42 %	42 %	42 %	42 %	42 %
15/08/16 12:51:33 INFO StatsReportListener: other time pct: (count: 1, mean: 58.227848, stdev: 0.000000, max: 58.227848, min: 58.227848)
15/08/16 12:51:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:33 INFO StatsReportListener: 	58 %	58 %	58 %	58 %	58 %	58 %	58 %	58 %	58 %
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(65640) called with curMem=2085624, maxMem=3333968363
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 64.1 KB, free 3.1 GB)
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(85049) called with curMem=2151264, maxMem=3333968363
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 83.1 KB, free 3.1 GB)
15/08/16 12:51:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:36543 (size: 83.1 KB, free: 3.1 GB)
15/08/16 12:51:33 INFO SparkContext: Created broadcast 15 from run at ThreadPoolExecutor.java:1145
15/08/16 12:51:33 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/16 12:51:33 INFO DAGScheduler: Got job 5 (run at ThreadPoolExecutor.java:1145) with 200 output partitions (allowLocal=false)
15/08/16 12:51:33 INFO DAGScheduler: Final stage: ResultStage 7(run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:33 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:51:33 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/16 12:51:33 INFO DAGScheduler: Missing parents: List()
15/08/16 12:51:33 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(4208) called with curMem=2236313, maxMem=3333968363
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(2380) called with curMem=2240521, maxMem=3333968363
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.3 KB, free 3.1 GB)
15/08/16 12:51:33 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:36543 (size: 2.3 KB, free: 3.1 GB)
15/08/16 12:51:33 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:33 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/16 12:51:33 INFO DAGScheduler: Got job 6 (run at ThreadPoolExecutor.java:1145) with 200 output partitions (allowLocal=false)
15/08/16 12:51:33 INFO DAGScheduler: Final stage: ResultStage 8(run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:33 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:51:33 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 419, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 420, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 421, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 422, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 423, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 424, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO DAGScheduler: Missing parents: List()
15/08/16 12:51:33 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 425, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/16 12:51:33 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 426, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 427, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(4208) called with curMem=2242901, maxMem=3333968363
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 428, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 429, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 430, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 431, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 432, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 433, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO MemoryStore: ensureFreeSpace(2380) called with curMem=2247109, maxMem=3333968363
15/08/16 12:51:33 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 434, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.3 KB, free 3.1 GB)
15/08/16 12:51:33 INFO Executor: Running task 2.0 in stage 7.0 (TID 421)
15/08/16 12:51:33 INFO Executor: Running task 0.0 in stage 7.0 (TID 419)
15/08/16 12:51:33 INFO Executor: Running task 6.0 in stage 7.0 (TID 425)
15/08/16 12:51:33 INFO Executor: Running task 8.0 in stage 7.0 (TID 427)
15/08/16 12:51:33 INFO Executor: Running task 13.0 in stage 7.0 (TID 432)
15/08/16 12:51:33 INFO Executor: Running task 12.0 in stage 7.0 (TID 431)
15/08/16 12:51:33 INFO Executor: Running task 4.0 in stage 7.0 (TID 423)
15/08/16 12:51:33 INFO Executor: Running task 3.0 in stage 7.0 (TID 422)
15/08/16 12:51:33 INFO Executor: Running task 10.0 in stage 7.0 (TID 429)
15/08/16 12:51:33 INFO Executor: Running task 1.0 in stage 7.0 (TID 420)
15/08/16 12:51:33 INFO Executor: Running task 5.0 in stage 7.0 (TID 424)
15/08/16 12:51:33 INFO Executor: Running task 9.0 in stage 7.0 (TID 428)
15/08/16 12:51:33 INFO Executor: Running task 11.0 in stage 7.0 (TID 430)
15/08/16 12:51:33 INFO Executor: Running task 7.0 in stage 7.0 (TID 426)
15/08/16 12:51:33 INFO Executor: Running task 15.0 in stage 7.0 (TID 434)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00072-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47915 length: 47915 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00100-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47882 length: 47882 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 14.0 in stage 7.0 (TID 433)
15/08/16 12:51:33 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:36543 (size: 2.3 KB, free: 3.1 GB)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00146-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47935 length: 47935 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00031-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48166 length: 48166 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00133-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48077 length: 48077 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00137-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47831 length: 47831 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00047-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47989 length: 47989 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00090-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00179-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48009 length: 48009 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00177-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47944 length: 47944 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00199-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48042 length: 48042 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00099-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48067 length: 48067 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00082-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48019 length: 48019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00052-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00073-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47906 length: 47906 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00121-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at run at ThreadPoolExecutor.java:1145)
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 5.0 in stage 7.0 (TID 424). 117386 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 8.0 in stage 7.0 (TID 427). 117457 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 435, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 6.0 in stage 7.0 (TID 425). 117453 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 16.0 in stage 7.0 (TID 435)
15/08/16 12:51:33 INFO Executor: Finished task 1.0 in stage 7.0 (TID 420). 117488 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 436, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 9.0 in stage 7.0 (TID 428). 117499 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 17.0 in stage 7.0 (TID 436)
15/08/16 12:51:33 INFO Executor: Finished task 11.0 in stage 7.0 (TID 430). 117485 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 419). 117465 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 13.0 in stage 7.0 (TID 432). 117468 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 4.0 in stage 7.0 (TID 423). 117445 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 437, localhost, ANY, 1707 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 15.0 in stage 7.0 (TID 434). 117464 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 14.0 in stage 7.0 (TID 433). 117521 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 18.0 in stage 7.0 (TID 437)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00126-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48186 length: 48186 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 438, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 3.0 in stage 7.0 (TID 422). 117426 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00095-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48098 length: 48098 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00153-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47882 length: 47882 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 439, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 19.0 in stage 7.0 (TID 438)
15/08/16 12:51:33 INFO Executor: Finished task 12.0 in stage 7.0 (TID 431). 117455 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 20.0 in stage 7.0 (TID 439)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 440, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 441, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 22.0 in stage 7.0 (TID 441)
15/08/16 12:51:33 INFO Executor: Running task 21.0 in stage 7.0 (TID 440)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00163-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48029 length: 48029 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 442, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 23.0 in stage 7.0 (TID 442)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00002-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 10.0 in stage 7.0 (TID 429). 117476 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 7.0 in stage 7.0 (TID 426). 117394 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 443, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 24.0 in stage 7.0 (TID 443)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00149-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47965 length: 47965 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00086-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47907 length: 47907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 444, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 25.0 in stage 7.0 (TID 444)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 445, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 26.0 in stage 7.0 (TID 445)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 446, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 27.0 in stage 7.0 (TID 446)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00070-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48123 length: 48123 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 2.0 in stage 7.0 (TID 421). 117425 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00184-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47922 length: 47922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00096-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47866 length: 47866 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00054-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 447, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00061-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48061 length: 48061 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 28.0 in stage 7.0 (TID 447)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00081-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48073 length: 48073 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 448, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 29.0 in stage 7.0 (TID 448)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 449, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 30.0 in stage 7.0 (TID 449)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00083-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47965 length: 47965 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00025-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48025 length: 48025 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 424) in 105 ms on localhost (1/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 427) in 105 ms on localhost (2/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 425) in 107 ms on localhost (3/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 450, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 31.0 in stage 7.0 (TID 450)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 420) in 113 ms on localhost (4/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00144-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47983 length: 47983 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 18.0 in stage 7.0 (TID 437). 117352 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 451, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 16.0 in stage 7.0 (TID 435). 117383 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 32.0 in stage 7.0 (TID 451)
15/08/16 12:51:33 INFO Executor: Finished task 17.0 in stage 7.0 (TID 436). 117507 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 452, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00109-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48144 length: 48144 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 33.0 in stage 7.0 (TID 452)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 453, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00009-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48028 length: 48028 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 24.0 in stage 7.0 (TID 443). 117520 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 34.0 in stage 7.0 (TID 453)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00066-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 20.0 in stage 7.0 (TID 439). 117402 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 454, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 19.0 in stage 7.0 (TID 438). 117418 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 23.0 in stage 7.0 (TID 442). 117461 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 26.0 in stage 7.0 (TID 445). 117487 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 430) in 123 ms on localhost (5/200)
15/08/16 12:51:33 INFO Executor: Running task 35.0 in stage 7.0 (TID 454)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 455, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 432) in 129 ms on localhost (6/200)
15/08/16 12:51:33 INFO Executor: Running task 36.0 in stage 7.0 (TID 455)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 428) in 131 ms on localhost (7/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00124-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 419) in 138 ms on localhost (8/200)
15/08/16 12:51:33 INFO Executor: Finished task 30.0 in stage 7.0 (TID 449). 117373 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00141-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47979 length: 47979 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 29.0 in stage 7.0 (TID 448). 117429 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 21.0 in stage 7.0 (TID 440). 117495 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 28.0 in stage 7.0 (TID 447). 117305 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 27.0 in stage 7.0 (TID 446). 117463 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 22.0 in stage 7.0 (TID 441). 117387 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 456, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 37.0 in stage 7.0 (TID 456)
15/08/16 12:51:33 INFO Executor: Finished task 33.0 in stage 7.0 (TID 452). 117336 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 457, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 38.0 in stage 7.0 (TID 457)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 458, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 31.0 in stage 7.0 (TID 450). 117481 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 39.0 in stage 7.0 (TID 458)
15/08/16 12:51:33 INFO Executor: Finished task 32.0 in stage 7.0 (TID 451). 117471 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00008-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00064-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47903 length: 47903 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 25.0 in stage 7.0 (TID 444). 117459 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 459, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00166-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47957 length: 47957 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 460, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 41.0 in stage 7.0 (TID 460)
15/08/16 12:51:33 INFO Executor: Running task 40.0 in stage 7.0 (TID 459)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 461, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00048-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48060 length: 48060 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 42.0 in stage 7.0 (TID 461)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00196-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48084 length: 48084 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 462, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 43.0 in stage 7.0 (TID 462)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 422) in 149 ms on localhost (9/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 434) in 143 ms on localhost (10/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00113-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47998 length: 47998 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00187-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47795 length: 47795 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 34.0 in stage 7.0 (TID 453). 117384 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 423) in 151 ms on localhost (11/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 433) in 147 ms on localhost (12/200)
15/08/16 12:51:33 INFO Executor: Finished task 36.0 in stage 7.0 (TID 455). 117452 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 35.0 in stage 7.0 (TID 454). 117494 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 463, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 431) in 151 ms on localhost (13/200)
15/08/16 12:51:33 INFO Executor: Running task 44.0 in stage 7.0 (TID 463)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 464, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 45.0 in stage 7.0 (TID 464)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 429) in 152 ms on localhost (14/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00050-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47970 length: 47970 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 465, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00069-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48075 length: 48075 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 46.0 in stage 7.0 (TID 465)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 466, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 47.0 in stage 7.0 (TID 466)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 426) in 158 ms on localhost (15/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00105-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47908 length: 47908 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00088-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48109 length: 48109 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 467, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 48.0 in stage 7.0 (TID 467)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00097-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48044 length: 48044 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 468, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Running task 49.0 in stage 7.0 (TID 468)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00102-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47936 length: 47936 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 437) in 92 ms on localhost (16/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 436) in 95 ms on localhost (17/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 435) in 97 ms on localhost (18/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 421) in 171 ms on localhost (19/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 38.0 in stage 7.0 (TID 457). 117498 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 469, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 50.0 in stage 7.0 (TID 469)
15/08/16 12:51:33 INFO Executor: Finished task 40.0 in stage 7.0 (TID 459). 117420 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 470, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 41.0 in stage 7.0 (TID 460). 117390 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 471, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 37.0 in stage 7.0 (TID 456). 117498 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 52.0 in stage 7.0 (TID 471)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 439) in 100 ms on localhost (20/200)
15/08/16 12:51:33 INFO Executor: Running task 51.0 in stage 7.0 (TID 470)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00161-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47945 length: 47945 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 39.0 in stage 7.0 (TID 458). 117469 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 438) in 105 ms on localhost (21/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00107-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47966 length: 47966 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 443) in 90 ms on localhost (22/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00114-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48019 length: 48019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 47.0 in stage 7.0 (TID 466). 117430 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 472, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 45.0 in stage 7.0 (TID 464). 117418 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 53.0 in stage 7.0 (TID 472)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 42.0 in stage 7.0 (TID 461). 117511 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 473, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 44.0 in stage 7.0 (TID 463). 117483 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00140-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48185 length: 48185 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 474, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 55.0 in stage 7.0 (TID 474)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 442) in 98 ms on localhost (23/200)
15/08/16 12:51:33 INFO Executor: Running task 54.0 in stage 7.0 (TID 473)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 445) in 92 ms on localhost (24/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00139-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48029 length: 48029 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00181-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48116 length: 48116 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 475, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 56.0 in stage 7.0 (TID 475)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 476, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 57.0 in stage 7.0 (TID 476)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 448) in 88 ms on localhost (25/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00197-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48048 length: 48048 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 49.0 in stage 7.0 (TID 468). 117457 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 440) in 105 ms on localhost (26/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00077-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48216 length: 48216 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 43.0 in stage 7.0 (TID 462). 117405 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 477, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 58.0 in stage 7.0 (TID 477)
15/08/16 12:51:33 INFO Executor: Finished task 48.0 in stage 7.0 (TID 467). 117428 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 478, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 59.0 in stage 7.0 (TID 478)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00085-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48033 length: 48033 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00154-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48005 length: 48005 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 479, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 60.0 in stage 7.0 (TID 479)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 480, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 61.0 in stage 7.0 (TID 480)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 447) in 102 ms on localhost (27/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00103-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 449) in 96 ms on localhost (28/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 481, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00118-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 62.0 in stage 7.0 (TID 481)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 482, localhost, ANY, 1707 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 446) in 107 ms on localhost (29/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00089-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 63.0 in stage 7.0 (TID 482)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 46.0 in stage 7.0 (TID 465). 117473 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00156-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48083 length: 48083 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 483, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 64.0 in stage 7.0 (TID 483)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 484, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 65.0 in stage 7.0 (TID 484)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00170-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48236 length: 48236 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 441) in 120 ms on localhost (30/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 451) in 91 ms on localhost (31/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00174-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48147 length: 48147 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 450) in 100 ms on localhost (32/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 452) in 91 ms on localhost (33/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 51.0 in stage 7.0 (TID 470). 117439 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 485, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 52.0 in stage 7.0 (TID 471). 117440 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 50.0 in stage 7.0 (TID 469). 117435 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 66.0 in stage 7.0 (TID 485)
15/08/16 12:51:33 INFO Executor: Finished task 55.0 in stage 7.0 (TID 474). 117483 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 54.0 in stage 7.0 (TID 473). 117444 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 486, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 67.0 in stage 7.0 (TID 486)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 455) in 84 ms on localhost (34/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00164-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00167-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47969 length: 47969 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 487, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 68.0 in stage 7.0 (TID 487)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 488, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 69.0 in stage 7.0 (TID 488)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00091-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47937 length: 47937 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 489, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 444) in 128 ms on localhost (35/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 454) in 97 ms on localhost (36/200)
15/08/16 12:51:33 INFO Executor: Running task 70.0 in stage 7.0 (TID 489)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 453) in 103 ms on localhost (37/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00110-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48022 length: 48022 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 53.0 in stage 7.0 (TID 472). 117351 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 57.0 in stage 7.0 (TID 476). 117376 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 59.0 in stage 7.0 (TID 478). 117504 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00112-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47912 length: 47912 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 56.0 in stage 7.0 (TID 475). 117466 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 62.0 in stage 7.0 (TID 481). 117444 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 460) in 85 ms on localhost (38/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 60.0 in stage 7.0 (TID 479). 117427 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 58.0 in stage 7.0 (TID 477). 117417 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 490, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 71.0 in stage 7.0 (TID 490)
15/08/16 12:51:33 INFO Executor: Finished task 65.0 in stage 7.0 (TID 484). 117374 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 491, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 61.0 in stage 7.0 (TID 480). 117429 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 72.0 in stage 7.0 (TID 491)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 492, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00010-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 73.0 in stage 7.0 (TID 492)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 493, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00172-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48190 length: 48190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 74.0 in stage 7.0 (TID 493)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 459) in 94 ms on localhost (39/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00151-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48021 length: 48021 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 456) in 97 ms on localhost (40/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00037-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 494, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 75.0 in stage 7.0 (TID 494)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 457) in 98 ms on localhost (41/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00148-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48070 length: 48070 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 63.0 in stage 7.0 (TID 482). 117425 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 495, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 458) in 102 ms on localhost (42/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 76.0 in stage 7.0 (TID 495)
15/08/16 12:51:33 INFO Executor: Finished task 67.0 in stage 7.0 (TID 486). 117459 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 496, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00017-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 66.0 in stage 7.0 (TID 485). 117408 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 77.0 in stage 7.0 (TID 496)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 466) in 85 ms on localhost (43/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00191-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47993 length: 47993 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 464) in 94 ms on localhost (44/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 461) in 105 ms on localhost (45/200)
15/08/16 12:51:33 INFO Executor: Finished task 71.0 in stage 7.0 (TID 490). 117400 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 64.0 in stage 7.0 (TID 483). 117330 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 69.0 in stage 7.0 (TID 488). 117385 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 497, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 70.0 in stage 7.0 (TID 489). 117475 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 78.0 in stage 7.0 (TID 497)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 498, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 68.0 in stage 7.0 (TID 487). 117433 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 72.0 in stage 7.0 (TID 491). 117406 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 79.0 in stage 7.0 (TID 498)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 468) in 89 ms on localhost (46/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 463) in 101 ms on localhost (47/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00045-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48006 length: 48006 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 467) in 93 ms on localhost (48/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00158-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48013 length: 48013 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 74.0 in stage 7.0 (TID 493). 117406 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 462) in 111 ms on localhost (49/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 499, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 75.0 in stage 7.0 (TID 494). 117422 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 80.0 in stage 7.0 (TID 499)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 500, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 81.0 in stage 7.0 (TID 500)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00060-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48068 length: 48068 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 73.0 in stage 7.0 (TID 492). 117447 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00094-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48140 length: 48140 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 501, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 82.0 in stage 7.0 (TID 501)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 471) in 82 ms on localhost (50/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 470) in 84 ms on localhost (51/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00058-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48097 length: 48097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 502, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 469) in 86 ms on localhost (52/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 83.0 in stage 7.0 (TID 502)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 503, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 84.0 in stage 7.0 (TID 503)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00041-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48006 length: 48006 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00078-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48190 length: 48190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 465) in 112 ms on localhost (53/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 504, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 77.0 in stage 7.0 (TID 496). 117433 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 85.0 in stage 7.0 (TID 504)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00019-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47959 length: 47959 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 505, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 76.0 in stage 7.0 (TID 495). 117395 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 474) in 88 ms on localhost (54/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 472) in 91 ms on localhost (55/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 473) in 90 ms on localhost (56/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 506, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 87.0 in stage 7.0 (TID 506)
15/08/16 12:51:33 INFO Executor: Finished task 79.0 in stage 7.0 (TID 498). 117430 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 507, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00022-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47954 length: 47954 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 78.0 in stage 7.0 (TID 497). 117458 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 86.0 in stage 7.0 (TID 505)
15/08/16 12:51:33 INFO Executor: Running task 88.0 in stage 7.0 (TID 507)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 82.0 in stage 7.0 (TID 501). 117406 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 508, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 89.0 in stage 7.0 (TID 508)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00076-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47868 length: 47868 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 476) in 91 ms on localhost (57/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00155-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 478) in 87 ms on localhost (58/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00039-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 81.0 in stage 7.0 (TID 500). 117419 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 481) in 83 ms on localhost (59/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 509, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 90.0 in stage 7.0 (TID 509)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 475) in 97 ms on localhost (60/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 80.0 in stage 7.0 (TID 499). 117378 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 84.0 in stage 7.0 (TID 503). 117394 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00042-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48049 length: 48049 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 510, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 91.0 in stage 7.0 (TID 510)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 477) in 96 ms on localhost (61/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00000-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48091 length: 48091 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 511, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 92.0 in stage 7.0 (TID 511)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 512, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 93.0 in stage 7.0 (TID 512)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 484) in 88 ms on localhost (62/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 479) in 98 ms on localhost (63/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00193-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48009 length: 48009 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00152-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 83.0 in stage 7.0 (TID 502). 117441 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 513, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 94.0 in stage 7.0 (TID 513)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 480) in 98 ms on localhost (64/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 482) in 94 ms on localhost (65/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00173-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47922 length: 47922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 514, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 95.0 in stage 7.0 (TID 514)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 85.0 in stage 7.0 (TID 504). 117462 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 515, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00040-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48033 length: 48033 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 96.0 in stage 7.0 (TID 515)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 486) in 85 ms on localhost (66/200)
15/08/16 12:51:33 INFO Executor: Finished task 87.0 in stage 7.0 (TID 506). 117401 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00186-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47910 length: 47910 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 516, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 517, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 97.0 in stage 7.0 (TID 516)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 86.0 in stage 7.0 (TID 505). 117498 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00104-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48022 length: 48022 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 89.0 in stage 7.0 (TID 508). 117398 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 98.0 in stage 7.0 (TID 517)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 488) in 90 ms on localhost (67/200)
15/08/16 12:51:33 INFO Executor: Finished task 88.0 in stage 7.0 (TID 507). 117433 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 485) in 96 ms on localhost (68/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 490) in 82 ms on localhost (69/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 483) in 106 ms on localhost (70/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00036-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48003 length: 48003 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 518, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 99.0 in stage 7.0 (TID 518)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 519, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 90.0 in stage 7.0 (TID 509). 117416 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00016-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 92.0 in stage 7.0 (TID 511). 117470 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 100.0 in stage 7.0 (TID 519)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 93.0 in stage 7.0 (TID 512). 117447 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00117-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47919 length: 47919 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 94.0 in stage 7.0 (TID 513). 117530 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 95.0 in stage 7.0 (TID 514). 117468 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 520, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 101.0 in stage 7.0 (TID 520)
15/08/16 12:51:33 INFO Executor: Finished task 91.0 in stage 7.0 (TID 510). 117434 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 491) in 89 ms on localhost (71/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 487) in 102 ms on localhost (72/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00165-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47984 length: 47984 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 493) in 89 ms on localhost (73/200)
15/08/16 12:51:33 INFO Executor: Finished task 96.0 in stage 7.0 (TID 515). 117519 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 489) in 101 ms on localhost (74/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 521, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 102.0 in stage 7.0 (TID 521)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 522, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00074-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48072 length: 48072 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 103.0 in stage 7.0 (TID 522)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00171-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48141 length: 48141 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 523, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 104.0 in stage 7.0 (TID 523)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 97.0 in stage 7.0 (TID 516). 117401 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 524, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00021-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 525, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 105.0 in stage 7.0 (TID 524)
15/08/16 12:51:33 INFO Executor: Running task 106.0 in stage 7.0 (TID 525)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 99.0 in stage 7.0 (TID 518). 117481 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 492) in 100 ms on localhost (75/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 494) in 97 ms on localhost (76/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 495) in 96 ms on localhost (77/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00142-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47983 length: 47983 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00194-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47963 length: 47963 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 526, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 107.0 in stage 7.0 (TID 526)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 496) in 96 ms on localhost (78/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00123-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 100.0 in stage 7.0 (TID 519). 117439 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 527, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 108.0 in stage 7.0 (TID 527)
15/08/16 12:51:33 INFO Executor: Finished task 98.0 in stage 7.0 (TID 517). 117476 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 528, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 109.0 in stage 7.0 (TID 528)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 101.0 in stage 7.0 (TID 520). 117459 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00062-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 498) in 89 ms on localhost (79/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00023-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48153 length: 48153 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 529, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 110.0 in stage 7.0 (TID 529)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 102.0 in stage 7.0 (TID 521). 117423 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 530, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 497) in 94 ms on localhost (80/200)
15/08/16 12:51:33 INFO Executor: Running task 111.0 in stage 7.0 (TID 530)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00106-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47918 length: 47918 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 103.0 in stage 7.0 (TID 522). 117414 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00150-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47898 length: 47898 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 531, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 112.0 in stage 7.0 (TID 531)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 500) in 93 ms on localhost (81/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 501) in 91 ms on localhost (82/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 499) in 95 ms on localhost (83/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00014-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 532, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 113.0 in stage 7.0 (TID 532)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 503) in 90 ms on localhost (84/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 105.0 in stage 7.0 (TID 524). 117439 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00030-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47977 length: 47977 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 533, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 534, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 114.0 in stage 7.0 (TID 533)
15/08/16 12:51:33 INFO Executor: Running task 115.0 in stage 7.0 (TID 534)
15/08/16 12:51:33 INFO Executor: Finished task 104.0 in stage 7.0 (TID 523). 117411 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 106.0 in stage 7.0 (TID 525). 117519 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 107.0 in stage 7.0 (TID 526). 117365 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 535, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 116.0 in stage 7.0 (TID 535)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 504) in 89 ms on localhost (85/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00044-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48057 length: 48057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 506) in 81 ms on localhost (86/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00018-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47980 length: 47980 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 505) in 89 ms on localhost (87/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00175-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48135 length: 48135 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 502) in 99 ms on localhost (88/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 110.0 in stage 7.0 (TID 529). 117512 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 108.0 in stage 7.0 (TID 527). 117417 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 536, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 111.0 in stage 7.0 (TID 530). 117468 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 117.0 in stage 7.0 (TID 536)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 508) in 88 ms on localhost (89/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 507) in 91 ms on localhost (90/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00176-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 509) in 87 ms on localhost (91/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 512) in 81 ms on localhost (92/200)
15/08/16 12:51:33 INFO Executor: Finished task 112.0 in stage 7.0 (TID 531). 117443 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 537, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 109.0 in stage 7.0 (TID 528). 117370 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 513) in 81 ms on localhost (93/200)
15/08/16 12:51:33 INFO Executor: Running task 118.0 in stage 7.0 (TID 537)
15/08/16 12:51:33 INFO Executor: Finished task 113.0 in stage 7.0 (TID 532). 117440 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 538, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 514) in 81 ms on localhost (94/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00198-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47950 length: 47950 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 511) in 87 ms on localhost (95/200)
15/08/16 12:51:33 INFO Executor: Running task 119.0 in stage 7.0 (TID 538)
15/08/16 12:51:33 INFO Executor: Finished task 115.0 in stage 7.0 (TID 534). 117408 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00098-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48161 length: 48161 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 539, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 510) in 95 ms on localhost (96/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 120.0 in stage 7.0 (TID 539)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 540, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 121.0 in stage 7.0 (TID 540)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00034-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47916 length: 47916 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00035-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47992 length: 47992 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 541, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 116.0 in stage 7.0 (TID 535). 117357 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 122.0 in stage 7.0 (TID 541)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 542, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Running task 123.0 in stage 7.0 (TID 542)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00049-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47993 length: 47993 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 543, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 516) in 88 ms on localhost (97/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 114.0 in stage 7.0 (TID 533). 117402 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00006-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48079 length: 48079 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 515) in 92 ms on localhost (98/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 124.0 in stage 7.0 (TID 543)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 518) in 80 ms on localhost (99/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00053-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47947 length: 47947 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 117.0 in stage 7.0 (TID 536). 117467 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 544, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 125.0 in stage 7.0 (TID 544)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00135-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48211 length: 48211 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 545, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 517) in 95 ms on localhost (100/200)
15/08/16 12:51:33 INFO Executor: Running task 126.0 in stage 7.0 (TID 545)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 520) in 84 ms on localhost (101/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 519) in 86 ms on localhost (102/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00046-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47959 length: 47959 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 119.0 in stage 7.0 (TID 538). 117353 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 118.0 in stage 7.0 (TID 537). 117443 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 121.0 in stage 7.0 (TID 540). 117447 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 546, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 120.0 in stage 7.0 (TID 539). 117450 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 521) in 81 ms on localhost (103/200)
15/08/16 12:51:33 INFO Executor: Running task 127.0 in stage 7.0 (TID 546)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 547, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 128.0 in stage 7.0 (TID 547)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00075-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48124 length: 48124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 548, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 129.0 in stage 7.0 (TID 548)
15/08/16 12:51:33 INFO Executor: Finished task 123.0 in stage 7.0 (TID 542). 117440 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00101-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48013 length: 48013 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 549, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 122.0 in stage 7.0 (TID 541). 117358 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 130.0 in stage 7.0 (TID 549)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 522) in 82 ms on localhost (104/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 524) in 80 ms on localhost (105/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 523) in 83 ms on localhost (106/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00190-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48105 length: 48105 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00068-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47989 length: 47989 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 550, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 131.0 in stage 7.0 (TID 550)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 551, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 132.0 in stage 7.0 (TID 551)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 552, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 133.0 in stage 7.0 (TID 552)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 125.0 in stage 7.0 (TID 544). 117429 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00138-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48051 length: 48051 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00093-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48047 length: 48047 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00119-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48129 length: 48129 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 553, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 134.0 in stage 7.0 (TID 553)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 554, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 135.0 in stage 7.0 (TID 554)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00024-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 555, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 136.0 in stage 7.0 (TID 555)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 525) in 88 ms on localhost (107/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 527) in 81 ms on localhost (108/200)
15/08/16 12:51:33 INFO Executor: Finished task 124.0 in stage 7.0 (TID 543). 117505 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00143-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48054 length: 48054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 529) in 78 ms on localhost (109/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00028-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48123 length: 48123 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 556, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 137.0 in stage 7.0 (TID 556)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 526) in 87 ms on localhost (110/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 557, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00180-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48144 length: 48144 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 138.0 in stage 7.0 (TID 557)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 558, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 139.0 in stage 7.0 (TID 558)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 531) in 79 ms on localhost (111/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00012-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48117 length: 48117 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 127.0 in stage 7.0 (TID 546). 117387 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00071-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48018 length: 48018 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 530) in 83 ms on localhost (112/200)
15/08/16 12:51:33 INFO Executor: Finished task 126.0 in stage 7.0 (TID 545). 117449 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 128.0 in stage 7.0 (TID 547). 117486 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 528) in 91 ms on localhost (113/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 559, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 140.0 in stage 7.0 (TID 559)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 532) in 82 ms on localhost (114/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 560, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00013-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47901 length: 47901 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 561, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 142.0 in stage 7.0 (TID 561)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 534) in 83 ms on localhost (115/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00122-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48119 length: 48119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 133.0 in stage 7.0 (TID 552). 117424 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 562, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 132.0 in stage 7.0 (TID 551). 117473 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 143.0 in stage 7.0 (TID 562)
15/08/16 12:51:33 INFO Executor: Finished task 130.0 in stage 7.0 (TID 549). 117453 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 141.0 in stage 7.0 (TID 560)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 563, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 144.0 in stage 7.0 (TID 563)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 533) in 88 ms on localhost (116/200)
15/08/16 12:51:33 INFO Executor: Finished task 135.0 in stage 7.0 (TID 554). 117398 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 129.0 in stage 7.0 (TID 548). 117386 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 136.0 in stage 7.0 (TID 555). 117423 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 564, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 535) in 88 ms on localhost (117/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00134-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48159 length: 48159 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00005-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48037 length: 48037 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 538) in 66 ms on localhost (118/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00001-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48017 length: 48017 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 145.0 in stage 7.0 (TID 564)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00033-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47930 length: 47930 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 565, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 139.0 in stage 7.0 (TID 558). 117449 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 146.0 in stage 7.0 (TID 565)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 566, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 147.0 in stage 7.0 (TID 566)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00157-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47985 length: 47985 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 536) in 90 ms on localhost (119/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 567, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00116-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48117 length: 48117 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 148.0 in stage 7.0 (TID 567)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 539) in 71 ms on localhost (120/200)
15/08/16 12:51:33 INFO Executor: Finished task 131.0 in stage 7.0 (TID 550). 117467 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00182-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48226 length: 48226 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 568, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 149.0 in stage 7.0 (TID 568)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 537) in 81 ms on localhost (121/200)
15/08/16 12:51:33 INFO Executor: Finished task 137.0 in stage 7.0 (TID 556). 117413 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00127-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48094 length: 48094 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 569, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 138.0 in stage 7.0 (TID 557). 117363 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 134.0 in stage 7.0 (TID 553). 117437 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 570, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 140.0 in stage 7.0 (TID 559). 117478 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Running task 150.0 in stage 7.0 (TID 569)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 542) in 74 ms on localhost (122/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 571, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00131-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48041 length: 48041 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 152.0 in stage 7.0 (TID 571)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 540) in 78 ms on localhost (123/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 572, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Running task 153.0 in stage 7.0 (TID 572)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 573, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00032-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48047 length: 48047 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 154.0 in stage 7.0 (TID 573)
15/08/16 12:51:33 INFO Executor: Finished task 144.0 in stage 7.0 (TID 563). 117411 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 574, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 142.0 in stage 7.0 (TID 561). 117412 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 575, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00029-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 156.0 in stage 7.0 (TID 575)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 151.0 in stage 7.0 (TID 570)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00056-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48000 length: 48000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 576, localhost, ANY, 1707 bytes)
15/08/16 12:51:33 INFO Executor: Running task 157.0 in stage 7.0 (TID 576)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 577, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 141.0 in stage 7.0 (TID 560). 117415 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 158.0 in stage 7.0 (TID 577)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00055-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48082 length: 48082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00051-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48133 length: 48133 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00147-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48124 length: 48124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 155.0 in stage 7.0 (TID 574)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00020-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47891 length: 47891 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 544) in 76 ms on localhost (124/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00168-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47995 length: 47995 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 143.0 in stage 7.0 (TID 562). 117398 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 145.0 in stage 7.0 (TID 564). 117527 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 578, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 148.0 in stage 7.0 (TID 567). 117319 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 159.0 in stage 7.0 (TID 578)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 543) in 82 ms on localhost (125/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 541) in 87 ms on localhost (126/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00087-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48107 length: 48107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 579, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 147.0 in stage 7.0 (TID 566). 117398 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 546) in 73 ms on localhost (127/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 160.0 in stage 7.0 (TID 579)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 580, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 581, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 161.0 in stage 7.0 (TID 580)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00059-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47995 length: 47995 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 149.0 in stage 7.0 (TID 568). 117463 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 582, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 146.0 in stage 7.0 (TID 565). 117434 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00162-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48057 length: 48057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 162.0 in stage 7.0 (TID 581)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 163.0 in stage 7.0 (TID 582)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 583, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 551) in 71 ms on localhost (128/200)
15/08/16 12:51:33 INFO Executor: Running task 164.0 in stage 7.0 (TID 583)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 552) in 71 ms on localhost (129/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 152.0 in stage 7.0 (TID 571). 117490 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00130-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00004-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48048 length: 48048 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 150.0 in stage 7.0 (TID 569). 117427 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 545) in 90 ms on localhost (130/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00195-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47873 length: 47873 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 549) in 80 ms on localhost (131/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 154.0 in stage 7.0 (TID 573). 117432 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 547) in 85 ms on localhost (132/200)
15/08/16 12:51:33 INFO Executor: Finished task 157.0 in stage 7.0 (TID 576). 117396 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 554) in 73 ms on localhost (133/200)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 584, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 158.0 in stage 7.0 (TID 577). 117415 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 165.0 in stage 7.0 (TID 584)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 585, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Running task 166.0 in stage 7.0 (TID 585)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 586, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00185-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47875 length: 47875 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 587, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00111-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48164 length: 48164 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 151.0 in stage 7.0 (TID 570). 117484 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 167.0 in stage 7.0 (TID 586)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 588, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 160.0 in stage 7.0 (TID 579). 117495 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 548) in 91 ms on localhost (134/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 156.0 in stage 7.0 (TID 575). 117407 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00011-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48139 length: 48139 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 168.0 in stage 7.0 (TID 587)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 555) in 79 ms on localhost (135/200)
15/08/16 12:51:33 INFO Executor: Running task 169.0 in stage 7.0 (TID 588)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 159.0 in stage 7.0 (TID 578). 117416 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 153.0 in stage 7.0 (TID 572). 117430 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 155.0 in stage 7.0 (TID 574). 117440 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 589, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00178-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47925 length: 47925 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 550) in 91 ms on localhost (136/200)
15/08/16 12:51:33 INFO Executor: Running task 170.0 in stage 7.0 (TID 589)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00108-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48056 length: 48056 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 590, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 171.0 in stage 7.0 (TID 590)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 161.0 in stage 7.0 (TID 580). 117424 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00129-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47816 length: 47816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 558) in 78 ms on localhost (137/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 591, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00027-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48137 length: 48137 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 172.0 in stage 7.0 (TID 591)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 592, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 556) in 85 ms on localhost (138/200)
15/08/16 12:51:33 INFO Executor: Running task 173.0 in stage 7.0 (TID 592)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 557) in 83 ms on localhost (139/200)
15/08/16 12:51:33 INFO Executor: Finished task 162.0 in stage 7.0 (TID 581). 117359 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 593, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 164.0 in stage 7.0 (TID 583). 117489 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00007-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48056 length: 48056 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00169-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47953 length: 47953 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 594, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 174.0 in stage 7.0 (TID 593)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00132-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48097 length: 48097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 595, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO Executor: Finished task 166.0 in stage 7.0 (TID 585). 117412 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 175.0 in stage 7.0 (TID 594)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 553) in 100 ms on localhost (140/200)
15/08/16 12:51:33 INFO Executor: Running task 176.0 in stage 7.0 (TID 595)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 561) in 80 ms on localhost (141/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 559) in 83 ms on localhost (142/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00160-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48113 length: 48113 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 563) in 75 ms on localhost (143/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00003-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47982 length: 47982 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 163.0 in stage 7.0 (TID 582). 117428 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 596, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 597, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 177.0 in stage 7.0 (TID 596)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 178.0 in stage 7.0 (TID 597)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 598, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 179.0 in stage 7.0 (TID 598)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00065-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00057-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47986 length: 47986 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 599, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 180.0 in stage 7.0 (TID 599)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00015-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48168 length: 48168 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 600, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 181.0 in stage 7.0 (TID 600)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00145-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 165.0 in stage 7.0 (TID 584). 117506 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 562) in 83 ms on localhost (144/200)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 601, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 182.0 in stage 7.0 (TID 601)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 567) in 73 ms on localhost (145/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00043-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47975 length: 47975 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 168.0 in stage 7.0 (TID 587). 117395 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 169.0 in stage 7.0 (TID 588). 117481 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00125-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47829 length: 47829 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 171.0 in stage 7.0 (TID 590). 117389 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 602, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 603, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 183.0 in stage 7.0 (TID 602)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 604, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 185.0 in stage 7.0 (TID 604)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 566) in 80 ms on localhost (146/200)
15/08/16 12:51:33 INFO Executor: Finished task 167.0 in stage 7.0 (TID 586). 117385 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Running task 184.0 in stage 7.0 (TID 603)
15/08/16 12:51:33 INFO Executor: Finished task 170.0 in stage 7.0 (TID 589). 117476 bytes result sent to driver
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00084-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47854 length: 47854 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00188-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48227 length: 48227 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 568) in 77 ms on localhost (147/200)
15/08/16 12:51:33 INFO Executor: Finished task 173.0 in stage 7.0 (TID 592). 117415 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 560) in 97 ms on localhost (148/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 605, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00115-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48000 length: 48000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 186.0 in stage 7.0 (TID 605)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 606, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 187.0 in stage 7.0 (TID 606)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 174.0 in stage 7.0 (TID 593). 117406 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 607, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00026-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48203 length: 48203 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 188.0 in stage 7.0 (TID 607)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00092-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48173 length: 48173 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 172.0 in stage 7.0 (TID 591). 117449 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 608, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00120-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48011 length: 48011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 189.0 in stage 7.0 (TID 608)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 609, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 190.0 in stage 7.0 (TID 609)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00192-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48046 length: 48046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 564) in 97 ms on localhost (149/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 179.0 in stage 7.0 (TID 598). 117440 bytes result sent to driver
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 571) in 80 ms on localhost (150/200)
15/08/16 12:51:33 INFO Executor: Finished task 180.0 in stage 7.0 (TID 599). 117421 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 176.0 in stage 7.0 (TID 595). 117441 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 569) in 86 ms on localhost (151/200)
15/08/16 12:51:33 INFO Executor: Finished task 175.0 in stage 7.0 (TID 594). 117542 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 181.0 in stage 7.0 (TID 600). 117513 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 565) in 99 ms on localhost (152/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 185.0 in stage 7.0 (TID 604). 117375 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00038-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48067 length: 48067 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 610, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 177.0 in stage 7.0 (TID 596). 117431 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Running task 191.0 in stage 7.0 (TID 610)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 611, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 192.0 in stage 7.0 (TID 611)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00128-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47962 length: 47962 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 612, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 193.0 in stage 7.0 (TID 612)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 613, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00067-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48014 length: 48014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 614, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00183-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47968 length: 47968 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 195.0 in stage 7.0 (TID 614)
15/08/16 12:51:33 INFO Executor: Running task 194.0 in stage 7.0 (TID 613)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 615, localhost, ANY, 1707 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00063-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48141 length: 48141 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 576) in 89 ms on localhost (153/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00136-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48010 length: 48010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 182.0 in stage 7.0 (TID 601). 117497 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 178.0 in stage 7.0 (TID 597). 117440 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 187.0 in stage 7.0 (TID 606). 117387 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 573) in 93 ms on localhost (154/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Finished task 186.0 in stage 7.0 (TID 605). 117435 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 188.0 in stage 7.0 (TID 607). 117396 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Finished task 183.0 in stage 7.0 (TID 602). 117460 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 190.0 in stage 7.0 (TID 609). 117358 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 184.0 in stage 7.0 (TID 603). 117443 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 616, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 197.0 in stage 7.0 (TID 616)
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 617, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 618, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO Executor: Running task 199.0 in stage 7.0 (TID 618)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 619, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 0.0 in stage 8.0 (TID 619)
15/08/16 12:51:33 INFO Executor: Running task 196.0 in stage 7.0 (TID 615)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00080-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48082 length: 48082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 198.0 in stage 7.0 (TID 617)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00079-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47973 length: 47973 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO Executor: Finished task 191.0 in stage 7.0 (TID 610). 117468 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 620, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 575) in 104 ms on localhost (155/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00100-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47882 length: 47882 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00189-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Running task 1.0 in stage 8.0 (TID 620)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 570) in 109 ms on localhost (156/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 579) in 98 ms on localhost (157/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 577) in 103 ms on localhost (158/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00199-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48042 length: 48042 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00159-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48092 length: 48092 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 621, localhost, ANY, 1704 bytes)
15/08/16 12:51:33 INFO Executor: Running task 2.0 in stage 8.0 (TID 621)
15/08/16 12:51:33 INFO Executor: Finished task 192.0 in stage 7.0 (TID 611). 117442 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 622, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 3.0 in stage 8.0 (TID 622)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00121-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO Executor: Finished task 195.0 in stage 7.0 (TID 614). 117397 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 194.0 in stage 7.0 (TID 613). 117434 bytes result sent to driver
15/08/16 12:51:33 INFO Executor: Finished task 189.0 in stage 7.0 (TID 608). 117482 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 623, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00047-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47989 length: 47989 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 572) in 117 ms on localhost (159/200)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 574) in 116 ms on localhost (160/200)
15/08/16 12:51:33 INFO Executor: Running task 4.0 in stage 8.0 (TID 623)
15/08/16 12:51:33 INFO Executor: Finished task 193.0 in stage 7.0 (TID 612). 117504 bytes result sent to driver
15/08/16 12:51:33 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 578) in 110 ms on localhost (161/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 580) in 106 ms on localhost (162/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00177-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47944 length: 47944 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 624, localhost, ANY, 1705 bytes)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO Executor: Running task 5.0 in stage 8.0 (TID 624)
15/08/16 12:51:33 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 625, localhost, ANY, 1703 bytes)
15/08/16 12:51:33 INFO Executor: Running task 6.0 in stage 8.0 (TID 625)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00072-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47915 length: 47915 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 626, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 7.0 in stage 8.0 (TID 626)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00031-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48166 length: 48166 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00099-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48067 length: 48067 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 627, localhost, ANY, 1706 bytes)
15/08/16 12:51:33 INFO Executor: Running task 8.0 in stage 8.0 (TID 627)
15/08/16 12:51:33 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 582) in 117 ms on localhost (163/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 585) in 106 ms on localhost (164/200)
15/08/16 12:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00133-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48077 length: 48077 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:33 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 583) in 120 ms on localhost (165/200)
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 581) in 123 ms on localhost (166/200)
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO Executor: Finished task 0.0 in stage 8.0 (TID 619). 117465 bytes result sent to driver
15/08/16 12:51:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 628, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 629, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 10.0 in stage 8.0 (TID 629)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 584) in 133 ms on localhost (167/200)
15/08/16 12:51:34 INFO Executor: Running task 9.0 in stage 8.0 (TID 628)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 590) in 123 ms on localhost (168/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00052-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 196.0 in stage 7.0 (TID 615). 117387 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00082-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48019 length: 48019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 630, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 197.0 in stage 7.0 (TID 616). 117519 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 587) in 137 ms on localhost (169/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 631, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 12.0 in stage 8.0 (TID 631)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 589) in 133 ms on localhost (170/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 586) in 139 ms on localhost (171/200)
15/08/16 12:51:34 INFO Executor: Running task 11.0 in stage 8.0 (TID 630)
15/08/16 12:51:34 INFO Executor: Finished task 3.0 in stage 8.0 (TID 622). 117426 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00090-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 198.0 in stage 7.0 (TID 617). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 632, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 13.0 in stage 8.0 (TID 632)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00179-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48009 length: 48009 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 7.0 in stage 8.0 (TID 626). 117394 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 199.0 in stage 7.0 (TID 618). 117438 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 4.0 in stage 8.0 (TID 623). 117445 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 5.0 in stage 8.0 (TID 624). 117386 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 633, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 1.0 in stage 8.0 (TID 620). 117488 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 14.0 in stage 8.0 (TID 633)
15/08/16 12:51:34 INFO Executor: Finished task 8.0 in stage 8.0 (TID 627). 117457 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 591) in 144 ms on localhost (172/200)
15/08/16 12:51:34 INFO Executor: Finished task 2.0 in stage 8.0 (TID 621). 117425 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00146-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47935 length: 47935 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 592) in 144 ms on localhost (173/200)
15/08/16 12:51:34 INFO Executor: Finished task 6.0 in stage 8.0 (TID 625). 117453 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00073-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47906 length: 47906 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 588) in 153 ms on localhost (174/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 593) in 144 ms on localhost (175/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 634, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 598) in 136 ms on localhost (176/200)
15/08/16 12:51:34 INFO Executor: Running task 15.0 in stage 8.0 (TID 634)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00137-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47831 length: 47831 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 635, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 11.0 in stage 8.0 (TID 630). 117485 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 16.0 in stage 8.0 (TID 635)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 9.0 in stage 8.0 (TID 628). 117499 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 636, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 17.0 in stage 8.0 (TID 636)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 599) in 141 ms on localhost (177/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00126-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48186 length: 48186 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 594) in 153 ms on localhost (178/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00153-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47882 length: 47882 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 637, localhost, ANY, 1707 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 18.0 in stage 8.0 (TID 637)
15/08/16 12:51:34 INFO Executor: Finished task 10.0 in stage 8.0 (TID 629). 117476 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 595) in 153 ms on localhost (179/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 638, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00095-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48098 length: 48098 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 19.0 in stage 8.0 (TID 638)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 639, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 20.0 in stage 8.0 (TID 639)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 12.0 in stage 8.0 (TID 631). 117455 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 600) in 146 ms on localhost (180/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00163-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48029 length: 48029 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00002-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 14.0 in stage 8.0 (TID 633). 117521 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 640, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 604) in 145 ms on localhost (181/200)
15/08/16 12:51:34 INFO Executor: Running task 21.0 in stage 8.0 (TID 640)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 641, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 13.0 in stage 8.0 (TID 632). 117468 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 22.0 in stage 8.0 (TID 641)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00149-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47965 length: 47965 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 596) in 159 ms on localhost (182/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 642, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 23.0 in stage 8.0 (TID 642)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00070-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48123 length: 48123 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 15.0 in stage 8.0 (TID 634). 117464 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 643, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 601) in 156 ms on localhost (183/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 597) in 161 ms on localhost (184/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00086-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47907 length: 47907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 17.0 in stage 8.0 (TID 636). 117507 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 16.0 in stage 8.0 (TID 635). 117383 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 644, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 25.0 in stage 8.0 (TID 644)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 24.0 in stage 8.0 (TID 643)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00184-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47922 length: 47922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00054-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 645, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 26.0 in stage 8.0 (TID 645)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 607) in 149 ms on localhost (185/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 606) in 151 ms on localhost (186/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 605) in 152 ms on localhost (187/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 20.0 in stage 8.0 (TID 639). 117402 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 602) in 159 ms on localhost (188/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00096-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47866 length: 47866 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 18.0 in stage 8.0 (TID 637). 117352 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 646, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 27.0 in stage 8.0 (TID 646)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 647, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 28.0 in stage 8.0 (TID 647)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00061-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48061 length: 48061 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 648, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 29.0 in stage 8.0 (TID 648)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00081-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48073 length: 48073 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00083-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47965 length: 47965 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 21.0 in stage 8.0 (TID 640). 117495 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 649, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 30.0 in stage 8.0 (TID 649)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 19.0 in stage 8.0 (TID 638). 117418 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 650, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00025-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48025 length: 48025 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 22.0 in stage 8.0 (TID 641). 117387 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 31.0 in stage 8.0 (TID 650)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 651, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 32.0 in stage 8.0 (TID 651)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 610) in 149 ms on localhost (189/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 609) in 159 ms on localhost (190/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00144-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47983 length: 47983 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00109-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48144 length: 48144 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 652, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 23.0 in stage 8.0 (TID 642). 117461 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 33.0 in stage 8.0 (TID 652)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 603) in 172 ms on localhost (191/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00009-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48028 length: 48028 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 25.0 in stage 8.0 (TID 644). 117459 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 653, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 34.0 in stage 8.0 (TID 653)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 654, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 35.0 in stage 8.0 (TID 654)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 611) in 158 ms on localhost (192/200)
15/08/16 12:51:34 INFO Executor: Finished task 26.0 in stage 8.0 (TID 645). 117487 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 655, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00124-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 24.0 in stage 8.0 (TID 643). 117520 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 656, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 37.0 in stage 8.0 (TID 656)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00066-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48039 length: 48039 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00008-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 36.0 in stage 8.0 (TID 655)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 657, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 614) in 159 ms on localhost (193/200)
15/08/16 12:51:34 INFO Executor: Running task 38.0 in stage 8.0 (TID 657)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 613) in 160 ms on localhost (194/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00141-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47979 length: 47979 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 608) in 177 ms on localhost (195/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 28.0 in stage 8.0 (TID 647). 117305 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00064-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47903 length: 47903 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 29.0 in stage 8.0 (TID 648). 117429 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 27.0 in stage 8.0 (TID 646). 117463 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 612) in 162 ms on localhost (196/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 658, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 39.0 in stage 8.0 (TID 658)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 659, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00166-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47957 length: 47957 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 660, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 40.0 in stage 8.0 (TID 659)
15/08/16 12:51:34 INFO Executor: Finished task 30.0 in stage 8.0 (TID 649). 117373 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 41.0 in stage 8.0 (TID 660)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 661, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 619) in 153 ms on localhost (1/200)
15/08/16 12:51:34 INFO Executor: Running task 42.0 in stage 8.0 (TID 661)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00196-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48084 length: 48084 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 662, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00048-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48060 length: 48060 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 663, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00187-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47795 length: 47795 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 44.0 in stage 8.0 (TID 663)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 43.0 in stage 8.0 (TID 662)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 616) in 161 ms on localhost (197/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 31.0 in stage 8.0 (TID 650). 117481 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 35.0 in stage 8.0 (TID 654). 117494 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 33.0 in stage 8.0 (TID 652). 117336 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00050-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47970 length: 47970 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 664, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 45.0 in stage 8.0 (TID 664)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 34.0 in stage 8.0 (TID 653). 117384 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00113-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47998 length: 47998 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 615) in 174 ms on localhost (198/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 617) in 165 ms on localhost (199/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 622) in 156 ms on localhost (2/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 665, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 46.0 in stage 8.0 (TID 665)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00069-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48075 length: 48075 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 37.0 in stage 8.0 (TID 656). 117498 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 32.0 in stage 8.0 (TID 651). 117471 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 626) in 148 ms on localhost (3/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 624) in 153 ms on localhost (4/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 666, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 623) in 157 ms on localhost (5/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 47.0 in stage 8.0 (TID 666)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 36.0 in stage 8.0 (TID 655). 117452 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00105-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47908 length: 47908 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 38.0 in stage 8.0 (TID 657). 117498 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 667, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00088-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48109 length: 48109 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 618) in 175 ms on localhost (200/200)
15/08/16 12:51:34 INFO DAGScheduler: ResultStage 7 (run at ThreadPoolExecutor.java:1145) finished in 0.744 s
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 48.0 in stage 8.0 (TID 667)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/16 12:51:34 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 627) in 153 ms on localhost (6/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO DAGScheduler: Job 5 finished: run at ThreadPoolExecutor.java:1145, took 0.778774 s
15/08/16 12:51:34 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@117098bb
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 620) in 175 ms on localhost (7/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 621) in 171 ms on localhost (8/200)
15/08/16 12:51:34 INFO StatsReportListener: task runtime:(count: 205, mean: 106.375610, stdev: 28.097746, max: 177.000000, min: 66.000000)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	66.0 ms	77.0 ms	80.0 ms	86.0 ms	96.0 ms	123.0 ms	153.0 ms	159.0 ms	177.0 ms
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 39.0 in stage 8.0 (TID 658). 117469 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 44.0 in stage 8.0 (TID 663). 117483 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 668, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 49.0 in stage 8.0 (TID 668)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00097-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48044 length: 48044 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 669, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 41.0 in stage 8.0 (TID 660). 117390 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 50.0 in stage 8.0 (TID 669)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 670, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00102-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47936 length: 47936 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 51.0 in stage 8.0 (TID 670)
15/08/16 12:51:34 INFO StatsReportListener: task result size:(count: 205, mean: 117435.863415, stdev: 44.314407, max: 117542.000000, min: 117305.000000)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 671, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00107-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47966 length: 47966 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO StatsReportListener: 	114.6 KB	114.6 KB	114.6 KB	114.7 KB	114.7 KB	114.7 KB	114.7 KB	114.8 KB	114.8 KB
15/08/16 12:51:34 INFO Executor: Running task 52.0 in stage 8.0 (TID 671)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 672, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 53.0 in stage 8.0 (TID 672)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00114-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48019 length: 48019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00161-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47945 length: 47945 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 42.0 in stage 8.0 (TID 661). 117511 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 628) in 135 ms on localhost (9/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00140-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48185 length: 48185 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO StatsReportListener: executor (non-fetch) time pct: (count: 205, mean: 22.149392, stdev: 7.442043, max: 53.333333, min: 8.053691)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	 8 %	11 %	13 %	17 %	21 %	27 %	32 %	34 %	53 %
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 673, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO StatsReportListener: other time pct: (count: 205, mean: 77.850608, stdev: 7.442043, max: 91.946309, min: 46.666667)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	47 %	66 %	68 %	73 %	79 %	83 %	87 %	89 %	92 %
15/08/16 12:51:34 INFO Executor: Finished task 45.0 in stage 8.0 (TID 664). 117418 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 54.0 in stage 8.0 (TID 673)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 40.0 in stage 8.0 (TID 659). 117420 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00181-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48116 length: 48116 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 43.0 in stage 8.0 (TID 662). 117405 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 674, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 631) in 129 ms on localhost (10/200)
15/08/16 12:51:34 INFO Executor: Running task 55.0 in stage 8.0 (TID 674)
15/08/16 12:51:34 INFO Executor: Finished task 46.0 in stage 8.0 (TID 665). 117473 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 630) in 133 ms on localhost (11/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00139-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48029 length: 48029 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 51.0 in stage 8.0 (TID 670). 117439 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 625) in 185 ms on localhost (12/200)
15/08/16 12:51:34 INFO Executor: Finished task 47.0 in stage 8.0 (TID 666). 117430 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 633) in 131 ms on localhost (13/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 629) in 151 ms on localhost (14/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 48.0 in stage 8.0 (TID 667). 117428 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 675, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 632) in 137 ms on localhost (15/200)
15/08/16 12:51:34 INFO Executor: Finished task 50.0 in stage 8.0 (TID 669). 117435 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 634) in 122 ms on localhost (16/200)
15/08/16 12:51:34 INFO Executor: Running task 56.0 in stage 8.0 (TID 675)
15/08/16 12:51:34 INFO Executor: Finished task 49.0 in stage 8.0 (TID 668). 117457 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 53.0 in stage 8.0 (TID 672). 117351 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 52.0 in stage 8.0 (TID 671). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00197-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48048 length: 48048 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 676, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 57.0 in stage 8.0 (TID 676)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 677, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 58.0 in stage 8.0 (TID 677)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00077-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48216 length: 48216 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00085-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48033 length: 48033 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 678, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 59.0 in stage 8.0 (TID 678)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 636) in 122 ms on localhost (17/200)
15/08/16 12:51:34 INFO Executor: Finished task 54.0 in stage 8.0 (TID 673). 117444 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 639) in 116 ms on localhost (18/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 635) in 125 ms on localhost (19/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 637) in 122 ms on localhost (20/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00154-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48005 length: 48005 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 679, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 60.0 in stage 8.0 (TID 679)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 680, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 61.0 in stage 8.0 (TID 680)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00103-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00118-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 55.0 in stage 8.0 (TID 674). 117483 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 681, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 62.0 in stage 8.0 (TID 681)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 682, localhost, ANY, 1707 bytes)
15/08/16 12:51:34 INFO Executor: Running task 63.0 in stage 8.0 (TID 682)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 641) in 118 ms on localhost (21/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00089-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 638) in 126 ms on localhost (22/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00156-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48083 length: 48083 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 56.0 in stage 8.0 (TID 675). 117466 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 683, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 64.0 in stage 8.0 (TID 683)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00170-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48236 length: 48236 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 684, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 65.0 in stage 8.0 (TID 684)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 685, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 66.0 in stage 8.0 (TID 685)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 645) in 118 ms on localhost (23/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00174-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48147 length: 48147 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00164-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 58.0 in stage 8.0 (TID 677). 117417 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 686, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 642) in 125 ms on localhost (24/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 67.0 in stage 8.0 (TID 686)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 640) in 133 ms on localhost (25/200)
15/08/16 12:51:34 INFO Executor: Finished task 59.0 in stage 8.0 (TID 678). 117504 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00167-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47969 length: 47969 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 687, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 57.0 in stage 8.0 (TID 676). 117376 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 688, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 69.0 in stage 8.0 (TID 688)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 61.0 in stage 8.0 (TID 680). 117429 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 689, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 68.0 in stage 8.0 (TID 687)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 644) in 126 ms on localhost (26/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00110-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48022 length: 48022 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 643) in 128 ms on localhost (27/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 647) in 117 ms on localhost (28/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00091-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47937 length: 47937 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 690, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 648) in 117 ms on localhost (29/200)
15/08/16 12:51:34 INFO Executor: Running task 71.0 in stage 8.0 (TID 690)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 70.0 in stage 8.0 (TID 689)
15/08/16 12:51:34 INFO Executor: Finished task 60.0 in stage 8.0 (TID 679). 117427 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 691, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 72.0 in stage 8.0 (TID 691)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00010-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00112-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47912 length: 47912 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 692, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 73.0 in stage 8.0 (TID 692)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 63.0 in stage 8.0 (TID 682). 117425 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 64.0 in stage 8.0 (TID 683). 117330 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 62.0 in stage 8.0 (TID 681). 117444 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 693, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 646) in 132 ms on localhost (30/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00172-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48190 length: 48190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00151-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48021 length: 48021 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 694, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 650) in 125 ms on localhost (31/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 649) in 127 ms on localhost (32/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 75.0 in stage 8.0 (TID 694)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 695, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 76.0 in stage 8.0 (TID 695)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 654) in 117 ms on localhost (33/200)
15/08/16 12:51:34 INFO Executor: Running task 74.0 in stage 8.0 (TID 693)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00148-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48070 length: 48070 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 696, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 77.0 in stage 8.0 (TID 696)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 697, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 78.0 in stage 8.0 (TID 697)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00037-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00017-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00191-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47993 length: 47993 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 66.0 in stage 8.0 (TID 685). 117408 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00045-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48006 length: 48006 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 65.0 in stage 8.0 (TID 684). 117374 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 652) in 129 ms on localhost (34/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 656) in 123 ms on localhost (35/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 651) in 135 ms on localhost (36/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 653) in 132 ms on localhost (37/200)
15/08/16 12:51:34 INFO Executor: Finished task 69.0 in stage 8.0 (TID 688). 117385 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 71.0 in stage 8.0 (TID 690). 117400 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 73.0 in stage 8.0 (TID 692). 117447 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 70.0 in stage 8.0 (TID 689). 117475 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 698, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 79.0 in stage 8.0 (TID 698)
15/08/16 12:51:34 INFO Executor: Finished task 67.0 in stage 8.0 (TID 686). 117459 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 68.0 in stage 8.0 (TID 687). 117433 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 699, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 80.0 in stage 8.0 (TID 699)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 658) in 272 ms on localhost (38/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 655) in 281 ms on localhost (39/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 663) in 267 ms on localhost (40/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00060-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48068 length: 48068 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 77.0 in stage 8.0 (TID 696). 117433 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 657) in 280 ms on localhost (41/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00158-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48013 length: 48013 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 700, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 81.0 in stage 8.0 (TID 700)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 701, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00094-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48140 length: 48140 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 82.0 in stage 8.0 (TID 701)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 702, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 660) in 276 ms on localhost (42/200)
15/08/16 12:51:34 INFO Executor: Running task 83.0 in stage 8.0 (TID 702)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:36543 in memory (size: 2.3 KB, free: 3.1 GB)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00058-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48097 length: 48097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 703, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00041-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48006 length: 48006 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 84.0 in stage 8.0 (TID 703)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 664) in 272 ms on localhost (43/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 661) in 277 ms on localhost (44/200)
15/08/16 12:51:34 INFO Executor: Finished task 72.0 in stage 8.0 (TID 691). 117406 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00078-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48190 length: 48190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 704, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:36543 in memory (size: 2.4 KB, free: 3.1 GB)
15/08/16 12:51:34 INFO Executor: Running task 85.0 in stage 8.0 (TID 704)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 705, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 659) in 282 ms on localhost (45/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 86.0 in stage 8.0 (TID 705)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00019-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47959 length: 47959 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:36543 in memory (size: 1676.0 B, free: 3.1 GB)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 665) in 268 ms on localhost (46/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00076-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47868 length: 47868 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 706, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 76.0 in stage 8.0 (TID 695). 117395 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 87.0 in stage 8.0 (TID 706)
15/08/16 12:51:34 INFO Executor: Finished task 78.0 in stage 8.0 (TID 697). 117458 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 707, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 74.0 in stage 8.0 (TID 693). 117406 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 88.0 in stage 8.0 (TID 707)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 670) in 254 ms on localhost (47/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00022-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47954 length: 47954 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00155-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47956 length: 47956 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 708, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 662) in 286 ms on localhost (48/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 667) in 266 ms on localhost (49/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 75.0 in stage 8.0 (TID 694). 117422 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 709, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 80.0 in stage 8.0 (TID 699). 117378 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 90.0 in stage 8.0 (TID 709)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 669) in 261 ms on localhost (50/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 89.0 in stage 8.0 (TID 708)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00042-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48049 length: 48049 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00039-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 84.0 in stage 8.0 (TID 703). 117394 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 666) in 276 ms on localhost (51/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 82.0 in stage 8.0 (TID 701). 117406 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 83.0 in stage 8.0 (TID 702). 117441 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 79.0 in stage 8.0 (TID 698). 117430 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 710, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 672) in 269 ms on localhost (52/200)
15/08/16 12:51:34 INFO Executor: Running task 91.0 in stage 8.0 (TID 710)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 668) in 273 ms on localhost (53/200)
15/08/16 12:51:34 INFO Executor: Finished task 81.0 in stage 8.0 (TID 700). 117419 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 671) in 271 ms on localhost (54/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 673) in 264 ms on localhost (55/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00000-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48091 length: 48091 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 711, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 92.0 in stage 8.0 (TID 711)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 712, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 85.0 in stage 8.0 (TID 704). 117462 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00193-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48009 length: 48009 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 713, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 87.0 in stage 8.0 (TID 706). 117401 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 94.0 in stage 8.0 (TID 713)
15/08/16 12:51:34 INFO Executor: Running task 93.0 in stage 8.0 (TID 712)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 714, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 86.0 in stage 8.0 (TID 705). 117498 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 88.0 in stage 8.0 (TID 707). 117433 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 95.0 in stage 8.0 (TID 714)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 675) in 250 ms on localhost (56/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 674) in 266 ms on localhost (57/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00173-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47922 length: 47922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00152-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 677) in 246 ms on localhost (58/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 678) in 246 ms on localhost (59/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00040-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48033 length: 48033 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 90.0 in stage 8.0 (TID 709). 117416 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 715, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 96.0 in stage 8.0 (TID 715)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 716, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 97.0 in stage 8.0 (TID 716)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 717, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00186-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47910 length: 47910 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 98.0 in stage 8.0 (TID 717)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00104-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48022 length: 48022 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 718, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 99.0 in stage 8.0 (TID 718)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 680) in 245 ms on localhost (60/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 676) in 255 ms on localhost (61/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 679) in 247 ms on localhost (62/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00036-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48003 length: 48003 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00016-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 682) in 242 ms on localhost (63/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 719, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 100.0 in stage 8.0 (TID 719)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 720, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 89.0 in stage 8.0 (TID 708). 117398 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 101.0 in stage 8.0 (TID 720)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 721, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 102.0 in stage 8.0 (TID 721)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00117-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47919 length: 47919 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 91.0 in stage 8.0 (TID 710). 117434 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 722, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 103.0 in stage 8.0 (TID 722)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00165-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47984 length: 47984 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 723, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 104.0 in stage 8.0 (TID 723)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00074-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48072 length: 48072 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 724, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00171-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48141 length: 48141 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 683) in 244 ms on localhost (64/200)
15/08/16 12:51:34 INFO Executor: Running task 105.0 in stage 8.0 (TID 724)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00021-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 681) in 252 ms on localhost (65/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00142-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47983 length: 47983 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 92.0 in stage 8.0 (TID 711). 117470 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 684) in 245 ms on localhost (66/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 688) in 238 ms on localhost (67/200)
15/08/16 12:51:34 INFO Executor: Finished task 94.0 in stage 8.0 (TID 713). 117530 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 95.0 in stage 8.0 (TID 714). 117468 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 725, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 726, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 106.0 in stage 8.0 (TID 725)
15/08/16 12:51:34 INFO Executor: Running task 107.0 in stage 8.0 (TID 726)
15/08/16 12:51:34 INFO Executor: Finished task 93.0 in stage 8.0 (TID 712). 117447 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 727, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 728, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00123-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 97.0 in stage 8.0 (TID 716). 117401 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 109.0 in stage 8.0 (TID 728)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00023-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48153 length: 48153 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00194-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47963 length: 47963 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 108.0 in stage 8.0 (TID 727)
15/08/16 12:51:34 INFO Executor: Finished task 99.0 in stage 8.0 (TID 718). 117481 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 101.0 in stage 8.0 (TID 720). 117459 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 690) in 240 ms on localhost (68/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 692) in 242 ms on localhost (69/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 689) in 248 ms on localhost (70/200)
15/08/16 12:51:34 INFO Executor: Finished task 100.0 in stage 8.0 (TID 719). 117439 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 96.0 in stage 8.0 (TID 715). 117519 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00062-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48038 length: 48038 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 105.0 in stage 8.0 (TID 724). 117439 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 104.0 in stage 8.0 (TID 723). 117411 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 729, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 110.0 in stage 8.0 (TID 729)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 730, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 111.0 in stage 8.0 (TID 730)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 685) in 260 ms on localhost (71/200)
15/08/16 12:51:34 INFO Executor: Finished task 98.0 in stage 8.0 (TID 717). 117476 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 696) in 236 ms on localhost (72/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00106-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47918 length: 47918 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 687) in 255 ms on localhost (73/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00150-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47898 length: 47898 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 731, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 102.0 in stage 8.0 (TID 721). 117423 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 732, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 113.0 in stage 8.0 (TID 732)
15/08/16 12:51:34 INFO Executor: Finished task 103.0 in stage 8.0 (TID 722). 117414 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 112.0 in stage 8.0 (TID 731)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 733, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 114.0 in stage 8.0 (TID 733)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00030-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47977 length: 47977 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00014-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 734, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 107.0 in stage 8.0 (TID 726). 117365 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00044-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48057 length: 48057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 115.0 in stage 8.0 (TID 734)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 735, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 116.0 in stage 8.0 (TID 735)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 736, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 117.0 in stage 8.0 (TID 736)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00018-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47980 length: 47980 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 737, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00175-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48135 length: 48135 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 118.0 in stage 8.0 (TID 737)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00176-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48120 length: 48120 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 738, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 691) in 257 ms on localhost (74/200)
15/08/16 12:51:34 INFO Executor: Running task 119.0 in stage 8.0 (TID 738)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 109.0 in stage 8.0 (TID 728). 117370 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 686) in 265 ms on localhost (75/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00198-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47950 length: 47950 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 739, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00098-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48161 length: 48161 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 695) in 248 ms on localhost (76/200)
15/08/16 12:51:34 INFO Executor: Running task 120.0 in stage 8.0 (TID 739)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 740, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 121.0 in stage 8.0 (TID 740)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00034-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47916 length: 47916 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 741, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 106.0 in stage 8.0 (TID 725). 117519 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 122.0 in stage 8.0 (TID 741)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 742, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00035-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47992 length: 47992 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 123.0 in stage 8.0 (TID 742)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 693) in 257 ms on localhost (77/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00049-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47993 length: 47993 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 743, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00006-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48079 length: 48079 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 124.0 in stage 8.0 (TID 743)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 697) in 250 ms on localhost (78/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00053-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47947 length: 47947 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 108.0 in stage 8.0 (TID 727). 117417 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 744, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 699) in 93 ms on localhost (79/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 694) in 261 ms on localhost (80/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 701) in 90 ms on localhost (81/200)
15/08/16 12:51:34 INFO Executor: Finished task 112.0 in stage 8.0 (TID 731). 117443 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 745, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 115.0 in stage 8.0 (TID 734). 117408 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 746, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 127.0 in stage 8.0 (TID 746)
15/08/16 12:51:34 INFO Executor: Running task 125.0 in stage 8.0 (TID 744)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00075-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48124 length: 48124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 121.0 in stage 8.0 (TID 740). 117447 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 119.0 in stage 8.0 (TID 738). 117353 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 117.0 in stage 8.0 (TID 736). 117467 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 110.0 in stage 8.0 (TID 729). 117512 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00135-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48211 length: 48211 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 111.0 in stage 8.0 (TID 730). 117468 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 698) in 251 ms on localhost (82/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 114.0 in stage 8.0 (TID 733). 117402 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 126.0 in stage 8.0 (TID 745)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 703) in 94 ms on localhost (83/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00046-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47959 length: 47959 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 113.0 in stage 8.0 (TID 732). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 118.0 in stage 8.0 (TID 737). 117443 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 116.0 in stage 8.0 (TID 735). 117357 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 123.0 in stage 8.0 (TID 742). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 747, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 128.0 in stage 8.0 (TID 747)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 748, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 129.0 in stage 8.0 (TID 748)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00101-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48013 length: 48013 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00190-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48105 length: 48105 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 749, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 702) in 104 ms on localhost (84/200)
15/08/16 12:51:34 INFO Executor: Running task 130.0 in stage 8.0 (TID 749)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 750, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 131.0 in stage 8.0 (TID 750)
15/08/16 12:51:34 INFO Executor: Finished task 124.0 in stage 8.0 (TID 743). 117505 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 706) in 98 ms on localhost (85/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00068-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47989 length: 47989 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00138-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48051 length: 48051 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 704) in 102 ms on localhost (86/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 120.0 in stage 8.0 (TID 739). 117450 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 700) in 111 ms on localhost (87/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 122.0 in stage 8.0 (TID 741). 117358 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 709) in 98 ms on localhost (88/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 707) in 104 ms on localhost (89/200)
15/08/16 12:51:34 INFO Executor: Finished task 127.0 in stage 8.0 (TID 746). 117387 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 751, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 132.0 in stage 8.0 (TID 751)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 126.0 in stage 8.0 (TID 745). 117449 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 752, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00093-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48047 length: 48047 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 708) in 104 ms on localhost (90/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 133.0 in stage 8.0 (TID 752)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 710) in 94 ms on localhost (91/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 711) in 91 ms on localhost (92/200)
15/08/16 12:51:34 INFO Executor: Finished task 125.0 in stage 8.0 (TID 744). 117429 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00119-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48129 length: 48129 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 128.0 in stage 8.0 (TID 747). 117486 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 705) in 115 ms on localhost (93/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 713) in 92 ms on localhost (94/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 753, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 129.0 in stage 8.0 (TID 748). 117386 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 134.0 in stage 8.0 (TID 753)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 716) in 89 ms on localhost (95/200)
15/08/16 12:51:34 INFO Executor: Finished task 131.0 in stage 8.0 (TID 750). 117467 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00024-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48106 length: 48106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 130.0 in stage 8.0 (TID 749). 117453 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 712) in 98 ms on localhost (96/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 714) in 96 ms on localhost (97/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 754, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 135.0 in stage 8.0 (TID 754)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 755, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 756, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 137.0 in stage 8.0 (TID 756)
15/08/16 12:51:34 INFO Executor: Running task 136.0 in stage 8.0 (TID 755)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 720) in 88 ms on localhost (98/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 757, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 138.0 in stage 8.0 (TID 757)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 758, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00143-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48054 length: 48054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 139.0 in stage 8.0 (TID 758)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00180-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48144 length: 48144 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 759, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 140.0 in stage 8.0 (TID 759)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 718) in 95 ms on localhost (99/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00012-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48117 length: 48117 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 715) in 98 ms on localhost (100/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00028-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48123 length: 48123 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 760, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 141.0 in stage 8.0 (TID 760)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00013-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47901 length: 47901 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00071-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48018 length: 48018 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 761, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 142.0 in stage 8.0 (TID 761)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 724) in 91 ms on localhost (101/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00134-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48159 length: 48159 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 719) in 96 ms on localhost (102/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00122-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48119 length: 48119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 762, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 143.0 in stage 8.0 (TID 762)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 717) in 103 ms on localhost (103/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00001-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48017 length: 48017 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 721) in 98 ms on localhost (104/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 723) in 97 ms on localhost (105/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 763, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 144.0 in stage 8.0 (TID 763)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 764, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 145.0 in stage 8.0 (TID 764)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00005-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48037 length: 48037 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 765, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 146.0 in stage 8.0 (TID 765)
15/08/16 12:51:34 INFO Executor: Finished task 132.0 in stage 8.0 (TID 751). 117473 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00157-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47985 length: 47985 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 766, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00033-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47930 length: 47930 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 147.0 in stage 8.0 (TID 766)
15/08/16 12:51:34 INFO Executor: Finished task 134.0 in stage 8.0 (TID 753). 117437 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 722) in 105 ms on localhost (106/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 767, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 725) in 98 ms on localhost (107/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 726) in 97 ms on localhost (108/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 728) in 97 ms on localhost (109/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 148.0 in stage 8.0 (TID 767)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00182-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48226 length: 48226 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00116-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48117 length: 48117 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 727) in 100 ms on localhost (110/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 768, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 149.0 in stage 8.0 (TID 768)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 734) in 86 ms on localhost (111/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 740) in 81 ms on localhost (112/200)
15/08/16 12:51:34 INFO Executor: Finished task 138.0 in stage 8.0 (TID 757). 117363 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 731) in 90 ms on localhost (113/200)
15/08/16 12:51:34 INFO Executor: Finished task 133.0 in stage 8.0 (TID 752). 117424 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00127-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48094 length: 48094 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 769, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 150.0 in stage 8.0 (TID 769)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 770, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 729) in 97 ms on localhost (114/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00131-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48041 length: 48041 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 736) in 89 ms on localhost (115/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 141.0 in stage 8.0 (TID 760). 117415 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 730) in 98 ms on localhost (116/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 771, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 135.0 in stage 8.0 (TID 754). 117398 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 152.0 in stage 8.0 (TID 771)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 772, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 151.0 in stage 8.0 (TID 770)
15/08/16 12:51:34 INFO Executor: Running task 153.0 in stage 8.0 (TID 772)
15/08/16 12:51:34 INFO Executor: Finished task 136.0 in stage 8.0 (TID 755). 117423 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00032-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48047 length: 48047 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 773, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00051-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48133 length: 48133 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 154.0 in stage 8.0 (TID 773)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 733) in 97 ms on localhost (117/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00056-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48000 length: 48000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00029-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 738) in 94 ms on localhost (118/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 732) in 100 ms on localhost (119/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 737) in 97 ms on localhost (120/200)
15/08/16 12:51:34 INFO Executor: Finished task 140.0 in stage 8.0 (TID 759). 117478 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 774, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Finished task 143.0 in stage 8.0 (TID 762). 117398 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 155.0 in stage 8.0 (TID 774)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 775, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 156.0 in stage 8.0 (TID 775)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 735) in 100 ms on localhost (121/200)
15/08/16 12:51:34 INFO Executor: Finished task 142.0 in stage 8.0 (TID 761). 117412 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 146.0 in stage 8.0 (TID 765). 117434 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00168-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47995 length: 47995 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 776, localhost, ANY, 1707 bytes)
15/08/16 12:51:34 INFO Executor: Running task 157.0 in stage 8.0 (TID 776)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00147-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48124 length: 48124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 777, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 743) in 93 ms on localhost (122/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 158.0 in stage 8.0 (TID 777)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00055-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48082 length: 48082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 739) in 100 ms on localhost (123/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 742) in 98 ms on localhost (124/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00020-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47891 length: 47891 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 741) in 100 ms on localhost (125/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 746) in 86 ms on localhost (126/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 139.0 in stage 8.0 (TID 758). 117449 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 778, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 744) in 98 ms on localhost (127/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 745) in 91 ms on localhost (128/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 748) in 82 ms on localhost (129/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 747) in 83 ms on localhost (130/200)
15/08/16 12:51:34 INFO Executor: Finished task 144.0 in stage 8.0 (TID 763). 117411 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 147.0 in stage 8.0 (TID 766). 117398 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Running task 159.0 in stage 8.0 (TID 778)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 779, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 160.0 in stage 8.0 (TID 779)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 780, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 161.0 in stage 8.0 (TID 780)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00059-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47995 length: 47995 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00087-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48107 length: 48107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 751) in 72 ms on localhost (131/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 137.0 in stage 8.0 (TID 756). 117413 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00162-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48057 length: 48057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 753) in 66 ms on localhost (132/200)
15/08/16 12:51:34 INFO Executor: Finished task 148.0 in stage 8.0 (TID 767). 117319 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 781, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 162.0 in stage 8.0 (TID 781)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 782, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 163.0 in stage 8.0 (TID 782)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 750) in 87 ms on localhost (133/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 749) in 91 ms on localhost (134/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00130-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48155 length: 48155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 145.0 in stage 8.0 (TID 764). 117527 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00004-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48048 length: 48048 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 783, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 164.0 in stage 8.0 (TID 783)
15/08/16 12:51:34 INFO Executor: Finished task 153.0 in stage 8.0 (TID 772). 117430 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 784, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 165.0 in stage 8.0 (TID 784)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 757) in 66 ms on localhost (135/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 752) in 83 ms on localhost (136/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00195-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47873 length: 47873 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 760) in 64 ms on localhost (137/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00185-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47875 length: 47875 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 154.0 in stage 8.0 (TID 773). 117432 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 785, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 166.0 in stage 8.0 (TID 785)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 755) in 73 ms on localhost (138/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 759) in 70 ms on localhost (139/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00111-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48164 length: 48164 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 754) in 76 ms on localhost (140/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 762) in 68 ms on localhost (141/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 765) in 63 ms on localhost (142/200)
15/08/16 12:51:34 INFO Executor: Finished task 152.0 in stage 8.0 (TID 771). 117490 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 786, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 167.0 in stage 8.0 (TID 786)
15/08/16 12:51:34 INFO Executor: Finished task 155.0 in stage 8.0 (TID 774). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00011-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48139 length: 48139 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 158.0 in stage 8.0 (TID 777). 117415 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 761) in 74 ms on localhost (143/200)
15/08/16 12:51:34 INFO Executor: Finished task 151.0 in stage 8.0 (TID 770). 117484 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 758) in 78 ms on localhost (144/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 157.0 in stage 8.0 (TID 776). 117396 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 787, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 766) in 67 ms on localhost (145/200)
15/08/16 12:51:34 INFO Executor: Running task 168.0 in stage 8.0 (TID 787)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 763) in 69 ms on localhost (146/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 788, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 169.0 in stage 8.0 (TID 788)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 789, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 170.0 in stage 8.0 (TID 789)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 790, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 171.0 in stage 8.0 (TID 790)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00178-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47925 length: 47925 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 767) in 64 ms on localhost (147/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00108-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48056 length: 48056 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00129-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47816 length: 47816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 150.0 in stage 8.0 (TID 769). 117427 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00027-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48137 length: 48137 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 149.0 in stage 8.0 (TID 768). 117463 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 791, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 172.0 in stage 8.0 (TID 791)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 792, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00169-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47953 length: 47953 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 764) in 75 ms on localhost (148/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 773) in 52 ms on localhost (149/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 772) in 55 ms on localhost (150/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 756) in 92 ms on localhost (151/200)
15/08/16 12:51:34 INFO Executor: Running task 173.0 in stage 8.0 (TID 792)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 159.0 in stage 8.0 (TID 778). 117416 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00007-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48056 length: 48056 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 793, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 174.0 in stage 8.0 (TID 793)
15/08/16 12:51:34 INFO Executor: Finished task 160.0 in stage 8.0 (TID 779). 117495 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00132-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48097 length: 48097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 164.0 in stage 8.0 (TID 783). 117489 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 770) in 65 ms on localhost (152/200)
15/08/16 12:51:34 INFO Executor: Finished task 161.0 in stage 8.0 (TID 780). 117424 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 771) in 63 ms on localhost (153/200)
15/08/16 12:51:34 INFO Executor: Finished task 156.0 in stage 8.0 (TID 775). 117407 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 165.0 in stage 8.0 (TID 784). 117506 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 777) in 55 ms on localhost (154/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 794, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 795, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 163.0 in stage 8.0 (TID 782). 117428 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 166.0 in stage 8.0 (TID 785). 117412 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 796, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 768) in 79 ms on localhost (155/200)
15/08/16 12:51:34 INFO Executor: Running task 177.0 in stage 8.0 (TID 796)
15/08/16 12:51:34 INFO Executor: Running task 176.0 in stage 8.0 (TID 795)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 175.0 in stage 8.0 (TID 794)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 162.0 in stage 8.0 (TID 781). 117359 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 167.0 in stage 8.0 (TID 786). 117385 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 774) in 65 ms on localhost (156/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00057-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47986 length: 47986 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00003-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47982 length: 47982 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 776) in 64 ms on localhost (157/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 171.0 in stage 8.0 (TID 790). 117389 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00160-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48113 length: 48113 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 769) in 81 ms on localhost (158/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 783) in 46 ms on localhost (159/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 797, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 178.0 in stage 8.0 (TID 797)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 798, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 179.0 in stage 8.0 (TID 798)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 170.0 in stage 8.0 (TID 789). 117476 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 168.0 in stage 8.0 (TID 787). 117395 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 173.0 in stage 8.0 (TID 792). 117415 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00015-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48168 length: 48168 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 172.0 in stage 8.0 (TID 791). 117449 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00065-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47987 length: 47987 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 169.0 in stage 8.0 (TID 788). 117481 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Finished task 174.0 in stage 8.0 (TID 793). 117406 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 799, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Running task 180.0 in stage 8.0 (TID 799)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 800, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 778) in 70 ms on localhost (160/200)
15/08/16 12:51:34 INFO Executor: Running task 181.0 in stage 8.0 (TID 800)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00145-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47976 length: 47976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 801, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 182.0 in stage 8.0 (TID 801)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 802, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 183.0 in stage 8.0 (TID 802)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00043-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47975 length: 47975 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 780) in 66 ms on localhost (161/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00125-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47829 length: 47829 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 779) in 68 ms on localhost (162/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00084-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47854 length: 47854 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 177.0 in stage 8.0 (TID 796). 117431 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 803, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 804, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 184.0 in stage 8.0 (TID 803)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 782) in 64 ms on localhost (163/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00115-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48000 length: 48000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 775) in 87 ms on localhost (164/200)
15/08/16 12:51:34 INFO Executor: Running task 185.0 in stage 8.0 (TID 804)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00188-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48227 length: 48227 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 176.0 in stage 8.0 (TID 795). 117441 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 175.0 in stage 8.0 (TID 794). 117542 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 784) in 63 ms on localhost (165/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 805, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 785) in 62 ms on localhost (166/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 786) in 56 ms on localhost (167/200)
15/08/16 12:51:34 INFO Executor: Running task 186.0 in stage 8.0 (TID 805)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 806, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 187.0 in stage 8.0 (TID 806)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 807, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO Executor: Running task 188.0 in stage 8.0 (TID 807)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00026-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48203 length: 48203 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 808, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 790) in 53 ms on localhost (168/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 781) in 75 ms on localhost (169/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00120-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48011 length: 48011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Running task 189.0 in stage 8.0 (TID 808)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 809, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO Executor: Running task 190.0 in stage 8.0 (TID 809)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 810, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 191.0 in stage 8.0 (TID 810)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00038-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48067 length: 48067 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00192-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48046 length: 48046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 811, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Running task 192.0 in stage 8.0 (TID 811)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00092-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48173 length: 48173 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00128-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47962 length: 47962 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 812, localhost, ANY, 1704 bytes)
15/08/16 12:51:34 INFO Executor: Running task 193.0 in stage 8.0 (TID 812)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 787) in 61 ms on localhost (170/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00067-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48014 length: 48014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 179.0 in stage 8.0 (TID 798). 117440 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 789) in 59 ms on localhost (171/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 813, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 194.0 in stage 8.0 (TID 813)
15/08/16 12:51:34 INFO Executor: Finished task 180.0 in stage 8.0 (TID 799). 117421 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00183-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47968 length: 47968 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 178.0 in stage 8.0 (TID 797). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00136-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48010 length: 48010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 791) in 58 ms on localhost (172/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 792) in 60 ms on localhost (173/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 788) in 64 ms on localhost (174/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 814, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 181.0 in stage 8.0 (TID 800). 117513 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 185.0 in stage 8.0 (TID 804). 117375 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 815, localhost, ANY, 1707 bytes)
15/08/16 12:51:34 INFO Executor: Running task 196.0 in stage 8.0 (TID 815)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 816, localhost, ANY, 1703 bytes)
15/08/16 12:51:34 INFO Executor: Running task 197.0 in stage 8.0 (TID 816)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 795) in 52 ms on localhost (175/200)
15/08/16 12:51:34 INFO Executor: Running task 195.0 in stage 8.0 (TID 814)
15/08/16 12:51:34 INFO Executor: Finished task 183.0 in stage 8.0 (TID 802). 117460 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 796) in 52 ms on localhost (176/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 793) in 60 ms on localhost (177/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00159-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48092 length: 48092 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00079-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 47973 length: 47973 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00063-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48141 length: 48141 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 182.0 in stage 8.0 (TID 801). 117497 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 191.0 in stage 8.0 (TID 810). 117468 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 797) in 45 ms on localhost (178/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 190.0 in stage 8.0 (TID 809). 117358 bytes result sent to driver
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO Executor: Finished task 186.0 in stage 8.0 (TID 805). 117435 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 794) in 60 ms on localhost (179/200)
15/08/16 12:51:34 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 817, localhost, ANY, 1705 bytes)
15/08/16 12:51:34 INFO Executor: Running task 198.0 in stage 8.0 (TID 817)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 818, localhost, ANY, 1706 bytes)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Running task 199.0 in stage 8.0 (TID 818)
15/08/16 12:51:34 INFO Executor: Finished task 184.0 in stage 8.0 (TID 803). 117443 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 799) in 47 ms on localhost (180/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 798) in 50 ms on localhost (181/200)
15/08/16 12:51:34 INFO Executor: Finished task 192.0 in stage 8.0 (TID 811). 117442 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 800) in 44 ms on localhost (182/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 7500
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00080-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48082 length: 48082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 804) in 39 ms on localhost (183/200)
15/08/16 12:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp1_par/part-r-00189-11463a4a-337f-478e-82e3-ed9dfcf432a4.gz.parquet start: 0 end: 48050 length: 48050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:34 INFO Executor: Finished task 189.0 in stage 8.0 (TID 808). 117482 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 802) in 46 ms on localhost (184/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 810) in 30 ms on localhost (185/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 809) in 32 ms on localhost (186/200)
15/08/16 12:51:34 INFO Executor: Finished task 193.0 in stage 8.0 (TID 812). 117504 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 801) in 50 ms on localhost (187/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 805) in 40 ms on localhost (188/200)
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:34 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 803) in 48 ms on localhost (189/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 188.0 in stage 8.0 (TID 807). 117396 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 811) in 34 ms on localhost (190/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 808) in 40 ms on localhost (191/200)
15/08/16 12:51:34 INFO Executor: Finished task 187.0 in stage 8.0 (TID 806). 117387 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 812) in 35 ms on localhost (192/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO Executor: Finished task 194.0 in stage 8.0 (TID 813). 117434 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 807) in 45 ms on localhost (193/200)
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 7500
15/08/16 12:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 7500 records.
15/08/16 12:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:34 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 806) in 48 ms on localhost (194/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 813) in 40 ms on localhost (195/200)
15/08/16 12:51:34 INFO Executor: Finished task 196.0 in stage 8.0 (TID 815). 117387 bytes result sent to driver
15/08/16 12:51:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 7500
15/08/16 12:51:34 INFO Executor: Finished task 197.0 in stage 8.0 (TID 816). 117519 bytes result sent to driver
15/08/16 12:51:34 INFO Executor: Finished task 195.0 in stage 8.0 (TID 814). 117397 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 815) in 36 ms on localhost (196/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 816) in 37 ms on localhost (197/200)
15/08/16 12:51:34 INFO Executor: Finished task 199.0 in stage 8.0 (TID 818). 117438 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 814) in 44 ms on localhost (198/200)
15/08/16 12:51:34 INFO Executor: Finished task 198.0 in stage 8.0 (TID 817). 117440 bytes result sent to driver
15/08/16 12:51:34 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 818) in 35 ms on localhost (199/200)
15/08/16 12:51:34 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 817) in 38 ms on localhost (200/200)
15/08/16 12:51:34 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/16 12:51:34 INFO DAGScheduler: ResultStage 8 (run at ThreadPoolExecutor.java:1145) finished in 1.262 s
15/08/16 12:51:34 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7aeee33b
15/08/16 12:51:34 INFO DAGScheduler: Job 6 finished: run at ThreadPoolExecutor.java:1145, took 1.355714 s
15/08/16 12:51:34 INFO StatsReportListener: task runtime:(count: 195, mean: 124.553846, stdev: 77.094886, max: 286.000000, min: 30.000000)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	30.0 ms	40.0 ms	48.0 ms	66.0 ms	97.0 ms	137.0 ms	261.0 ms	272.0 ms	286.0 ms
15/08/16 12:51:34 INFO StatsReportListener: task result size:(count: 195, mean: 117436.512821, stdev: 44.833179, max: 117542.000000, min: 117305.000000)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	114.6 KB	114.6 KB	114.6 KB	114.7 KB	114.7 KB	114.7 KB	114.7 KB	114.8 KB	114.8 KB
15/08/16 12:51:34 INFO StatsReportListener: executor (non-fetch) time pct: (count: 195, mean: 27.220972, stdev: 19.256656, max: 72.916667, min: 4.800000)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	 5 %	 7 %	 8 %	12 %	18 %	42 %	58 %	67 %	73 %
15/08/16 12:51:34 INFO StatsReportListener: other time pct: (count: 195, mean: 72.779028, stdev: 19.256656, max: 95.200000, min: 27.083333)
15/08/16 12:51:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:34 INFO StatsReportListener: 	27 %	33 %	42 %	58 %	82 %	88 %	92 %	93 %	95 %
15/08/16 12:51:36 INFO MemoryStore: ensureFreeSpace(234708768) called with curMem=2231464, maxMem=3333968363
15/08/16 12:51:36 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 223.8 MB, free 2.9 GB)
15/08/16 12:51:36 INFO MemoryStore: ensureFreeSpace(234708768) called with curMem=236940232, maxMem=3333968363
15/08/16 12:51:36 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 223.8 MB, free 2.7 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=471649000, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.0 MB, free 2.7 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=475843304, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_19_piece1 stored as bytes in memory (estimated size 4.0 MB, free 2.7 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_19_piece1 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=480037608, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_19_piece2 stored as bytes in memory (estimated size 4.0 MB, free 2.7 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_19_piece2 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=484231912, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_19_piece3 stored as bytes in memory (estimated size 4.0 MB, free 2.7 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_19_piece3 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(1704300) called with curMem=488426216, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_19_piece4 stored as bytes in memory (estimated size 1664.4 KB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_19_piece4 in memory on localhost:36543 (size: 1664.4 KB, free: 3.1 GB)
15/08/16 12:51:39 INFO SparkContext: Created broadcast 19 from run at ThreadPoolExecutor.java:1145
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=490130516, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.0 MB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=494324820, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_18_piece1 stored as bytes in memory (estimated size 4.0 MB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_18_piece1 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=498519124, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_18_piece2 stored as bytes in memory (estimated size 4.0 MB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_18_piece2 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(4194304) called with curMem=502713428, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_18_piece3 stored as bytes in memory (estimated size 4.0 MB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_18_piece3 in memory on localhost:36543 (size: 4.0 MB, free: 3.1 GB)
15/08/16 12:51:39 INFO MemoryStore: ensureFreeSpace(1704300) called with curMem=506907732, maxMem=3333968363
15/08/16 12:51:39 INFO MemoryStore: Block broadcast_18_piece4 stored as bytes in memory (estimated size 1664.4 KB, free 2.6 GB)
15/08/16 12:51:39 INFO BlockManagerInfo: Added broadcast_18_piece4 in memory on localhost:36543 (size: 1664.4 KB, free: 3.1 GB)
15/08/16 12:51:39 INFO SparkContext: Created broadcast 18 from run at ThreadPoolExecutor.java:1145
15/08/16 12:51:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/16 12:51:39 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:51:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 37 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 42 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 47 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 54 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 59 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 67 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Registering RDD 74 (processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Got job 7 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/16 12:51:40 INFO DAGScheduler: Final stage: ResultStage 16(processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
15/08/16 12:51:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
15/08/16 12:51:40 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[37] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(6192) called with curMem=508612032, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 6.0 KB, free 2.6 GB)
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(3359) called with curMem=508618224, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.3 KB, free 2.6 GB)
15/08/16 12:51:40 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:36543 (size: 3.3 KB, free: 3.1 GB)
15/08/16 12:51:40 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:40 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[37] at processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO TaskSchedulerImpl: Adding task set 9.0 with 200 tasks
15/08/16 12:51:40 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 819, localhost, ANY, 1693 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 820, localhost, ANY, 1691 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 821, localhost, ANY, 1693 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 822, localhost, ANY, 1693 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 823, localhost, ANY, 1693 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 824, localhost, ANY, 1689 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 825, localhost, ANY, 1692 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 826, localhost, ANY, 1692 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 827, localhost, ANY, 1691 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 828, localhost, ANY, 1692 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 829, localhost, ANY, 1691 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 830, localhost, ANY, 1693 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 831, localhost, ANY, 1692 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 832, localhost, ANY, 1692 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 833, localhost, ANY, 1691 bytes)
15/08/16 12:51:40 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 834, localhost, ANY, 1690 bytes)
15/08/16 12:51:40 INFO Executor: Running task 1.0 in stage 9.0 (TID 820)
15/08/16 12:51:40 INFO Executor: Running task 2.0 in stage 9.0 (TID 821)
15/08/16 12:51:40 INFO Executor: Running task 5.0 in stage 9.0 (TID 824)
15/08/16 12:51:40 INFO Executor: Running task 7.0 in stage 9.0 (TID 826)
15/08/16 12:51:40 INFO Executor: Running task 0.0 in stage 9.0 (TID 819)
15/08/16 12:51:40 INFO Executor: Running task 14.0 in stage 9.0 (TID 833)
15/08/16 12:51:40 INFO Executor: Running task 3.0 in stage 9.0 (TID 822)
15/08/16 12:51:40 INFO Executor: Running task 4.0 in stage 9.0 (TID 823)
15/08/16 12:51:40 INFO Executor: Running task 12.0 in stage 9.0 (TID 831)
15/08/16 12:51:40 INFO Executor: Running task 6.0 in stage 9.0 (TID 825)
15/08/16 12:51:40 INFO Executor: Running task 10.0 in stage 9.0 (TID 829)
15/08/16 12:51:40 INFO Executor: Running task 11.0 in stage 9.0 (TID 830)
15/08/16 12:51:40 INFO Executor: Running task 13.0 in stage 9.0 (TID 832)
15/08/16 12:51:40 INFO Executor: Running task 8.0 in stage 9.0 (TID 827)
15/08/16 12:51:40 INFO Executor: Running task 9.0 in stage 9.0 (TID 828)
15/08/16 12:51:40 INFO Executor: Running task 15.0 in stage 9.0 (TID 834)
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00104-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39589 length: 39589 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00091-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39655 length: 39655 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00121-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39567 length: 39567 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00061-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39619 length: 39619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00083-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39757 length: 39757 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00035-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39630 length: 39630 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00021-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39545 length: 39545 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00171-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39348 length: 39348 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00087-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39527 length: 39527 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00173-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39426 length: 39426 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00130-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39540 length: 39540 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00128-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39574 length: 39574 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00138-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39446 length: 39446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00136-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39781 length: 39781 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00120-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39566 length: 39566 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00008-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39469 length: 39469 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:40 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[42] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6875 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6880 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6892 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6882 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6870 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6880 records.
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(7608) called with curMem=508621583, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 7.4 KB, free 2.6 GB)
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6900 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6905 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6914 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6869 records.
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(4009) called with curMem=508629191, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.9 KB, free 2.6 GB)
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6854 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6901 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6865 records.
15/08/16 12:51:40 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:36543 (size: 3.9 KB, free: 3.1 GB)
15/08/16 12:51:40 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:40 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[42] at processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO TaskSchedulerImpl: Adding task set 10.0 with 8 tasks
15/08/16 12:51:40 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(7808) called with curMem=508633200, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 7.6 KB, free 2.6 GB)
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(4097) called with curMem=508641008, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.0 KB, free 2.6 GB)
15/08/16 12:51:40 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:36543 (size: 4.0 KB, free: 3.1 GB)
15/08/16 12:51:40 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:40 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO TaskSchedulerImpl: Adding task set 11.0 with 8 tasks
15/08/16 12:51:40 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(9752) called with curMem=508645105, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.5 KB, free 2.6 GB)
15/08/16 12:51:40 INFO MemoryStore: ensureFreeSpace(4996) called with curMem=508654857, maxMem=3333968363
15/08/16 12:51:40 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.9 KB, free 2.6 GB)
15/08/16 12:51:40 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:36543 (size: 4.9 KB, free: 3.1 GB)
15/08/16 12:51:40 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at processCmd at CliDriver.java:423)
15/08/16 12:51:40 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6854
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6872
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6878
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6880
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6882
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 6865
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 6870
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 6892
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 6905
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 6875
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 6869
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 6872
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 6901
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 6914
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 6880
15/08/16 12:51:40 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/16 12:51:40 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 6900
15/08/16 12:51:41 INFO Executor: Finished task 7.0 in stage 9.0 (TID 826). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 835, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO Executor: Running task 16.0 in stage 9.0 (TID 835)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 826) in 1420 ms on localhost (1/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00038-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39633 length: 39633 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO Executor: Finished task 6.0 in stage 9.0 (TID 825). 2125 bytes result sent to driver
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO Executor: Finished task 8.0 in stage 9.0 (TID 827). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 836, localhost, ANY, 1692 bytes)
15/08/16 12:51:41 INFO Executor: Running task 17.0 in stage 9.0 (TID 836)
15/08/16 12:51:41 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 837, localhost, ANY, 1692 bytes)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00007-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39576 length: 39576 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 825) in 1431 ms on localhost (2/200)
15/08/16 12:51:41 INFO Executor: Running task 18.0 in stage 9.0 (TID 837)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 827) in 1432 ms on localhost (3/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00018-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39445 length: 39445 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6891 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6865 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6857 records.
15/08/16 12:51:41 INFO Executor: Finished task 12.0 in stage 9.0 (TID 831). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 838, localhost, ANY, 1691 bytes)
15/08/16 12:51:41 INFO Executor: Running task 19.0 in stage 9.0 (TID 838)
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00023-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39286 length: 39286 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6865
15/08/16 12:51:41 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 831) in 1441 ms on localhost (4/200)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6891
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6836 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6857
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO Executor: Finished task 0.0 in stage 9.0 (TID 819). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 6836
15/08/16 12:51:41 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 839, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO Executor: Finished task 9.0 in stage 9.0 (TID 828). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Running task 20.0 in stage 9.0 (TID 839)
15/08/16 12:51:41 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 840, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO Executor: Running task 21.0 in stage 9.0 (TID 840)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00113-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39633 length: 39633 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 828) in 1464 ms on localhost (5/200)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 819) in 1469 ms on localhost (6/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00163-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39629 length: 39629 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6881 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6908 records.
15/08/16 12:51:41 INFO Executor: Finished task 3.0 in stage 9.0 (TID 822). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Finished task 15.0 in stage 9.0 (TID 834). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Finished task 11.0 in stage 9.0 (TID 830). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 841, localhost, ANY, 1692 bytes)
15/08/16 12:51:41 INFO Executor: Finished task 13.0 in stage 9.0 (TID 832). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Finished task 14.0 in stage 9.0 (TID 833). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 842, localhost, ANY, 1691 bytes)
15/08/16 12:51:41 INFO Executor: Running task 22.0 in stage 9.0 (TID 841)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 822) in 1478 ms on localhost (7/200)
15/08/16 12:51:41 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 843, localhost, ANY, 1694 bytes)
15/08/16 12:51:41 INFO Executor: Finished task 1.0 in stage 9.0 (TID 820). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Running task 24.0 in stage 9.0 (TID 843)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00114-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39704 length: 39704 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO Executor: Finished task 10.0 in stage 9.0 (TID 829). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 844, localhost, ANY, 1694 bytes)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO Executor: Running task 25.0 in stage 9.0 (TID 844)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 834) in 1476 ms on localhost (8/200)
15/08/16 12:51:41 INFO Executor: Finished task 2.0 in stage 9.0 (TID 821). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Finished task 4.0 in stage 9.0 (TID 823). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO Executor: Running task 23.0 in stage 9.0 (TID 842)
15/08/16 12:51:41 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 845, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00156-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39756 length: 39756 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 830) in 1479 ms on localhost (9/200)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 833) in 1478 ms on localhost (10/200)
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00144-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39550 length: 39550 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 832) in 1479 ms on localhost (11/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00189-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39613 length: 39613 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO Executor: Running task 26.0 in stage 9.0 (TID 845)
15/08/16 12:51:41 INFO Executor: Finished task 5.0 in stage 9.0 (TID 824). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6881
15/08/16 12:51:41 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 846, localhost, ANY, 1691 bytes)
15/08/16 12:51:41 INFO Executor: Running task 27.0 in stage 9.0 (TID 846)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00197-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39652 length: 39652 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 847, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6891 records.
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO Executor: Running task 28.0 in stage 9.0 (TID 847)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 820) in 1487 ms on localhost (12/200)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00005-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39823 length: 39823 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 829) in 1484 ms on localhost (13/200)
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00155-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39491 length: 39491 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 848, localhost, ANY, 1694 bytes)
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO Executor: Running task 29.0 in stage 9.0 (TID 848)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 821) in 1489 ms on localhost (14/200)
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6908
15/08/16 12:51:41 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 849, localhost, ANY, 1693 bytes)
15/08/16 12:51:41 INFO Executor: Running task 30.0 in stage 9.0 (TID 849)
15/08/16 12:51:41 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 850, localhost, ANY, 1690 bytes)
15/08/16 12:51:41 INFO Executor: Running task 31.0 in stage 9.0 (TID 850)
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6892 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6867 records.
15/08/16 12:51:41 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 823) in 1491 ms on localhost (15/200)
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6889 records.
15/08/16 12:51:41 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 824) in 1491 ms on localhost (16/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00049-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39380 length: 39380 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00022-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39700 length: 39700 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00086-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39546 length: 39546 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6903 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6891
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6889
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6891 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6915 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6879 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6903 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6892
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6903
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6844 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 6867
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6891
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6903
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6879
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6915
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6844
15/08/16 12:51:41 INFO Executor: Finished task 16.0 in stage 9.0 (TID 835). 2125 bytes result sent to driver
15/08/16 12:51:41 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 851, localhost, ANY, 1692 bytes)
15/08/16 12:51:41 INFO Executor: Running task 32.0 in stage 9.0 (TID 851)
15/08/16 12:51:41 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 835) in 305 ms on localhost (17/200)
15/08/16 12:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00181-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39576 length: 39576 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6878
15/08/16 12:51:42 INFO Executor: Finished task 17.0 in stage 9.0 (TID 836). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 18.0 in stage 9.0 (TID 837). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 852, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 33.0 in stage 9.0 (TID 852)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00116-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39582 length: 39582 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6865 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 853, localhost, ANY, 1690 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 836) in 467 ms on localhost (18/200)
15/08/16 12:51:42 INFO Executor: Running task 34.0 in stage 9.0 (TID 853)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 837) in 462 ms on localhost (19/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00065-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39533 length: 39533 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6880 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6865
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6880
15/08/16 12:51:42 INFO Executor: Finished task 19.0 in stage 9.0 (TID 838). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 854, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 35.0 in stage 9.0 (TID 854)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 838) in 488 ms on localhost (20/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00158-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39501 length: 39501 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6855 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 6855
15/08/16 12:51:42 INFO Executor: Finished task 20.0 in stage 9.0 (TID 839). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 28.0 in stage 9.0 (TID 847). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 21.0 in stage 9.0 (TID 840). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 855, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 36.0 in stage 9.0 (TID 855)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00111-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39431 length: 39431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 856, localhost, ANY, 1691 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 839) in 578 ms on localhost (21/200)
15/08/16 12:51:42 INFO Executor: Running task 37.0 in stage 9.0 (TID 856)
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00122-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39282 length: 39282 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 857, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 38.0 in stage 9.0 (TID 857)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 847) in 561 ms on localhost (22/200)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 840) in 583 ms on localhost (23/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00034-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39378 length: 39378 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6861 records.
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6821 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6867 records.
15/08/16 12:51:42 INFO Executor: Finished task 23.0 in stage 9.0 (TID 842). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 858, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 39.0 in stage 9.0 (TID 858)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6821
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6861
15/08/16 12:51:42 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 842) in 583 ms on localhost (24/200)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00170-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39618 length: 39618 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6867
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO Executor: Finished task 30.0 in stage 9.0 (TID 849). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 27.0 in stage 9.0 (TID 846). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 22.0 in stage 9.0 (TID 841). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 25.0 in stage 9.0 (TID 844). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 31.0 in stage 9.0 (TID 850). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 24.0 in stage 9.0 (TID 843). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 26.0 in stage 9.0 (TID 845). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 29.0 in stage 9.0 (TID 848). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 859, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO Executor: Running task 40.0 in stage 9.0 (TID 859)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00159-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39644 length: 39644 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 860, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 41.0 in stage 9.0 (TID 860)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 849) in 587 ms on localhost (25/200)
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6879 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6872
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00016-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39581 length: 39581 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 861, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 846) in 597 ms on localhost (26/200)
15/08/16 12:51:42 INFO Executor: Running task 42.0 in stage 9.0 (TID 861)
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00067-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39590 length: 39590 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 862, localhost, ANY, 1691 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 841) in 610 ms on localhost (27/200)
15/08/16 12:51:42 INFO Executor: Running task 43.0 in stage 9.0 (TID 862)
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00074-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39517 length: 39517 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 863, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 44.0 in stage 9.0 (TID 863)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 844) in 607 ms on localhost (28/200)
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00160-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39445 length: 39445 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 864, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 850) in 597 ms on localhost (29/200)
15/08/16 12:51:42 INFO Executor: Running task 45.0 in stage 9.0 (TID 864)
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6879
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:42 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 843) in 611 ms on localhost (30/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00195-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39604 length: 39604 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6870 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 865, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 845) in 612 ms on localhost (31/200)
15/08/16 12:51:42 INFO Executor: Running task 46.0 in stage 9.0 (TID 865)
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6892 records.
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00187-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39651 length: 39651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6897 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 866, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 47.0 in stage 9.0 (TID 866)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00095-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39522 length: 39522 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 848) in 613 ms on localhost (32/200)
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6892 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6880 records.
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6872
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6883 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 6892
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6880
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6892
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6897
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6883
15/08/16 12:51:42 INFO Executor: Finished task 32.0 in stage 9.0 (TID 851). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6870
15/08/16 12:51:42 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 867, localhost, ANY, 1691 bytes)
15/08/16 12:51:42 INFO Executor: Running task 48.0 in stage 9.0 (TID 867)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 851) in 426 ms on localhost (33/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00140-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39286 length: 39286 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6850 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6850
15/08/16 12:51:42 INFO Executor: Finished task 33.0 in stage 9.0 (TID 852). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 34.0 in stage 9.0 (TID 853). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 868, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 49.0 in stage 9.0 (TID 868)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00139-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39508 length: 39508 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6859 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 869, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 852) in 347 ms on localhost (34/200)
15/08/16 12:51:42 INFO Executor: Running task 50.0 in stage 9.0 (TID 869)
15/08/16 12:51:42 INFO Executor: Finished task 35.0 in stage 9.0 (TID 854). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00162-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39581 length: 39581 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6883 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6859
15/08/16 12:51:42 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 870, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 853) in 347 ms on localhost (35/200)
15/08/16 12:51:42 INFO Executor: Running task 51.0 in stage 9.0 (TID 870)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 854) in 309 ms on localhost (36/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00041-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39555 length: 39555 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6883
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6885 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6885
15/08/16 12:51:42 INFO Executor: Finished task 36.0 in stage 9.0 (TID 855). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 38.0 in stage 9.0 (TID 857). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 871, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 52.0 in stage 9.0 (TID 871)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00129-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39794 length: 39794 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO Executor: Finished task 37.0 in stage 9.0 (TID 856). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6907 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 872, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO Executor: Running task 53.0 in stage 9.0 (TID 872)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 855) in 440 ms on localhost (37/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00150-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39884 length: 39884 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6928 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6907
15/08/16 12:51:42 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 873, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 857) in 428 ms on localhost (38/200)
15/08/16 12:51:42 INFO Executor: Running task 54.0 in stage 9.0 (TID 873)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 856) in 434 ms on localhost (39/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00119-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39746 length: 39746 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6928
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6886 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6886
15/08/16 12:51:42 INFO Executor: Finished task 39.0 in stage 9.0 (TID 858). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 40.0 in stage 9.0 (TID 859). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 874, localhost, ANY, 1691 bytes)
15/08/16 12:51:42 INFO Executor: Running task 55.0 in stage 9.0 (TID 874)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00188-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39481 length: 39481 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6865 records.
15/08/16 12:51:42 INFO Executor: Finished task 47.0 in stage 9.0 (TID 866). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 46.0 in stage 9.0 (TID 865). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 43.0 in stage 9.0 (TID 862). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 875, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 858) in 498 ms on localhost (40/200)
15/08/16 12:51:42 INFO Executor: Running task 56.0 in stage 9.0 (TID 875)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6865
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00093-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39317 length: 39317 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO Executor: Finished task 45.0 in stage 9.0 (TID 864). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6826 records.
15/08/16 12:51:42 INFO Executor: Finished task 44.0 in stage 9.0 (TID 863). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 876, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 57.0 in stage 9.0 (TID 876)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 859) in 496 ms on localhost (41/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00172-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39483 length: 39483 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO Executor: Finished task 41.0 in stage 9.0 (TID 860). 2125 bytes result sent to driver
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 877, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 866) in 472 ms on localhost (42/200)
15/08/16 12:51:42 INFO Executor: Running task 58.0 in stage 9.0 (TID 877)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6826
15/08/16 12:51:42 INFO Executor: Finished task 48.0 in stage 9.0 (TID 867). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6860 records.
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00164-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39608 length: 39608 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 878, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO Executor: Running task 59.0 in stage 9.0 (TID 878)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 865) in 483 ms on localhost (43/200)
15/08/16 12:51:42 INFO Executor: Finished task 42.0 in stage 9.0 (TID 861). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00142-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39503 length: 39503 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6894 records.
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6860
15/08/16 12:51:42 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 879, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO Executor: Running task 60.0 in stage 9.0 (TID 879)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 862) in 497 ms on localhost (44/200)
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6867 records.
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00196-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39294 length: 39294 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6894
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 880, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 864) in 500 ms on localhost (45/200)
15/08/16 12:51:42 INFO Executor: Running task 61.0 in stage 9.0 (TID 880)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6867
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6833 records.
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00153-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39725 length: 39725 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 881, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 62.0 in stage 9.0 (TID 881)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 863) in 510 ms on localhost (46/200)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 860) in 525 ms on localhost (47/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00118-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39644 length: 39644 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6897 records.
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO Executor: Finished task 49.0 in stage 9.0 (TID 868). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6897
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 6833
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6899 records.
15/08/16 12:51:42 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 882, localhost, ANY, 1692 bytes)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 867) in 495 ms on localhost (48/200)
15/08/16 12:51:42 INFO Executor: Running task 63.0 in stage 9.0 (TID 882)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00056-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39473 length: 39473 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO Executor: Finished task 50.0 in stage 9.0 (TID 869). 2125 bytes result sent to driver
15/08/16 12:51:42 INFO Executor: Finished task 51.0 in stage 9.0 (TID 870). 2125 bytes result sent to driver
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 883, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO Executor: Running task 64.0 in stage 9.0 (TID 883)
15/08/16 12:51:42 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 884, localhost, ANY, 1694 bytes)
15/08/16 12:51:42 INFO Executor: Running task 65.0 in stage 9.0 (TID 884)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 861) in 538 ms on localhost (49/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00076-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39838 length: 39838 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 885, localhost, ANY, 1691 bytes)
15/08/16 12:51:42 INFO Executor: Running task 66.0 in stage 9.0 (TID 885)
15/08/16 12:51:42 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 886, localhost, ANY, 1693 bytes)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00149-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39629 length: 39629 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 INFO Executor: Running task 67.0 in stage 9.0 (TID 886)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 868) in 431 ms on localhost (50/200)
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00106-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39474 length: 39474 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00101-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39747 length: 39747 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 870) in 403 ms on localhost (51/200)
15/08/16 12:51:42 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 869) in 417 ms on localhost (52/200)
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6899
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6877 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6896 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6858 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6879 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6877
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6896
15/08/16 12:51:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6911 records.
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6858
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6879
15/08/16 12:51:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6911
15/08/16 12:51:43 INFO Executor: Finished task 52.0 in stage 9.0 (TID 871). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 887, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 68.0 in stage 9.0 (TID 887)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 871) in 348 ms on localhost (53/200)
15/08/16 12:51:43 INFO Executor: Finished task 54.0 in stage 9.0 (TID 873). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00071-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39545 length: 39545 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 888, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 69.0 in stage 9.0 (TID 888)
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 873) in 327 ms on localhost (54/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00133-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39730 length: 39730 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6891 records.
15/08/16 12:51:43 INFO Executor: Finished task 53.0 in stage 9.0 (TID 872). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 889, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 70.0 in stage 9.0 (TID 889)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 872) in 351 ms on localhost (55/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00080-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39411 length: 39411 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6891
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6872
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6854 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6854
15/08/16 12:51:43 INFO Executor: Finished task 55.0 in stage 9.0 (TID 874). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 890, localhost, ANY, 1691 bytes)
15/08/16 12:51:43 INFO Executor: Running task 71.0 in stage 9.0 (TID 890)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 874) in 354 ms on localhost (56/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00088-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39778 length: 39778 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6900 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6900
15/08/16 12:51:43 INFO Executor: Finished task 56.0 in stage 9.0 (TID 875). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 891, localhost, ANY, 1691 bytes)
15/08/16 12:51:43 INFO Executor: Running task 72.0 in stage 9.0 (TID 891)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 875) in 359 ms on localhost (57/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00064-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39684 length: 39684 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6907 records.
15/08/16 12:51:43 INFO Executor: Finished task 57.0 in stage 9.0 (TID 876). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 892, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 73.0 in stage 9.0 (TID 892)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 876) in 352 ms on localhost (58/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00115-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39727 length: 39727 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6907
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6910 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6910
15/08/16 12:51:43 INFO Executor: Finished task 59.0 in stage 9.0 (TID 878). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 893, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 74.0 in stage 9.0 (TID 893)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 878) in 360 ms on localhost (59/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00070-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39611 length: 39611 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:43 INFO Executor: Finished task 58.0 in stage 9.0 (TID 877). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 75.0 in stage 9.0 (TID 894, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 75.0 in stage 9.0 (TID 894)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 877) in 380 ms on localhost (60/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00015-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39580 length: 39580 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6878
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6876 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6876
15/08/16 12:51:43 INFO Executor: Finished task 60.0 in stage 9.0 (TID 879). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 76.0 in stage 9.0 (TID 895, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Finished task 62.0 in stage 9.0 (TID 881). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO Executor: Running task 76.0 in stage 9.0 (TID 895)
15/08/16 12:51:43 INFO TaskSetManager: Starting task 77.0 in stage 9.0 (TID 896, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 77.0 in stage 9.0 (TID 896)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 881) in 408 ms on localhost (61/200)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 879) in 423 ms on localhost (62/200)
15/08/16 12:51:43 INFO Executor: Finished task 61.0 in stage 9.0 (TID 880). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00090-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39352 length: 39352 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00079-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39793 length: 39793 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO TaskSetManager: Starting task 78.0 in stage 9.0 (TID 897, localhost, ANY, 1691 bytes)
15/08/16 12:51:43 INFO Executor: Running task 78.0 in stage 9.0 (TID 897)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 880) in 890 ms on localhost (63/200)
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00053-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39472 length: 39472 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6923 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6862 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6923
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6862
15/08/16 12:51:43 INFO Executor: Finished task 66.0 in stage 9.0 (TID 885). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO Executor: Finished task 63.0 in stage 9.0 (TID 882). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO Executor: Finished task 67.0 in stage 9.0 (TID 886). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 79.0 in stage 9.0 (TID 898, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 79.0 in stage 9.0 (TID 898)
15/08/16 12:51:43 INFO TaskSetManager: Starting task 80.0 in stage 9.0 (TID 899, localhost, ANY, 1690 bytes)
15/08/16 12:51:43 INFO Executor: Running task 80.0 in stage 9.0 (TID 899)
15/08/16 12:51:43 INFO TaskSetManager: Starting task 81.0 in stage 9.0 (TID 900, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 885) in 881 ms on localhost (64/200)
15/08/16 12:51:43 INFO Executor: Running task 81.0 in stage 9.0 (TID 900)
15/08/16 12:51:43 INFO Executor: Finished task 65.0 in stage 9.0 (TID 884). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00002-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39297 length: 39297 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00013-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39606 length: 39606 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00039-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39334 length: 39334 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO TaskSetManager: Starting task 82.0 in stage 9.0 (TID 901, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 82.0 in stage 9.0 (TID 901)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 882) in 903 ms on localhost (65/200)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 886) in 882 ms on localhost (66/200)
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 884) in 885 ms on localhost (67/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00075-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39565 length: 39565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO Executor: Finished task 64.0 in stage 9.0 (TID 883). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 83.0 in stage 9.0 (TID 902, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 83.0 in stage 9.0 (TID 902)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 883) in 894 ms on localhost (68/200)
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6851 records.
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00055-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39575 length: 39575 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6857 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6876 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6877 records.
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6876
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6877
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6851
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6857
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6878
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6860 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6860
15/08/16 12:51:43 INFO Executor: Finished task 69.0 in stage 9.0 (TID 888). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 84.0 in stage 9.0 (TID 903, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO Executor: Running task 84.0 in stage 9.0 (TID 903)
15/08/16 12:51:43 INFO Executor: Finished task 68.0 in stage 9.0 (TID 887). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 85.0 in stage 9.0 (TID 904, localhost, ANY, 1691 bytes)
15/08/16 12:51:43 INFO Executor: Running task 85.0 in stage 9.0 (TID 904)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 888) in 845 ms on localhost (69/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00048-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39507 length: 39507 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 887) in 851 ms on localhost (70/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00000-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39464 length: 39464 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO Executor: Finished task 70.0 in stage 9.0 (TID 889). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 86.0 in stage 9.0 (TID 905, localhost, ANY, 1691 bytes)
15/08/16 12:51:43 INFO Executor: Running task 86.0 in stage 9.0 (TID 905)
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6857 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6860 records.
15/08/16 12:51:43 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 889) in 844 ms on localhost (71/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00058-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39426 length: 39426 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6867 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6857
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6860
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6867
15/08/16 12:51:43 INFO Executor: Finished task 71.0 in stage 9.0 (TID 890). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 87.0 in stage 9.0 (TID 906, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 87.0 in stage 9.0 (TID 906)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 890) in 833 ms on localhost (72/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00102-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39554 length: 39554 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO Executor: Finished task 72.0 in stage 9.0 (TID 891). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 88.0 in stage 9.0 (TID 907, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 88.0 in stage 9.0 (TID 907)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00109-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39665 length: 39665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 891) in 812 ms on localhost (73/200)
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6869 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6885 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6885
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6869
15/08/16 12:51:43 INFO Executor: Finished task 73.0 in stage 9.0 (TID 892). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 89.0 in stage 9.0 (TID 908, localhost, ANY, 1692 bytes)
15/08/16 12:51:43 INFO Executor: Running task 89.0 in stage 9.0 (TID 908)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 892) in 835 ms on localhost (74/200)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00168-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39318 length: 39318 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6833 records.
15/08/16 12:51:43 INFO Executor: Finished task 74.0 in stage 9.0 (TID 893). 2125 bytes result sent to driver
15/08/16 12:51:43 INFO TaskSetManager: Starting task 90.0 in stage 9.0 (TID 909, localhost, ANY, 1693 bytes)
15/08/16 12:51:43 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 893) in 824 ms on localhost (75/200)
15/08/16 12:51:43 INFO Executor: Running task 90.0 in stage 9.0 (TID 909)
15/08/16 12:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00078-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39338 length: 39338 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6833
15/08/16 12:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6841 records.
15/08/16 12:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6841
15/08/16 12:51:44 INFO Executor: Finished task 75.0 in stage 9.0 (TID 894). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 91.0 in stage 9.0 (TID 910, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 91.0 in stage 9.0 (TID 910)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 75.0 in stage 9.0 (TID 894) in 854 ms on localhost (76/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00183-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39503 length: 39503 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6863 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6863
15/08/16 12:51:44 INFO Executor: Finished task 76.0 in stage 9.0 (TID 895). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 77.0 in stage 9.0 (TID 896). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 92.0 in stage 9.0 (TID 911, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 92.0 in stage 9.0 (TID 911)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 93.0 in stage 9.0 (TID 912, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 93.0 in stage 9.0 (TID 912)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 76.0 in stage 9.0 (TID 895) in 865 ms on localhost (77/200)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 77.0 in stage 9.0 (TID 896) in 864 ms on localhost (78/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00146-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39700 length: 39700 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00043-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39824 length: 39824 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6882 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6917 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6882
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6917
15/08/16 12:51:44 INFO Executor: Finished task 82.0 in stage 9.0 (TID 901). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 79.0 in stage 9.0 (TID 898). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 94.0 in stage 9.0 (TID 913, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 95.0 in stage 9.0 (TID 914, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 95.0 in stage 9.0 (TID 914)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 82.0 in stage 9.0 (TID 901) in 393 ms on localhost (79/200)
15/08/16 12:51:44 INFO Executor: Running task 94.0 in stage 9.0 (TID 913)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 79.0 in stage 9.0 (TID 898) in 396 ms on localhost (80/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00062-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39661 length: 39661 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00051-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39365 length: 39365 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO Executor: Finished task 80.0 in stage 9.0 (TID 899). 2125 bytes result sent to driver
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO TaskSetManager: Starting task 96.0 in stage 9.0 (TID 915, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 96.0 in stage 9.0 (TID 915)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 80.0 in stage 9.0 (TID 899) in 400 ms on localhost (81/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00052-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39771 length: 39771 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:44 INFO Executor: Finished task 83.0 in stage 9.0 (TID 902). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6836 records.
15/08/16 12:51:44 INFO TaskSetManager: Starting task 97.0 in stage 9.0 (TID 916, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 97.0 in stage 9.0 (TID 916)
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6908 records.
15/08/16 12:51:44 INFO TaskSetManager: Finished task 83.0 in stage 9.0 (TID 902) in 399 ms on localhost (82/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00123-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39584 length: 39584 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO Executor: Finished task 81.0 in stage 9.0 (TID 900). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6873 records.
15/08/16 12:51:44 INFO TaskSetManager: Starting task 98.0 in stage 9.0 (TID 917, localhost, ANY, 1691 bytes)
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO Executor: Running task 98.0 in stage 9.0 (TID 917)
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6836
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6908
15/08/16 12:51:44 INFO TaskSetManager: Finished task 81.0 in stage 9.0 (TID 900) in 412 ms on localhost (83/200)
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6878
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00148-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39781 length: 39781 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6873
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6917 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6917
15/08/16 12:51:44 INFO Executor: Finished task 78.0 in stage 9.0 (TID 897). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 99.0 in stage 9.0 (TID 918, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 99.0 in stage 9.0 (TID 918)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 78.0 in stage 9.0 (TID 897) in 946 ms on localhost (84/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00194-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39863 length: 39863 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6931 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6931
15/08/16 12:51:44 INFO Executor: Finished task 84.0 in stage 9.0 (TID 903). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 100.0 in stage 9.0 (TID 919, localhost, ANY, 1691 bytes)
15/08/16 12:51:44 INFO Executor: Running task 100.0 in stage 9.0 (TID 919)
15/08/16 12:51:44 INFO Executor: Finished task 85.0 in stage 9.0 (TID 904). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 101.0 in stage 9.0 (TID 920, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 84.0 in stage 9.0 (TID 903) in 374 ms on localhost (85/200)
15/08/16 12:51:44 INFO Executor: Finished task 86.0 in stage 9.0 (TID 905). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Running task 101.0 in stage 9.0 (TID 920)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 102.0 in stage 9.0 (TID 921, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 102.0 in stage 9.0 (TID 921)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 85.0 in stage 9.0 (TID 904) in 373 ms on localhost (86/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00054-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39549 length: 39549 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO TaskSetManager: Finished task 86.0 in stage 9.0 (TID 905) in 366 ms on localhost (87/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00060-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39687 length: 39687 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00127-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39438 length: 39438 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6876 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6855 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6902 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6855
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6902
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6876
15/08/16 12:51:44 INFO Executor: Finished task 88.0 in stage 9.0 (TID 907). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 103.0 in stage 9.0 (TID 922, localhost, ANY, 1691 bytes)
15/08/16 12:51:44 INFO Executor: Running task 103.0 in stage 9.0 (TID 922)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 88.0 in stage 9.0 (TID 907) in 365 ms on localhost (88/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00198-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39791 length: 39791 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO Executor: Finished task 87.0 in stage 9.0 (TID 906). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 104.0 in stage 9.0 (TID 923, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 104.0 in stage 9.0 (TID 923)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 87.0 in stage 9.0 (TID 906) in 380 ms on localhost (89/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00108-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39761 length: 39761 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6897 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6903 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6897
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6903
15/08/16 12:51:44 INFO Executor: Finished task 89.0 in stage 9.0 (TID 908). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 90.0 in stage 9.0 (TID 909). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 105.0 in stage 9.0 (TID 924, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 105.0 in stage 9.0 (TID 924)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 106.0 in stage 9.0 (TID 925, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 106.0 in stage 9.0 (TID 925)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 89.0 in stage 9.0 (TID 908) in 377 ms on localhost (90/200)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 90.0 in stage 9.0 (TID 909) in 364 ms on localhost (91/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00063-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39439 length: 39439 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00020-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39508 length: 39508 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6844 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6869 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6844
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6869
15/08/16 12:51:44 INFO Executor: Finished task 91.0 in stage 9.0 (TID 910). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 107.0 in stage 9.0 (TID 926, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 107.0 in stage 9.0 (TID 926)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 91.0 in stage 9.0 (TID 910) in 363 ms on localhost (92/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00184-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39640 length: 39640 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6879 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6879
15/08/16 12:51:44 INFO Executor: Finished task 92.0 in stage 9.0 (TID 911). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 108.0 in stage 9.0 (TID 927, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 108.0 in stage 9.0 (TID 927)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 92.0 in stage 9.0 (TID 911) in 381 ms on localhost (93/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00175-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39615 length: 39615 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6873 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6873
15/08/16 12:51:44 INFO Executor: Finished task 93.0 in stage 9.0 (TID 912). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 109.0 in stage 9.0 (TID 928, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 109.0 in stage 9.0 (TID 928)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 93.0 in stage 9.0 (TID 912) in 409 ms on localhost (94/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00110-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39405 length: 39405 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6861 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6861
15/08/16 12:51:44 INFO Executor: Finished task 94.0 in stage 9.0 (TID 913). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 95.0 in stage 9.0 (TID 914). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 96.0 in stage 9.0 (TID 915). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 110.0 in stage 9.0 (TID 929, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 110.0 in stage 9.0 (TID 929)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 111.0 in stage 9.0 (TID 930, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 111.0 in stage 9.0 (TID 930)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 112.0 in stage 9.0 (TID 931, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 94.0 in stage 9.0 (TID 913) in 402 ms on localhost (95/200)
15/08/16 12:51:44 INFO Executor: Running task 112.0 in stage 9.0 (TID 931)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 95.0 in stage 9.0 (TID 914) in 401 ms on localhost (96/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00011-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39463 length: 39463 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00167-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39609 length: 39609 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00126-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39403 length: 39403 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO TaskSetManager: Finished task 96.0 in stage 9.0 (TID 915) in 399 ms on localhost (97/200)
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO Executor: Finished task 97.0 in stage 9.0 (TID 916). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 113.0 in stage 9.0 (TID 932, localhost, ANY, 1691 bytes)
15/08/16 12:51:44 INFO Executor: Running task 113.0 in stage 9.0 (TID 932)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 97.0 in stage 9.0 (TID 916) in 400 ms on localhost (98/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00094-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39459 length: 39459 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6877 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6859 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6856 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6851 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6859
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6856
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6851
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6877
15/08/16 12:51:44 INFO Executor: Finished task 98.0 in stage 9.0 (TID 917). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 114.0 in stage 9.0 (TID 933, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 98.0 in stage 9.0 (TID 917) in 411 ms on localhost (99/200)
15/08/16 12:51:44 INFO Executor: Running task 114.0 in stage 9.0 (TID 933)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00176-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39917 length: 39917 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6941 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 6941
15/08/16 12:51:44 INFO Executor: Finished task 99.0 in stage 9.0 (TID 918). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 115.0 in stage 9.0 (TID 934, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 115.0 in stage 9.0 (TID 934)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 99.0 in stage 9.0 (TID 918) in 406 ms on localhost (100/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00185-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39355 length: 39355 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6863 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6863
15/08/16 12:51:44 INFO Executor: Finished task 100.0 in stage 9.0 (TID 919). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 102.0 in stage 9.0 (TID 921). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 116.0 in stage 9.0 (TID 935, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 116.0 in stage 9.0 (TID 935)
15/08/16 12:51:44 INFO Executor: Finished task 101.0 in stage 9.0 (TID 920). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 117.0 in stage 9.0 (TID 936, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 117.0 in stage 9.0 (TID 936)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 118.0 in stage 9.0 (TID 937, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 118.0 in stage 9.0 (TID 937)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 102.0 in stage 9.0 (TID 921) in 408 ms on localhost (101/200)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 100.0 in stage 9.0 (TID 919) in 412 ms on localhost (102/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00157-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39908 length: 39908 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00068-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39670 length: 39670 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00092-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39859 length: 39859 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO TaskSetManager: Finished task 101.0 in stage 9.0 (TID 920) in 412 ms on localhost (103/200)
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6919 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6930 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6894 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6894
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6930
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6919
15/08/16 12:51:44 INFO Executor: Finished task 104.0 in stage 9.0 (TID 923). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 103.0 in stage 9.0 (TID 922). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 119.0 in stage 9.0 (TID 938, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 120.0 in stage 9.0 (TID 939, localhost, ANY, 1691 bytes)
15/08/16 12:51:44 INFO Executor: Running task 120.0 in stage 9.0 (TID 939)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 104.0 in stage 9.0 (TID 923) in 394 ms on localhost (104/200)
15/08/16 12:51:44 INFO Executor: Running task 119.0 in stage 9.0 (TID 938)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 103.0 in stage 9.0 (TID 922) in 401 ms on localhost (105/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00027-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39470 length: 39470 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00132-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39616 length: 39616 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6859 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6882 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6859
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6882
15/08/16 12:51:44 INFO Executor: Finished task 105.0 in stage 9.0 (TID 924). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 121.0 in stage 9.0 (TID 940, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 121.0 in stage 9.0 (TID 940)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 105.0 in stage 9.0 (TID 924) in 395 ms on localhost (106/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00073-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39561 length: 39561 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6886 records.
15/08/16 12:51:44 INFO Executor: Finished task 106.0 in stage 9.0 (TID 925). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 122.0 in stage 9.0 (TID 941, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 122.0 in stage 9.0 (TID 941)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 106.0 in stage 9.0 (TID 925) in 406 ms on localhost (107/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00117-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39623 length: 39623 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6908 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6886
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6908
15/08/16 12:51:44 INFO Executor: Finished task 107.0 in stage 9.0 (TID 926). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 123.0 in stage 9.0 (TID 942, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 123.0 in stage 9.0 (TID 942)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 107.0 in stage 9.0 (TID 926) in 398 ms on localhost (108/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00190-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39502 length: 39502 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6858 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6858
15/08/16 12:51:44 INFO Executor: Finished task 108.0 in stage 9.0 (TID 927). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 124.0 in stage 9.0 (TID 943, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 124.0 in stage 9.0 (TID 943)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00033-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39508 length: 39508 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO TaskSetManager: Finished task 108.0 in stage 9.0 (TID 927) in 399 ms on localhost (109/200)
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6859 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6859
15/08/16 12:51:44 INFO Executor: Finished task 109.0 in stage 9.0 (TID 928). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 125.0 in stage 9.0 (TID 944, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO Executor: Running task 125.0 in stage 9.0 (TID 944)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 109.0 in stage 9.0 (TID 928) in 397 ms on localhost (110/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00143-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39667 length: 39667 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6885 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 6885
15/08/16 12:51:44 INFO Executor: Finished task 112.0 in stage 9.0 (TID 931). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 111.0 in stage 9.0 (TID 930). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO Executor: Finished task 113.0 in stage 9.0 (TID 932). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 126.0 in stage 9.0 (TID 945, localhost, ANY, 1692 bytes)
15/08/16 12:51:44 INFO Executor: Running task 126.0 in stage 9.0 (TID 945)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 127.0 in stage 9.0 (TID 946, localhost, ANY, 1690 bytes)
15/08/16 12:51:44 INFO Executor: Running task 127.0 in stage 9.0 (TID 946)
15/08/16 12:51:44 INFO TaskSetManager: Starting task 128.0 in stage 9.0 (TID 947, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 112.0 in stage 9.0 (TID 931) in 411 ms on localhost (111/200)
15/08/16 12:51:44 INFO Executor: Running task 128.0 in stage 9.0 (TID 947)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 111.0 in stage 9.0 (TID 930) in 413 ms on localhost (112/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00001-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39273 length: 39273 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00085-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39419 length: 39419 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 INFO TaskSetManager: Finished task 113.0 in stage 9.0 (TID 932) in 409 ms on localhost (113/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00107-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39764 length: 39764 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6918 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6832 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6847 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6918
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6832
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6847
15/08/16 12:51:44 INFO Executor: Finished task 110.0 in stage 9.0 (TID 929). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 129.0 in stage 9.0 (TID 948, localhost, ANY, 1694 bytes)
15/08/16 12:51:44 INFO Executor: Running task 129.0 in stage 9.0 (TID 948)
15/08/16 12:51:44 INFO TaskSetManager: Finished task 110.0 in stage 9.0 (TID 929) in 437 ms on localhost (114/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00105-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39850 length: 39850 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6923 records.
15/08/16 12:51:44 INFO Executor: Finished task 114.0 in stage 9.0 (TID 933). 2125 bytes result sent to driver
15/08/16 12:51:44 INFO TaskSetManager: Starting task 130.0 in stage 9.0 (TID 949, localhost, ANY, 1693 bytes)
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO Executor: Running task 130.0 in stage 9.0 (TID 949)
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6923
15/08/16 12:51:44 INFO TaskSetManager: Finished task 114.0 in stage 9.0 (TID 933) in 428 ms on localhost (115/200)
15/08/16 12:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00019-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39698 length: 39698 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6895 records.
15/08/16 12:51:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6895
15/08/16 12:51:44 INFO Executor: Finished task 115.0 in stage 9.0 (TID 934). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 131.0 in stage 9.0 (TID 950, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 131.0 in stage 9.0 (TID 950)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 115.0 in stage 9.0 (TID 934) in 439 ms on localhost (116/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00100-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39646 length: 39646 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6907 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6907
15/08/16 12:51:45 INFO Executor: Finished task 116.0 in stage 9.0 (TID 935). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO Executor: Finished task 118.0 in stage 9.0 (TID 937). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 132.0 in stage 9.0 (TID 951, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO Executor: Running task 132.0 in stage 9.0 (TID 951)
15/08/16 12:51:45 INFO TaskSetManager: Starting task 133.0 in stage 9.0 (TID 952, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 133.0 in stage 9.0 (TID 952)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 116.0 in stage 9.0 (TID 935) in 436 ms on localhost (117/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 118.0 in stage 9.0 (TID 937) in 435 ms on localhost (118/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00192-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39558 length: 39558 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00179-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39711 length: 39711 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO Executor: Finished task 117.0 in stage 9.0 (TID 936). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 134.0 in stage 9.0 (TID 953, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 134.0 in stage 9.0 (TID 953)
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO TaskSetManager: Finished task 117.0 in stage 9.0 (TID 936) in 439 ms on localhost (119/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00112-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39708 length: 39708 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6915 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6914 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6868 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6914
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6868
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6915
15/08/16 12:51:45 INFO Executor: Finished task 119.0 in stage 9.0 (TID 938). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO Executor: Finished task 120.0 in stage 9.0 (TID 939). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 135.0 in stage 9.0 (TID 954, localhost, ANY, 1691 bytes)
15/08/16 12:51:45 INFO Executor: Running task 135.0 in stage 9.0 (TID 954)
15/08/16 12:51:45 INFO TaskSetManager: Starting task 136.0 in stage 9.0 (TID 955, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 136.0 in stage 9.0 (TID 955)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 120.0 in stage 9.0 (TID 939) in 437 ms on localhost (120/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 119.0 in stage 9.0 (TID 938) in 439 ms on localhost (121/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00032-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39712 length: 39712 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00147-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39671 length: 39671 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6904 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6893 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6893
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6904
15/08/16 12:51:45 INFO Executor: Finished task 121.0 in stage 9.0 (TID 940). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 137.0 in stage 9.0 (TID 956, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 137.0 in stage 9.0 (TID 956)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 121.0 in stage 9.0 (TID 940) in 441 ms on localhost (122/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00028-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39643 length: 39643 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO Executor: Finished task 122.0 in stage 9.0 (TID 941). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 138.0 in stage 9.0 (TID 957, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 138.0 in stage 9.0 (TID 957)
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6880 records.
15/08/16 12:51:45 INFO TaskSetManager: Finished task 122.0 in stage 9.0 (TID 941) in 438 ms on localhost (123/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00166-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39490 length: 39490 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6880
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6872
15/08/16 12:51:45 INFO Executor: Finished task 123.0 in stage 9.0 (TID 942). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 139.0 in stage 9.0 (TID 958, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 139.0 in stage 9.0 (TID 958)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 123.0 in stage 9.0 (TID 942) in 432 ms on localhost (124/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00025-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39504 length: 39504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6879 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6879
15/08/16 12:51:45 INFO Executor: Finished task 124.0 in stage 9.0 (TID 943). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 140.0 in stage 9.0 (TID 959, localhost, ANY, 1691 bytes)
15/08/16 12:51:45 INFO Executor: Running task 140.0 in stage 9.0 (TID 959)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 124.0 in stage 9.0 (TID 943) in 429 ms on localhost (125/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00009-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39201 length: 39201 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6822 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6822
15/08/16 12:51:45 INFO Executor: Finished task 125.0 in stage 9.0 (TID 944). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 141.0 in stage 9.0 (TID 960, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 141.0 in stage 9.0 (TID 960)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 125.0 in stage 9.0 (TID 944) in 424 ms on localhost (126/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00010-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39644 length: 39644 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6869 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6869
15/08/16 12:51:45 INFO Executor: Finished task 128.0 in stage 9.0 (TID 947). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO Executor: Finished task 126.0 in stage 9.0 (TID 945). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 142.0 in stage 9.0 (TID 961, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO Executor: Running task 142.0 in stage 9.0 (TID 961)
15/08/16 12:51:45 INFO Executor: Finished task 127.0 in stage 9.0 (TID 946). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 143.0 in stage 9.0 (TID 962, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 143.0 in stage 9.0 (TID 962)
15/08/16 12:51:45 INFO TaskSetManager: Starting task 144.0 in stage 9.0 (TID 963, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 128.0 in stage 9.0 (TID 947) in 424 ms on localhost (127/200)
15/08/16 12:51:45 INFO Executor: Running task 144.0 in stage 9.0 (TID 963)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 126.0 in stage 9.0 (TID 945) in 427 ms on localhost (128/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00174-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39408 length: 39408 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00199-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39334 length: 39334 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO TaskSetManager: Finished task 127.0 in stage 9.0 (TID 946) in 427 ms on localhost (129/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00045-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39433 length: 39433 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6858 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6863 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6845 records.
15/08/16 12:51:45 INFO Executor: Finished task 129.0 in stage 9.0 (TID 948). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 145.0 in stage 9.0 (TID 964, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 145.0 in stage 9.0 (TID 964)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 129.0 in stage 9.0 (TID 948) in 415 ms on localhost (130/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00014-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39340 length: 39340 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6858
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6845
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6863
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6829 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6829
15/08/16 12:51:45 INFO Executor: Finished task 130.0 in stage 9.0 (TID 949). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 146.0 in stage 9.0 (TID 965, localhost, ANY, 1690 bytes)
15/08/16 12:51:45 INFO Executor: Running task 146.0 in stage 9.0 (TID 965)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 130.0 in stage 9.0 (TID 949) in 424 ms on localhost (131/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00036-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39690 length: 39690 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6896 records.
15/08/16 12:51:45 INFO Executor: Finished task 131.0 in stage 9.0 (TID 950). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO TaskSetManager: Starting task 147.0 in stage 9.0 (TID 966, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6896
15/08/16 12:51:45 INFO Executor: Running task 147.0 in stage 9.0 (TID 966)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 131.0 in stage 9.0 (TID 950) in 413 ms on localhost (132/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00137-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39169 length: 39169 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6841 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6841
15/08/16 12:51:45 INFO Executor: Finished task 134.0 in stage 9.0 (TID 953). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO Executor: Finished task 132.0 in stage 9.0 (TID 951). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 148.0 in stage 9.0 (TID 967, localhost, ANY, 1690 bytes)
15/08/16 12:51:45 INFO Executor: Running task 148.0 in stage 9.0 (TID 967)
15/08/16 12:51:45 INFO TaskSetManager: Starting task 149.0 in stage 9.0 (TID 968, localhost, ANY, 1691 bytes)
15/08/16 12:51:45 INFO Executor: Running task 149.0 in stage 9.0 (TID 968)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 134.0 in stage 9.0 (TID 953) in 412 ms on localhost (133/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 132.0 in stage 9.0 (TID 951) in 417 ms on localhost (134/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00084-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39539 length: 39539 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00004-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39418 length: 39418 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO Executor: Finished task 133.0 in stage 9.0 (TID 952). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 150.0 in stage 9.0 (TID 969, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO Executor: Running task 150.0 in stage 9.0 (TID 969)
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO TaskSetManager: Finished task 133.0 in stage 9.0 (TID 952) in 419 ms on localhost (135/200)
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00031-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39568 length: 39568 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6843 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6886 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6883 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6883
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6843
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6886
15/08/16 12:51:45 INFO Executor: Finished task 136.0 in stage 9.0 (TID 955). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 151.0 in stage 9.0 (TID 970, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 151.0 in stage 9.0 (TID 970)
15/08/16 12:51:45 INFO Executor: Finished task 135.0 in stage 9.0 (TID 954). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 152.0 in stage 9.0 (TID 971, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 152.0 in stage 9.0 (TID 971)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 136.0 in stage 9.0 (TID 955) in 429 ms on localhost (136/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 135.0 in stage 9.0 (TID 954) in 431 ms on localhost (137/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00131-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39615 length: 39615 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00098-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39595 length: 39595 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6878 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6881 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6881
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6878
15/08/16 12:51:45 INFO Executor: Finished task 137.0 in stage 9.0 (TID 956). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 153.0 in stage 9.0 (TID 972, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO Executor: Running task 153.0 in stage 9.0 (TID 972)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 137.0 in stage 9.0 (TID 956) in 431 ms on localhost (138/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00006-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39644 length: 39644 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO Executor: Finished task 138.0 in stage 9.0 (TID 957). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 154.0 in stage 9.0 (TID 973, localhost, ANY, 1692 bytes)
15/08/16 12:51:45 INFO Executor: Running task 154.0 in stage 9.0 (TID 973)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 138.0 in stage 9.0 (TID 957) in 431 ms on localhost (139/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00193-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39484 length: 39484 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6896 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6872 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6896
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6872
15/08/16 12:51:45 INFO Executor: Finished task 139.0 in stage 9.0 (TID 958). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 155.0 in stage 9.0 (TID 974, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 155.0 in stage 9.0 (TID 974)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 139.0 in stage 9.0 (TID 958) in 428 ms on localhost (140/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00047-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39454 length: 39454 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6843 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6843
15/08/16 12:51:45 INFO Executor: Finished task 140.0 in stage 9.0 (TID 959). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 156.0 in stage 9.0 (TID 975, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 156.0 in stage 9.0 (TID 975)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 140.0 in stage 9.0 (TID 959) in 601 ms on localhost (141/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00191-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39645 length: 39645 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6894 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO Executor: Finished task 141.0 in stage 9.0 (TID 960). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6894
15/08/16 12:51:45 INFO TaskSetManager: Starting task 157.0 in stage 9.0 (TID 976, localhost, ANY, 1694 bytes)
15/08/16 12:51:45 INFO Executor: Running task 157.0 in stage 9.0 (TID 976)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00077-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39694 length: 39694 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO TaskSetManager: Finished task 141.0 in stage 9.0 (TID 960) in 588 ms on localhost (142/200)
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6887 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6887
15/08/16 12:51:45 INFO Executor: Finished task 142.0 in stage 9.0 (TID 961). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO Executor: Finished task 143.0 in stage 9.0 (TID 962). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 158.0 in stage 9.0 (TID 977, localhost, ANY, 1691 bytes)
15/08/16 12:51:45 INFO Executor: Running task 158.0 in stage 9.0 (TID 977)
15/08/16 12:51:45 INFO Executor: Finished task 144.0 in stage 9.0 (TID 963). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 159.0 in stage 9.0 (TID 978, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 159.0 in stage 9.0 (TID 978)
15/08/16 12:51:45 INFO TaskSetManager: Starting task 160.0 in stage 9.0 (TID 979, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 160.0 in stage 9.0 (TID 979)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 143.0 in stage 9.0 (TID 962) in 598 ms on localhost (143/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 142.0 in stage 9.0 (TID 961) in 600 ms on localhost (144/200)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 144.0 in stage 9.0 (TID 963) in 598 ms on localhost (145/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00151-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39535 length: 39535 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00072-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39324 length: 39324 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00161-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39429 length: 39429 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO Executor: Finished task 145.0 in stage 9.0 (TID 964). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6853 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6876 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6846 records.
15/08/16 12:51:45 INFO TaskSetManager: Starting task 161.0 in stage 9.0 (TID 980, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 161.0 in stage 9.0 (TID 980)
15/08/16 12:51:45 INFO TaskSetManager: Finished task 145.0 in stage 9.0 (TID 964) in 599 ms on localhost (146/200)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00169-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39401 length: 39401 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6858 records.
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6846
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6876
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6853
15/08/16 12:51:45 INFO Executor: Finished task 146.0 in stage 9.0 (TID 965). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6858
15/08/16 12:51:45 INFO TaskSetManager: Starting task 162.0 in stage 9.0 (TID 981, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 162.0 in stage 9.0 (TID 981)
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00145-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39572 length: 39572 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:45 INFO TaskSetManager: Finished task 146.0 in stage 9.0 (TID 965) in 589 ms on localhost (147/200)
15/08/16 12:51:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6882 records.
15/08/16 12:51:45 INFO Executor: Finished task 147.0 in stage 9.0 (TID 966). 2125 bytes result sent to driver
15/08/16 12:51:45 INFO TaskSetManager: Starting task 163.0 in stage 9.0 (TID 982, localhost, ANY, 1693 bytes)
15/08/16 12:51:45 INFO Executor: Running task 163.0 in stage 9.0 (TID 982)
15/08/16 12:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:45 INFO TaskSetManager: Finished task 147.0 in stage 9.0 (TID 966) in 586 ms on localhost (148/200)
15/08/16 12:51:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6882
15/08/16 12:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00012-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39192 length: 39192 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6815 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6815
15/08/16 12:51:46 INFO Executor: Finished task 149.0 in stage 9.0 (TID 968). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 148.0 in stage 9.0 (TID 967). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 150.0 in stage 9.0 (TID 969). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 164.0 in stage 9.0 (TID 983, localhost, ANY, 1690 bytes)
15/08/16 12:51:46 INFO Executor: Running task 164.0 in stage 9.0 (TID 983)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 165.0 in stage 9.0 (TID 984, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 165.0 in stage 9.0 (TID 984)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 166.0 in stage 9.0 (TID 985, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 149.0 in stage 9.0 (TID 968) in 579 ms on localhost (149/200)
15/08/16 12:51:46 INFO Executor: Running task 166.0 in stage 9.0 (TID 985)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 148.0 in stage 9.0 (TID 967) in 582 ms on localhost (150/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00003-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39552 length: 39552 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00059-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39720 length: 39720 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 150.0 in stage 9.0 (TID 969) in 578 ms on localhost (151/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00042-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39365 length: 39365 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6908 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6854 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6890 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6908
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6890
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6854
15/08/16 12:51:46 INFO Executor: Finished task 152.0 in stage 9.0 (TID 971). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 167.0 in stage 9.0 (TID 986, localhost, ANY, 1694 bytes)
15/08/16 12:51:46 INFO Executor: Running task 167.0 in stage 9.0 (TID 986)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 152.0 in stage 9.0 (TID 971) in 555 ms on localhost (152/200)
15/08/16 12:51:46 INFO Executor: Finished task 151.0 in stage 9.0 (TID 970). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00097-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39742 length: 39742 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Starting task 168.0 in stage 9.0 (TID 987, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 168.0 in stage 9.0 (TID 987)
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO TaskSetManager: Finished task 151.0 in stage 9.0 (TID 970) in 563 ms on localhost (153/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00069-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39431 length: 39431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6913 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6849 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6913
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6849
15/08/16 12:51:46 INFO Executor: Finished task 153.0 in stage 9.0 (TID 972). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 154.0 in stage 9.0 (TID 973). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 169.0 in stage 9.0 (TID 988, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 169.0 in stage 9.0 (TID 988)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 170.0 in stage 9.0 (TID 989, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 170.0 in stage 9.0 (TID 989)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 153.0 in stage 9.0 (TID 972) in 562 ms on localhost (154/200)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 154.0 in stage 9.0 (TID 973) in 556 ms on localhost (155/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00178-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39429 length: 39429 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00082-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39597 length: 39597 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6845 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6899 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 6899
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 6845
15/08/16 12:51:46 INFO Executor: Finished task 155.0 in stage 9.0 (TID 974). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 171.0 in stage 9.0 (TID 990, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 171.0 in stage 9.0 (TID 990)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 155.0 in stage 9.0 (TID 974) in 560 ms on localhost (156/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00050-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39784 length: 39784 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6912 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6912
15/08/16 12:51:46 INFO Executor: Finished task 156.0 in stage 9.0 (TID 975). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 172.0 in stage 9.0 (TID 991, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 172.0 in stage 9.0 (TID 991)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 156.0 in stage 9.0 (TID 975) in 388 ms on localhost (157/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00103-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39468 length: 39468 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO Executor: Finished task 157.0 in stage 9.0 (TID 976). 2125 bytes result sent to driver
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO TaskSetManager: Starting task 173.0 in stage 9.0 (TID 992, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 173.0 in stage 9.0 (TID 992)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 157.0 in stage 9.0 (TID 976) in 371 ms on localhost (158/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00066-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39466 length: 39466 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6839 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6852 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6852
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6839
15/08/16 12:51:46 INFO Executor: Finished task 159.0 in stage 9.0 (TID 978). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 174.0 in stage 9.0 (TID 993, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 174.0 in stage 9.0 (TID 993)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 159.0 in stage 9.0 (TID 978) in 388 ms on localhost (159/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00040-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39692 length: 39692 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 158.0 in stage 9.0 (TID 977). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6898 records.
15/08/16 12:51:46 INFO Executor: Finished task 160.0 in stage 9.0 (TID 979). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 175.0 in stage 9.0 (TID 994, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 175.0 in stage 9.0 (TID 994)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 176.0 in stage 9.0 (TID 995, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 176.0 in stage 9.0 (TID 995)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 158.0 in stage 9.0 (TID 977) in 401 ms on localhost (160/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00029-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39669 length: 39669 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 160.0 in stage 9.0 (TID 979) in 399 ms on localhost (161/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00099-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39713 length: 39713 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6899 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6898 records.
15/08/16 12:51:46 INFO Executor: Finished task 161.0 in stage 9.0 (TID 980). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6898
15/08/16 12:51:46 INFO TaskSetManager: Starting task 177.0 in stage 9.0 (TID 996, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 177.0 in stage 9.0 (TID 996)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 161.0 in stage 9.0 (TID 980) in 398 ms on localhost (162/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00154-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39445 length: 39445 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6899
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6859 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6898
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6859
15/08/16 12:51:46 INFO Executor: Finished task 162.0 in stage 9.0 (TID 981). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 178.0 in stage 9.0 (TID 997, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 178.0 in stage 9.0 (TID 997)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 162.0 in stage 9.0 (TID 981) in 402 ms on localhost (163/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00180-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39568 length: 39568 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 163.0 in stage 9.0 (TID 982). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6877 records.
15/08/16 12:51:46 INFO TaskSetManager: Starting task 179.0 in stage 9.0 (TID 998, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 179.0 in stage 9.0 (TID 998)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 163.0 in stage 9.0 (TID 982) in 395 ms on localhost (164/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00030-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39515 length: 39515 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6884 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6877
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 6884
15/08/16 12:51:46 INFO Executor: Finished task 165.0 in stage 9.0 (TID 984). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 180.0 in stage 9.0 (TID 999, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Finished task 164.0 in stage 9.0 (TID 983). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Running task 180.0 in stage 9.0 (TID 999)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 181.0 in stage 9.0 (TID 1000, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 181.0 in stage 9.0 (TID 1000)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 165.0 in stage 9.0 (TID 984) in 384 ms on localhost (165/200)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 164.0 in stage 9.0 (TID 983) in 386 ms on localhost (166/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00096-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39497 length: 39497 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00186-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39742 length: 39742 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO Executor: Finished task 166.0 in stage 9.0 (TID 985). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 182.0 in stage 9.0 (TID 1001, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Running task 182.0 in stage 9.0 (TID 1001)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 166.0 in stage 9.0 (TID 985) in 387 ms on localhost (167/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00135-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39719 length: 39719 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6907 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6887 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6884 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6884
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6907
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6887
15/08/16 12:51:46 INFO Executor: Finished task 168.0 in stage 9.0 (TID 987). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 167.0 in stage 9.0 (TID 986). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 183.0 in stage 9.0 (TID 1002, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 183.0 in stage 9.0 (TID 1002)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 184.0 in stage 9.0 (TID 1003, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 184.0 in stage 9.0 (TID 1003)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 168.0 in stage 9.0 (TID 987) in 373 ms on localhost (168/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00124-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39953 length: 39953 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00017-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39436 length: 39436 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO TaskSetManager: Finished task 167.0 in stage 9.0 (TID 986) in 380 ms on localhost (169/200)
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6874 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6928 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6928
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 6874
15/08/16 12:51:46 INFO Executor: Finished task 170.0 in stage 9.0 (TID 989). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 185.0 in stage 9.0 (TID 1004, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 185.0 in stage 9.0 (TID 1004)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 170.0 in stage 9.0 (TID 989) in 386 ms on localhost (170/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00057-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39442 length: 39442 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 169.0 in stage 9.0 (TID 988). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 186.0 in stage 9.0 (TID 1005, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 186.0 in stage 9.0 (TID 1005)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 169.0 in stage 9.0 (TID 988) in 395 ms on localhost (171/200)
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6849 records.
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00134-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39671 length: 39671 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6896 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6849
15/08/16 12:51:46 INFO Executor: Finished task 171.0 in stage 9.0 (TID 990). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO TaskSetManager: Starting task 187.0 in stage 9.0 (TID 1006, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6896
15/08/16 12:51:46 INFO Executor: Running task 187.0 in stage 9.0 (TID 1006)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 171.0 in stage 9.0 (TID 990) in 380 ms on localhost (172/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00024-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39625 length: 39625 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6882 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6882
15/08/16 12:51:46 INFO Executor: Finished task 172.0 in stage 9.0 (TID 991). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 188.0 in stage 9.0 (TID 1007, localhost, ANY, 1694 bytes)
15/08/16 12:51:46 INFO Executor: Running task 188.0 in stage 9.0 (TID 1007)
15/08/16 12:51:46 INFO Executor: Finished task 173.0 in stage 9.0 (TID 992). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 189.0 in stage 9.0 (TID 1008, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 189.0 in stage 9.0 (TID 1008)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 172.0 in stage 9.0 (TID 991) in 379 ms on localhost (173/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00165-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39881 length: 39881 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 173.0 in stage 9.0 (TID 992) in 369 ms on localhost (174/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00141-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39397 length: 39397 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6847 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6925 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6847
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6925
15/08/16 12:51:46 INFO Executor: Finished task 174.0 in stage 9.0 (TID 993). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 190.0 in stage 9.0 (TID 1009, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 190.0 in stage 9.0 (TID 1009)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 174.0 in stage 9.0 (TID 993) in 371 ms on localhost (175/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00026-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39374 length: 39374 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 175.0 in stage 9.0 (TID 994). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 191.0 in stage 9.0 (TID 1010, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 191.0 in stage 9.0 (TID 1010)
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6819 records.
15/08/16 12:51:46 INFO TaskSetManager: Finished task 175.0 in stage 9.0 (TID 994) in 372 ms on localhost (176/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00044-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39554 length: 39554 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 177.0 in stage 9.0 (TID 996). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6863 records.
15/08/16 12:51:46 INFO TaskSetManager: Starting task 192.0 in stage 9.0 (TID 1011, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 192.0 in stage 9.0 (TID 1011)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00037-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39565 length: 39565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 177.0 in stage 9.0 (TID 996) in 371 ms on localhost (177/200)
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6819
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6863
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6877 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6877
15/08/16 12:51:46 INFO Executor: Finished task 176.0 in stage 9.0 (TID 995). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 193.0 in stage 9.0 (TID 1012, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 176.0 in stage 9.0 (TID 995) in 396 ms on localhost (178/200)
15/08/16 12:51:46 INFO Executor: Running task 193.0 in stage 9.0 (TID 1012)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00081-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39494 length: 39494 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6870 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO Executor: Finished task 178.0 in stage 9.0 (TID 997). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 179.0 in stage 9.0 (TID 998). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6870
15/08/16 12:51:46 INFO TaskSetManager: Starting task 194.0 in stage 9.0 (TID 1013, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 194.0 in stage 9.0 (TID 1013)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 195.0 in stage 9.0 (TID 1014, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 195.0 in stage 9.0 (TID 1014)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 178.0 in stage 9.0 (TID 997) in 396 ms on localhost (179/200)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 179.0 in stage 9.0 (TID 998) in 385 ms on localhost (180/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00177-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39431 length: 39431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00125-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39527 length: 39527 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6852 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6857 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6852
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6857
15/08/16 12:51:46 INFO Executor: Finished task 181.0 in stage 9.0 (TID 1000). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Finished task 180.0 in stage 9.0 (TID 999). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 196.0 in stage 9.0 (TID 1015, localhost, ANY, 1691 bytes)
15/08/16 12:51:46 INFO Executor: Running task 196.0 in stage 9.0 (TID 1015)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 197.0 in stage 9.0 (TID 1016, localhost, ANY, 1692 bytes)
15/08/16 12:51:46 INFO Executor: Running task 197.0 in stage 9.0 (TID 1016)
15/08/16 12:51:46 INFO Executor: Finished task 182.0 in stage 9.0 (TID 1001). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Finished task 181.0 in stage 9.0 (TID 1000) in 388 ms on localhost (181/200)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 198.0 in stage 9.0 (TID 1017, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Running task 198.0 in stage 9.0 (TID 1017)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 180.0 in stage 9.0 (TID 999) in 390 ms on localhost (182/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00089-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39306 length: 39306 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 182.0 in stage 9.0 (TID 1001) in 387 ms on localhost (183/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00182-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39370 length: 39370 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00152-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39518 length: 39518 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6839 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6863 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6841 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6841
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6863
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 6839
15/08/16 12:51:46 INFO Executor: Finished task 183.0 in stage 9.0 (TID 1002). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 199.0 in stage 9.0 (TID 1018, localhost, ANY, 1693 bytes)
15/08/16 12:51:46 INFO Executor: Finished task 184.0 in stage 9.0 (TID 1003). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO Executor: Running task 199.0 in stage 9.0 (TID 1018)
15/08/16 12:51:46 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1019, localhost, ANY, 1719 bytes)
15/08/16 12:51:46 INFO Executor: Running task 0.0 in stage 10.0 (TID 1019)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 183.0 in stage 9.0 (TID 1002) in 392 ms on localhost (184/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_tmp2_par/part-r-00046-43af336f-a1a4-43b6-a0a3-8008ead66fa2.gz.parquet start: 0 end: 39336 length: 39336 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 count_suppkey;
  optional int32 max_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"count_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"max_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Finished task 184.0 in stage 9.0 (TID 1003) in 393 ms on localhost (185/200)
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 0 end: 7121831 length: 7121831 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6846 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189063 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 6846
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 189063
15/08/16 12:51:46 INFO Executor: Finished task 186.0 in stage 9.0 (TID 1005). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 1020, localhost, ANY, 1720 bytes)
15/08/16 12:51:46 INFO Executor: Running task 1.0 in stage 10.0 (TID 1020)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 186.0 in stage 9.0 (TID 1005) in 363 ms on localhost (186/200)
15/08/16 12:51:46 INFO Executor: Finished task 185.0 in stage 9.0 (TID 1004). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 0 end: 7057748 length: 7057748 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 1021, localhost, ANY, 1722 bytes)
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Running task 2.0 in stage 10.0 (TID 1021)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 185.0 in stage 9.0 (TID 1004) in 376 ms on localhost (187/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 0 end: 7054827 length: 7054827 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187233 records.
15/08/16 12:51:46 INFO Executor: Finished task 187.0 in stage 9.0 (TID 1006). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187078 records.
15/08/16 12:51:46 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 1022, localhost, ANY, 1721 bytes)
15/08/16 12:51:46 INFO Executor: Running task 3.0 in stage 10.0 (TID 1022)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 187.0 in stage 9.0 (TID 1006) in 361 ms on localhost (188/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 0 end: 7055186 length: 7055186 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187238 records.
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 187078
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 187233
15/08/16 12:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:46 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 187238
15/08/16 12:51:46 INFO Executor: Finished task 189.0 in stage 9.0 (TID 1008). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 1023, localhost, ANY, 1721 bytes)
15/08/16 12:51:46 INFO Executor: Running task 4.0 in stage 10.0 (TID 1023)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 189.0 in stage 9.0 (TID 1008) in 362 ms on localhost (189/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 0 end: 7053443 length: 7053443 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO Executor: Finished task 188.0 in stage 9.0 (TID 1007). 2125 bytes result sent to driver
15/08/16 12:51:46 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 1024, localhost, ANY, 1722 bytes)
15/08/16 12:51:46 INFO Executor: Running task 5.0 in stage 10.0 (TID 1024)
15/08/16 12:51:46 INFO TaskSetManager: Finished task 188.0 in stage 9.0 (TID 1007) in 370 ms on localhost (190/200)
15/08/16 12:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 0 end: 7054838 length: 7054838 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187255 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187249 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 187255
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 187249
15/08/16 12:51:47 INFO Executor: Finished task 190.0 in stage 9.0 (TID 1009). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 1025, localhost, ANY, 1721 bytes)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 190.0 in stage 9.0 (TID 1009) in 360 ms on localhost (191/200)
15/08/16 12:51:47 INFO Executor: Running task 6.0 in stage 10.0 (TID 1025)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 0 end: 7072309 length: 7072309 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO Executor: Finished task 192.0 in stage 9.0 (TID 1011). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 1026, localhost, ANY, 1719 bytes)
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO Executor: Running task 7.0 in stage 10.0 (TID 1026)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 192.0 in stage 9.0 (TID 1011) in 347 ms on localhost (192/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 0 end: 7055717 length: 7055717 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional binary o_orderstatus (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187649 records.
15/08/16 12:51:47 INFO Executor: Finished task 191.0 in stage 9.0 (TID 1010). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 187235 records.
15/08/16 12:51:47 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 1027, localhost, ANY, 1792 bytes)
15/08/16 12:51:47 INFO Executor: Running task 0.0 in stage 11.0 (TID 1027)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 191.0 in stage 9.0 (TID 1010) in 363 ms on localhost (193/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 26485016 length: 26485016 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO Executor: Finished task 193.0 in stage 9.0 (TID 1012). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 1028, localhost, ANY, 1795 bytes)
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO Executor: Running task 1.0 in stage 11.0 (TID 1028)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 193.0 in stage 9.0 (TID 1012) in 343 ms on localhost (194/200)
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 187649
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 187235
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 26536257 length: 26536257 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 755812 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749050 records.
15/08/16 12:51:47 INFO Executor: Finished task 195.0 in stage 9.0 (TID 1014). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 1029, localhost, ANY, 1792 bytes)
15/08/16 12:51:47 INFO Executor: Running task 2.0 in stage 11.0 (TID 1029)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 195.0 in stage 9.0 (TID 1014) in 332 ms on localhost (195/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 26505368 length: 26505368 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749056 records.
15/08/16 12:51:47 INFO Executor: Finished task 194.0 in stage 9.0 (TID 1013). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 1030, localhost, ANY, 1792 bytes)
15/08/16 12:51:47 INFO Executor: Running task 3.0 in stage 11.0 (TID 1030)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 194.0 in stage 9.0 (TID 1013) in 350 ms on localhost (196/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 26243215 length: 26243215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 755812
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 749050
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749157 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 749056
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 749157
15/08/16 12:51:47 INFO Executor: Finished task 198.0 in stage 9.0 (TID 1017). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO Executor: Finished task 197.0 in stage 9.0 (TID 1016). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 1031, localhost, ANY, 1792 bytes)
15/08/16 12:51:47 INFO Executor: Running task 4.0 in stage 11.0 (TID 1031)
15/08/16 12:51:47 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 1032, localhost, ANY, 1794 bytes)
15/08/16 12:51:47 INFO Executor: Running task 5.0 in stage 11.0 (TID 1032)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 197.0 in stage 9.0 (TID 1016) in 334 ms on localhost (197/200)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 198.0 in stage 9.0 (TID 1017) in 332 ms on localhost (198/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 26234990 length: 26234990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 26210131 length: 26210131 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749096 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 748901 records.
15/08/16 12:51:47 INFO Executor: Finished task 196.0 in stage 9.0 (TID 1015). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 1033, localhost, ANY, 1794 bytes)
15/08/16 12:51:47 INFO Executor: Running task 6.0 in stage 11.0 (TID 1033)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 196.0 in stage 9.0 (TID 1015) in 351 ms on localhost (199/200)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 26235204 length: 26235204 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO Executor: Finished task 199.0 in stage 9.0 (TID 1018). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 1034, localhost, ANY, 1793 bytes)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 199.0 in stage 9.0 (TID 1018) in 316 ms on localhost (200/200)
15/08/16 12:51:47 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/16 12:51:47 INFO DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:423) finished in 6.961 s
15/08/16 12:51:47 INFO Executor: Running task 7.0 in stage 11.0 (TID 1034)
15/08/16 12:51:47 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:47 INFO DAGScheduler: running: Set(ShuffleMapStage 12, ShuffleMapStage 10, ShuffleMapStage 11)
15/08/16 12:51:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16, ShuffleMapStage 13, ShuffleMapStage 14)
15/08/16 12:51:47 INFO DAGScheduler: failed: Set()
15/08/16 12:51:47 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@14ab164d
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO StatsReportListener: task runtime:(count: 200, mean: 552.400000, stdev: 304.551703, max: 1491.000000, min: 305.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	305.0 ms	347.0 ms	361.0 ms	387.0 ms	424.0 ms	583.0 ms	890.0 ms	1.5 s	1.5 s
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 26576747 length: 26576747 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional int32 l_suppkey;
  optional binary l_receiptdate (UTF8);
  optional binary l_commitdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 89525.750000, stdev: 305.101209, max: 90305.000000, min: 88763.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	86.7 KB	87.0 KB	87.0 KB	87.2 KB	87.4 KB	87.6 KB	87.8 KB	88.0 KB	88.2 KB
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List(ShuffleMapStage 14)
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749107 records.
15/08/16 12:51:47 INFO StatsReportListener: task result size:(count: 200, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 751036 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 749096
15/08/16 12:51:47 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.420864, stdev: 5.536887, max: 99.435028, min: 45.617978)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 748901
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	46 %	90 %	93 %	97 %	98 %	98 %	99 %	99 %	99 %
15/08/16 12:51:47 INFO StatsReportListener: other time pct: (count: 200, mean: 3.579136, stdev: 5.536887, max: 54.382022, min: 0.564972)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	 7 %	10 %	54 %
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 13: List(ShuffleMapStage 12, ShuffleMapStage 11)
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 14: List(ShuffleMapStage 13, ShuffleMapStage 10)
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 751036
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 749107
15/08/16 12:51:47 INFO Executor: Finished task 2.0 in stage 10.0 (TID 1021). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 1035, localhost, ANY, 1671 bytes)
15/08/16 12:51:47 INFO Executor: Running task 0.0 in stage 12.0 (TID 1035)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 1021) in 442 ms on localhost (1/8)
15/08/16 12:51:47 INFO Executor: Finished task 3.0 in stage 10.0 (TID 1022). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 1022) in 437 ms on localhost (2/8)
15/08/16 12:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 2330 length: 2330 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/16 12:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/16 12:51:47 INFO Executor: Finished task 1.0 in stage 10.0 (TID 1020). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 1020) in 454 ms on localhost (3/8)
15/08/16 12:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
15/08/16 12:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/16 12:51:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 25
15/08/16 12:51:47 INFO Executor: Finished task 5.0 in stage 10.0 (TID 1024). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 1024) in 415 ms on localhost (4/8)
15/08/16 12:51:47 INFO Executor: Finished task 7.0 in stage 10.0 (TID 1026). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 1026) in 387 ms on localhost (5/8)
15/08/16 12:51:47 INFO Executor: Finished task 4.0 in stage 10.0 (TID 1023). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 1023) in 495 ms on localhost (6/8)
15/08/16 12:51:47 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1019). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO Executor: Finished task 6.0 in stage 10.0 (TID 1025). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1019) in 630 ms on localhost (7/8)
15/08/16 12:51:47 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 1025) in 446 ms on localhost (8/8)
15/08/16 12:51:47 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/16 12:51:47 INFO DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:423) finished in 7.229 s
15/08/16 12:51:47 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:47 INFO DAGScheduler: running: Set(ShuffleMapStage 12, ShuffleMapStage 11)
15/08/16 12:51:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16, ShuffleMapStage 13, ShuffleMapStage 14)
15/08/16 12:51:47 INFO DAGScheduler: failed: Set()
15/08/16 12:51:47 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@40ac4687
15/08/16 12:51:47 INFO StatsReportListener: task runtime:(count: 8, mean: 463.250000, stdev: 69.353713, max: 630.000000, min: 387.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	387.0 ms	387.0 ms	387.0 ms	437.0 ms	446.0 ms	495.0 ms	630.0 ms	630.0 ms	630.0 ms
15/08/16 12:51:47 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 572159.375000, stdev: 1859.073017, max: 576776.000000, min: 570406.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	557.0 KB	557.0 KB	557.0 KB	558.0 KB	558.2 KB	559.1 KB	563.3 KB	563.3 KB	563.3 KB
15/08/16 12:51:47 INFO StatsReportListener: task result size:(count: 8, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:47 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 97.828916, stdev: 0.557721, max: 98.398169, min: 96.412556)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	96 %	96 %	96 %	98 %	98 %	98 %	98 %	98 %	98 %
15/08/16 12:51:47 INFO StatsReportListener: other time pct: (count: 8, mean: 2.171084, stdev: 0.557721, max: 3.587444, min: 1.601831)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	 2 %	 2 %	 2 %	 2 %	 2 %	 2 %	 4 %	 4 %	 4 %
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List(ShuffleMapStage 14)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 13: List(ShuffleMapStage 12, ShuffleMapStage 11)
15/08/16 12:51:47 INFO Executor: Finished task 0.0 in stage 12.0 (TID 1035). 2125 bytes result sent to driver
15/08/16 12:51:47 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 1035) in 172 ms on localhost (1/1)
15/08/16 12:51:47 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 14: List(ShuffleMapStage 13)
15/08/16 12:51:47 INFO DAGScheduler: ShuffleMapStage 12 (processCmd at CliDriver.java:423) finished in 7.196 s
15/08/16 12:51:47 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:47 INFO DAGScheduler: running: Set(ShuffleMapStage 11)
15/08/16 12:51:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16, ShuffleMapStage 13, ShuffleMapStage 14)
15/08/16 12:51:47 INFO DAGScheduler: failed: Set()
15/08/16 12:51:47 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@20b6fd73
15/08/16 12:51:47 INFO StatsReportListener: task runtime:(count: 1, mean: 172.000000, stdev: 0.000000, max: 172.000000, min: 172.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	172.0 ms	172.0 ms	172.0 ms	172.0 ms	172.0 ms	172.0 ms	172.0 ms	172.0 ms	172.0 ms
15/08/16 12:51:47 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 12735.000000, stdev: 0.000000, max: 12735.000000, min: 12735.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	12.4 KB	12.4 KB	12.4 KB	12.4 KB	12.4 KB	12.4 KB	12.4 KB	12.4 KB	12.4 KB
15/08/16 12:51:47 INFO StatsReportListener: task result size:(count: 1, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:47 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.186047, stdev: 0.000000, max: 94.186047, min: 94.186047)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %
15/08/16 12:51:47 INFO StatsReportListener: other time pct: (count: 1, mean: 5.813953, stdev: 0.000000, max: 5.813953, min: 5.813953)
15/08/16 12:51:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:47 INFO StatsReportListener: 	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List(ShuffleMapStage 14)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 13: List(ShuffleMapStage 11)
15/08/16 12:51:47 INFO DAGScheduler: Missing parents for ShuffleMapStage 14: List(ShuffleMapStage 13)
15/08/16 12:51:48 INFO Executor: Finished task 3.0 in stage 11.0 (TID 1030). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 1030) in 1276 ms on localhost (1/8)
15/08/16 12:51:48 INFO Executor: Finished task 4.0 in stage 11.0 (TID 1031). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 1031) in 1268 ms on localhost (2/8)
15/08/16 12:51:48 INFO Executor: Finished task 0.0 in stage 11.0 (TID 1027). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 1027) in 1388 ms on localhost (3/8)
15/08/16 12:51:48 INFO Executor: Finished task 2.0 in stage 11.0 (TID 1029). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 1029) in 1384 ms on localhost (4/8)
15/08/16 12:51:48 INFO Executor: Finished task 6.0 in stage 11.0 (TID 1033). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 1033) in 1314 ms on localhost (5/8)
15/08/16 12:51:48 INFO Executor: Finished task 7.0 in stage 11.0 (TID 1034). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO Executor: Finished task 5.0 in stage 11.0 (TID 1032). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO Executor: Finished task 1.0 in stage 11.0 (TID 1028). 2125 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 1034) in 1323 ms on localhost (6/8)
15/08/16 12:51:48 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 1032) in 1346 ms on localhost (7/8)
15/08/16 12:51:48 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 1028) in 1426 ms on localhost (8/8)
15/08/16 12:51:48 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/16 12:51:48 INFO DAGScheduler: ShuffleMapStage 11 (processCmd at CliDriver.java:423) finished in 8.192 s
15/08/16 12:51:48 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:48 INFO DAGScheduler: running: Set()
15/08/16 12:51:48 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16, ShuffleMapStage 13, ShuffleMapStage 14)
15/08/16 12:51:48 INFO DAGScheduler: failed: Set()
15/08/16 12:51:48 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@f29575f
15/08/16 12:51:48 INFO StatsReportListener: task runtime:(count: 8, mean: 1340.625000, stdev: 52.337696, max: 1426.000000, min: 1268.000000)
15/08/16 12:51:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:48 INFO StatsReportListener: 	1.3 s	1.3 s	1.3 s	1.3 s	1.3 s	1.4 s	1.4 s	1.4 s	1.4 s
15/08/16 12:51:48 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 4362209.000000, stdev: 7586.601265, max: 4374943.000000, min: 4352947.000000)
15/08/16 12:51:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:48 INFO StatsReportListener: 	4.2 MB	4.2 MB	4.2 MB	4.2 MB	4.2 MB	4.2 MB	4.2 MB	4.2 MB	4.2 MB
15/08/16 12:51:48 INFO StatsReportListener: task result size:(count: 8, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/16 12:51:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:48 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/16 12:51:48 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 99.270043, stdev: 0.167853, max: 99.494220, min: 99.017385)
15/08/16 12:51:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:48 INFO StatsReportListener: 	99 %	99 %	99 %	99 %	99 %	99 %	99 %	99 %	99 %
15/08/16 12:51:48 INFO StatsReportListener: other time pct: (count: 8, mean: 0.729957, stdev: 0.167853, max: 0.982615, min: 0.505780)
15/08/16 12:51:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:48 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 1 %	 1 %	 1 %	 1 %	 1 %
15/08/16 12:51:48 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List(ShuffleMapStage 14)
15/08/16 12:51:48 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:48 INFO DAGScheduler: Missing parents for ShuffleMapStage 13: List()
15/08/16 12:51:48 INFO DAGScheduler: Missing parents for ShuffleMapStage 14: List(ShuffleMapStage 13)
15/08/16 12:51:48 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[59] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:51:48 INFO MemoryStore: ensureFreeSpace(12264) called with curMem=508659853, maxMem=3333968363
15/08/16 12:51:48 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 12.0 KB, free 2.6 GB)
15/08/16 12:51:48 INFO MemoryStore: ensureFreeSpace(6170) called with curMem=508672117, maxMem=3333968363
15/08/16 12:51:48 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.0 KB, free 2.6 GB)
15/08/16 12:51:48 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:36543 (size: 6.0 KB, free: 3.1 GB)
15/08/16 12:51:48 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:48 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[59] at processCmd at CliDriver.java:423)
15/08/16 12:51:48 INFO TaskSchedulerImpl: Adding task set 13.0 with 200 tasks
15/08/16 12:51:48 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 1036, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 1037, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 1038, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 1039, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 1040, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 1041, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 1042, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 1043, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 1044, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 1045, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 1046, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 1047, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 1048, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 1049, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 1050, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 1051, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO Executor: Running task 0.0 in stage 13.0 (TID 1036)
15/08/16 12:51:48 INFO Executor: Running task 2.0 in stage 13.0 (TID 1038)
15/08/16 12:51:48 INFO Executor: Running task 5.0 in stage 13.0 (TID 1041)
15/08/16 12:51:48 INFO Executor: Running task 3.0 in stage 13.0 (TID 1039)
15/08/16 12:51:48 INFO Executor: Running task 1.0 in stage 13.0 (TID 1037)
15/08/16 12:51:48 INFO Executor: Running task 6.0 in stage 13.0 (TID 1042)
15/08/16 12:51:48 INFO Executor: Running task 8.0 in stage 13.0 (TID 1044)
15/08/16 12:51:48 INFO Executor: Running task 9.0 in stage 13.0 (TID 1045)
15/08/16 12:51:48 INFO Executor: Running task 11.0 in stage 13.0 (TID 1047)
15/08/16 12:51:48 INFO Executor: Running task 7.0 in stage 13.0 (TID 1043)
15/08/16 12:51:48 INFO Executor: Running task 4.0 in stage 13.0 (TID 1040)
15/08/16 12:51:48 INFO Executor: Running task 10.0 in stage 13.0 (TID 1046)
15/08/16 12:51:48 INFO Executor: Running task 12.0 in stage 13.0 (TID 1048)
15/08/16 12:51:48 INFO Executor: Running task 13.0 in stage 13.0 (TID 1049)
15/08/16 12:51:48 INFO Executor: Running task 15.0 in stage 13.0 (TID 1051)
15/08/16 12:51:48 INFO Executor: Running task 14.0 in stage 13.0 (TID 1050)
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO Executor: Finished task 6.0 in stage 13.0 (TID 1042). 1219 bytes result sent to driver
15/08/16 12:51:48 INFO Executor: Finished task 4.0 in stage 13.0 (TID 1040). 1219 bytes result sent to driver
15/08/16 12:51:48 INFO Executor: Finished task 9.0 in stage 13.0 (TID 1045). 1219 bytes result sent to driver
15/08/16 12:51:48 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 1052, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO Executor: Running task 16.0 in stage 13.0 (TID 1052)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 1053, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO Executor: Running task 17.0 in stage 13.0 (TID 1053)
15/08/16 12:51:48 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 1054, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:48 INFO Executor: Running task 18.0 in stage 13.0 (TID 1054)
15/08/16 12:51:48 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 1040) in 119 ms on localhost (1/200)
15/08/16 12:51:48 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 1045) in 118 ms on localhost (2/200)
15/08/16 12:51:48 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 1042) in 120 ms on localhost (3/200)
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 15.0 in stage 13.0 (TID 1051). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 1055, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 19.0 in stage 13.0 (TID 1055)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 1051) in 556 ms on localhost (4/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 5.0 in stage 13.0 (TID 1041). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 1.0 in stage 13.0 (TID 1037). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 1056, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 20.0 in stage 13.0 (TID 1056)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 1057, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 21.0 in stage 13.0 (TID 1057)
15/08/16 12:51:49 INFO Executor: Finished task 13.0 in stage 13.0 (TID 1049). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 1058, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 22.0 in stage 13.0 (TID 1058)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 1041) in 570 ms on localhost (5/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 1037) in 571 ms on localhost (6/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 1049) in 570 ms on localhost (7/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 39 ms
15/08/16 12:51:49 INFO Executor: Finished task 10.0 in stage 13.0 (TID 1046). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 1059, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 23.0 in stage 13.0 (TID 1059)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 1046) in 623 ms on localhost (8/200)
15/08/16 12:51:49 INFO Executor: Finished task 11.0 in stage 13.0 (TID 1047). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 1060, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 24.0 in stage 13.0 (TID 1060)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 1047) in 627 ms on localhost (9/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 2.0 in stage 13.0 (TID 1038). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 3.0 in stage 13.0 (TID 1039). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO Executor: Finished task 22.0 in stage 13.0 (TID 1058). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 1061, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 25.0 in stage 13.0 (TID 1061)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 1062, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 26.0 in stage 13.0 (TID 1062)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 1063, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 27.0 in stage 13.0 (TID 1063)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 1038) in 633 ms on localhost (10/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 1058) in 63 ms on localhost (11/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 0.0 in stage 13.0 (TID 1036). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 1064, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 28.0 in stage 13.0 (TID 1064)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 1039) in 636 ms on localhost (12/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 1036) in 642 ms on localhost (13/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 12.0 in stage 13.0 (TID 1048). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 1065, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 29.0 in stage 13.0 (TID 1065)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 1048) in 644 ms on localhost (14/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 27.0 in stage 13.0 (TID 1063). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 7.0 in stage 13.0 (TID 1043). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 1066, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 30.0 in stage 13.0 (TID 1066)
15/08/16 12:51:49 INFO Executor: Finished task 16.0 in stage 13.0 (TID 1052). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 1067, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 31.0 in stage 13.0 (TID 1067)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 1068, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 32.0 in stage 13.0 (TID 1068)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 1063) in 26 ms on localhost (15/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 1052) in 541 ms on localhost (16/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 1043) in 659 ms on localhost (17/200)
15/08/16 12:51:49 INFO Executor: Finished task 8.0 in stage 13.0 (TID 1044). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 1069, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 33.0 in stage 13.0 (TID 1069)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 1044) in 661 ms on localhost (18/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 18.0 in stage 13.0 (TID 1054). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 17.0 in stage 13.0 (TID 1053). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 1070, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 34.0 in stage 13.0 (TID 1070)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 1071, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 35.0 in stage 13.0 (TID 1071)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 1054) in 553 ms on localhost (19/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 1053) in 554 ms on localhost (20/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 14.0 in stage 13.0 (TID 1050). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 1072, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 36.0 in stage 13.0 (TID 1072)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 1050) in 677 ms on localhost (21/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 20.0 in stage 13.0 (TID 1056). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 1073, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 37.0 in stage 13.0 (TID 1073)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 1056) in 142 ms on localhost (22/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 21.0 in stage 13.0 (TID 1057). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 1074, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 38.0 in stage 13.0 (TID 1074)
15/08/16 12:51:49 INFO Executor: Finished task 19.0 in stage 13.0 (TID 1055). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 1075, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 39.0 in stage 13.0 (TID 1075)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 1057) in 173 ms on localhost (23/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 1055) in 188 ms on localhost (24/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 25.0 in stage 13.0 (TID 1061). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 1076, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 40.0 in stage 13.0 (TID 1076)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 1061) in 198 ms on localhost (25/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 23.0 in stage 13.0 (TID 1059). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 1077, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 41.0 in stage 13.0 (TID 1077)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 1059) in 410 ms on localhost (26/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 26.0 in stage 13.0 (TID 1062). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 1078, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 42.0 in stage 13.0 (TID 1078)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 1062) in 428 ms on localhost (27/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 24.0 in stage 13.0 (TID 1060). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 1079, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 43.0 in stage 13.0 (TID 1079)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 1060) in 438 ms on localhost (28/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 30.0 in stage 13.0 (TID 1066). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 1080, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 44.0 in stage 13.0 (TID 1080)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 1066) in 449 ms on localhost (29/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 33.0 in stage 13.0 (TID 1069). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 1081, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 45.0 in stage 13.0 (TID 1081)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 1069) in 452 ms on localhost (30/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 34.0 in stage 13.0 (TID 1070). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 1082, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 46.0 in stage 13.0 (TID 1082)
15/08/16 12:51:49 INFO Executor: Finished task 31.0 in stage 13.0 (TID 1067). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 1083, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 47.0 in stage 13.0 (TID 1083)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 1070) in 448 ms on localhost (31/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 1067) in 462 ms on localhost (32/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 28.0 in stage 13.0 (TID 1064). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 1084, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Running task 48.0 in stage 13.0 (TID 1084)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 1064) in 484 ms on localhost (33/200)
15/08/16 12:51:49 INFO Executor: Finished task 29.0 in stage 13.0 (TID 1065). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 1085, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO Executor: Running task 49.0 in stage 13.0 (TID 1085)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 1065) in 479 ms on localhost (34/200)
15/08/16 12:51:49 INFO Executor: Finished task 35.0 in stage 13.0 (TID 1071). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 50.0 in stage 13.0 (TID 1086, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Running task 50.0 in stage 13.0 (TID 1086)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 1071) in 457 ms on localhost (35/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 32.0 in stage 13.0 (TID 1068). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 51.0 in stage 13.0 (TID 1087, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 51.0 in stage 13.0 (TID 1087)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 1068) in 476 ms on localhost (36/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 37.0 in stage 13.0 (TID 1073). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 39.0 in stage 13.0 (TID 1075). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 52.0 in stage 13.0 (TID 1088, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 52.0 in stage 13.0 (TID 1088)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 53.0 in stage 13.0 (TID 1089, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO Executor: Finished task 47.0 in stage 13.0 (TID 1083). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 54.0 in stage 13.0 (TID 1090, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 53.0 in stage 13.0 (TID 1089)
15/08/16 12:51:49 INFO Executor: Running task 54.0 in stage 13.0 (TID 1090)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 1073) in 433 ms on localhost (37/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 1075) in 400 ms on localhost (38/200)
15/08/16 12:51:49 INFO Executor: Finished task 36.0 in stage 13.0 (TID 1072). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 1083) in 26 ms on localhost (39/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 55.0 in stage 13.0 (TID 1091, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Running task 55.0 in stage 13.0 (TID 1091)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 1072) in 466 ms on localhost (40/200)
15/08/16 12:51:49 INFO Executor: Finished task 38.0 in stage 13.0 (TID 1074). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Starting task 56.0 in stage 13.0 (TID 1092, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 56.0 in stage 13.0 (TID 1092)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 1074) in 408 ms on localhost (41/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 54.0 in stage 13.0 (TID 1090). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 40.0 in stage 13.0 (TID 1076). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 57.0 in stage 13.0 (TID 1093, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 57.0 in stage 13.0 (TID 1093)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 58.0 in stage 13.0 (TID 1094, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 58.0 in stage 13.0 (TID 1094)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 54.0 in stage 13.0 (TID 1090) in 41 ms on localhost (42/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 1076) in 355 ms on localhost (43/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 58.0 in stage 13.0 (TID 1094). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 59.0 in stage 13.0 (TID 1095, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 59.0 in stage 13.0 (TID 1095)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 58.0 in stage 13.0 (TID 1094) in 56 ms on localhost (44/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 41.0 in stage 13.0 (TID 1077). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 60.0 in stage 13.0 (TID 1096, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 60.0 in stage 13.0 (TID 1096)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 1077) in 261 ms on localhost (45/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 60.0 in stage 13.0 (TID 1096). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO Executor: Finished task 42.0 in stage 13.0 (TID 1078). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 61.0 in stage 13.0 (TID 1097, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 61.0 in stage 13.0 (TID 1097)
15/08/16 12:51:49 INFO TaskSetManager: Starting task 62.0 in stage 13.0 (TID 1098, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 62.0 in stage 13.0 (TID 1098)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 60.0 in stage 13.0 (TID 1096) in 43 ms on localhost (46/200)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 1078) in 277 ms on localhost (47/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 61.0 in stage 13.0 (TID 1097). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 63.0 in stage 13.0 (TID 1099, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 63.0 in stage 13.0 (TID 1099)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 61.0 in stage 13.0 (TID 1097) in 29 ms on localhost (48/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO Executor: Finished task 43.0 in stage 13.0 (TID 1079). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 64.0 in stage 13.0 (TID 1100, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 64.0 in stage 13.0 (TID 1100)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 1079) in 305 ms on localhost (49/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO Executor: Finished task 44.0 in stage 13.0 (TID 1080). 1219 bytes result sent to driver
15/08/16 12:51:49 INFO TaskSetManager: Starting task 65.0 in stage 13.0 (TID 1101, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:49 INFO Executor: Running task 65.0 in stage 13.0 (TID 1101)
15/08/16 12:51:49 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 1080) in 341 ms on localhost (50/200)
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 46.0 in stage 13.0 (TID 1082). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 66.0 in stage 13.0 (TID 1102, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 66.0 in stage 13.0 (TID 1102)
15/08/16 12:51:50 INFO Executor: Finished task 48.0 in stage 13.0 (TID 1084). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 67.0 in stage 13.0 (TID 1103, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 67.0 in stage 13.0 (TID 1103)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 1084) in 395 ms on localhost (51/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 1082) in 399 ms on localhost (52/200)
15/08/16 12:51:50 INFO Executor: Finished task 45.0 in stage 13.0 (TID 1081). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO TaskSetManager: Starting task 68.0 in stage 13.0 (TID 1104, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 68.0 in stage 13.0 (TID 1104)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 1081) in 408 ms on localhost (53/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 50.0 in stage 13.0 (TID 1086). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 69.0 in stage 13.0 (TID 1105, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 69.0 in stage 13.0 (TID 1105)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 50.0 in stage 13.0 (TID 1086) in 408 ms on localhost (54/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO Executor: Finished task 49.0 in stage 13.0 (TID 1085). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 70.0 in stage 13.0 (TID 1106, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 70.0 in stage 13.0 (TID 1106)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 1085) in 418 ms on localhost (55/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 52.0 in stage 13.0 (TID 1088). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO Executor: Finished task 53.0 in stage 13.0 (TID 1089). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 71.0 in stage 13.0 (TID 1107, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 71.0 in stage 13.0 (TID 1107)
15/08/16 12:51:50 INFO TaskSetManager: Starting task 72.0 in stage 13.0 (TID 1108, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 72.0 in stage 13.0 (TID 1108)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 52.0 in stage 13.0 (TID 1088) in 422 ms on localhost (56/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 53.0 in stage 13.0 (TID 1089) in 422 ms on localhost (57/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 56.0 in stage 13.0 (TID 1092). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 73.0 in stage 13.0 (TID 1109, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 73.0 in stage 13.0 (TID 1109)
15/08/16 12:51:50 INFO Executor: Finished task 55.0 in stage 13.0 (TID 1091). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 74.0 in stage 13.0 (TID 1110, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 56.0 in stage 13.0 (TID 1092) in 420 ms on localhost (58/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 55.0 in stage 13.0 (TID 1091) in 423 ms on localhost (59/200)
15/08/16 12:51:50 INFO Executor: Running task 74.0 in stage 13.0 (TID 1110)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 51.0 in stage 13.0 (TID 1087). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 75.0 in stage 13.0 (TID 1111, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Running task 75.0 in stage 13.0 (TID 1111)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 51.0 in stage 13.0 (TID 1087) in 439 ms on localhost (60/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 57.0 in stage 13.0 (TID 1093). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 76.0 in stage 13.0 (TID 1112, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 76.0 in stage 13.0 (TID 1112)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 57.0 in stage 13.0 (TID 1093) in 416 ms on localhost (61/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 59.0 in stage 13.0 (TID 1095). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 77.0 in stage 13.0 (TID 1113, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 77.0 in stage 13.0 (TID 1113)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 59.0 in stage 13.0 (TID 1095) in 396 ms on localhost (62/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 62.0 in stage 13.0 (TID 1098). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 78.0 in stage 13.0 (TID 1114, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 78.0 in stage 13.0 (TID 1114)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 62.0 in stage 13.0 (TID 1098) in 364 ms on localhost (63/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 63.0 in stage 13.0 (TID 1099). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 79.0 in stage 13.0 (TID 1115, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 79.0 in stage 13.0 (TID 1115)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 63.0 in stage 13.0 (TID 1099) in 388 ms on localhost (64/200)
15/08/16 12:51:50 INFO Executor: Finished task 64.0 in stage 13.0 (TID 1100). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 80.0 in stage 13.0 (TID 1116, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 80.0 in stage 13.0 (TID 1116)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 64.0 in stage 13.0 (TID 1100) in 384 ms on localhost (65/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 79.0 in stage 13.0 (TID 1115). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 81.0 in stage 13.0 (TID 1117, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 81.0 in stage 13.0 (TID 1117)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 79.0 in stage 13.0 (TID 1115) in 29 ms on localhost (66/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 81.0 in stage 13.0 (TID 1117). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO Executor: Finished task 65.0 in stage 13.0 (TID 1101). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 82.0 in stage 13.0 (TID 1118, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 82.0 in stage 13.0 (TID 1118)
15/08/16 12:51:50 INFO TaskSetManager: Starting task 83.0 in stage 13.0 (TID 1119, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 83.0 in stage 13.0 (TID 1119)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 81.0 in stage 13.0 (TID 1117) in 26 ms on localhost (67/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 65.0 in stage 13.0 (TID 1101) in 356 ms on localhost (68/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 82.0 in stage 13.0 (TID 1118). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 84.0 in stage 13.0 (TID 1120, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 84.0 in stage 13.0 (TID 1120)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 82.0 in stage 13.0 (TID 1118) in 26 ms on localhost (69/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 67.0 in stage 13.0 (TID 1103). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 85.0 in stage 13.0 (TID 1121, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 85.0 in stage 13.0 (TID 1121)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 67.0 in stage 13.0 (TID 1103) in 364 ms on localhost (70/200)
15/08/16 12:51:50 INFO Executor: Finished task 66.0 in stage 13.0 (TID 1102). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 86.0 in stage 13.0 (TID 1122, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 86.0 in stage 13.0 (TID 1122)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 66.0 in stage 13.0 (TID 1102) in 369 ms on localhost (71/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 68.0 in stage 13.0 (TID 1104). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 87.0 in stage 13.0 (TID 1123, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 87.0 in stage 13.0 (TID 1123)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 68.0 in stage 13.0 (TID 1104) in 375 ms on localhost (72/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 70.0 in stage 13.0 (TID 1106). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 88.0 in stage 13.0 (TID 1124, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 88.0 in stage 13.0 (TID 1124)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 70.0 in stage 13.0 (TID 1106) in 376 ms on localhost (73/200)
15/08/16 12:51:50 INFO Executor: Finished task 69.0 in stage 13.0 (TID 1105). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 89.0 in stage 13.0 (TID 1125, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 89.0 in stage 13.0 (TID 1125)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 69.0 in stage 13.0 (TID 1105) in 384 ms on localhost (74/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 72.0 in stage 13.0 (TID 1108). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 90.0 in stage 13.0 (TID 1126, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 90.0 in stage 13.0 (TID 1126)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 72.0 in stage 13.0 (TID 1108) in 390 ms on localhost (75/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 71.0 in stage 13.0 (TID 1107). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 91.0 in stage 13.0 (TID 1127, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 91.0 in stage 13.0 (TID 1127)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 71.0 in stage 13.0 (TID 1107) in 407 ms on localhost (76/200)
15/08/16 12:51:50 INFO Executor: Finished task 75.0 in stage 13.0 (TID 1111). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO Executor: Finished task 73.0 in stage 13.0 (TID 1109). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 92.0 in stage 13.0 (TID 1128, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 92.0 in stage 13.0 (TID 1128)
15/08/16 12:51:50 INFO TaskSetManager: Starting task 93.0 in stage 13.0 (TID 1129, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 93.0 in stage 13.0 (TID 1129)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 75.0 in stage 13.0 (TID 1111) in 398 ms on localhost (77/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Finished task 73.0 in stage 13.0 (TID 1109) in 403 ms on localhost (78/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 74.0 in stage 13.0 (TID 1110). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 94.0 in stage 13.0 (TID 1130, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 94.0 in stage 13.0 (TID 1130)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 74.0 in stage 13.0 (TID 1110) in 415 ms on localhost (79/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 90.0 in stage 13.0 (TID 1126). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 95.0 in stage 13.0 (TID 1131, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO Executor: Running task 95.0 in stage 13.0 (TID 1131)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO Executor: Finished task 76.0 in stage 13.0 (TID 1112). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 96.0 in stage 13.0 (TID 1132, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 90.0 in stage 13.0 (TID 1126) in 41 ms on localhost (80/200)
15/08/16 12:51:50 INFO Executor: Running task 96.0 in stage 13.0 (TID 1132)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 76.0 in stage 13.0 (TID 1112) in 394 ms on localhost (81/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 77.0 in stage 13.0 (TID 1113). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 97.0 in stage 13.0 (TID 1133, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 97.0 in stage 13.0 (TID 1133)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 77.0 in stage 13.0 (TID 1113) in 370 ms on localhost (82/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 95.0 in stage 13.0 (TID 1131). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 98.0 in stage 13.0 (TID 1134, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 98.0 in stage 13.0 (TID 1134)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 95.0 in stage 13.0 (TID 1131) in 136 ms on localhost (83/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 78.0 in stage 13.0 (TID 1114). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 99.0 in stage 13.0 (TID 1135, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 99.0 in stage 13.0 (TID 1135)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 78.0 in stage 13.0 (TID 1114) in 460 ms on localhost (84/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 80.0 in stage 13.0 (TID 1116). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 100.0 in stage 13.0 (TID 1136, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 100.0 in stage 13.0 (TID 1136)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 80.0 in stage 13.0 (TID 1116) in 441 ms on localhost (85/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 83.0 in stage 13.0 (TID 1119). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 101.0 in stage 13.0 (TID 1137, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 101.0 in stage 13.0 (TID 1137)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 83.0 in stage 13.0 (TID 1119) in 457 ms on localhost (86/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 84.0 in stage 13.0 (TID 1120). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 102.0 in stage 13.0 (TID 1138, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 102.0 in stage 13.0 (TID 1138)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 84.0 in stage 13.0 (TID 1120) in 460 ms on localhost (87/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 85.0 in stage 13.0 (TID 1121). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 103.0 in stage 13.0 (TID 1139, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 103.0 in stage 13.0 (TID 1139)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 85.0 in stage 13.0 (TID 1121) in 452 ms on localhost (88/200)
15/08/16 12:51:50 INFO Executor: Finished task 86.0 in stage 13.0 (TID 1122). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO Executor: Finished task 87.0 in stage 13.0 (TID 1123). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Starting task 104.0 in stage 13.0 (TID 1140, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 104.0 in stage 13.0 (TID 1140)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO TaskSetManager: Starting task 105.0 in stage 13.0 (TID 1141, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 105.0 in stage 13.0 (TID 1141)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 86.0 in stage 13.0 (TID 1122) in 452 ms on localhost (89/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 87.0 in stage 13.0 (TID 1123) in 441 ms on localhost (90/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 103.0 in stage 13.0 (TID 1139). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO Executor: Finished task 104.0 in stage 13.0 (TID 1140). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 106.0 in stage 13.0 (TID 1142, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 106.0 in stage 13.0 (TID 1142)
15/08/16 12:51:50 INFO TaskSetManager: Starting task 107.0 in stage 13.0 (TID 1143, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 107.0 in stage 13.0 (TID 1143)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 103.0 in stage 13.0 (TID 1139) in 31 ms on localhost (91/200)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 104.0 in stage 13.0 (TID 1140) in 27 ms on localhost (92/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 89.0 in stage 13.0 (TID 1125). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 108.0 in stage 13.0 (TID 1144, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 108.0 in stage 13.0 (TID 1144)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 89.0 in stage 13.0 (TID 1125) in 456 ms on localhost (93/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 88.0 in stage 13.0 (TID 1124). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 109.0 in stage 13.0 (TID 1145, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 109.0 in stage 13.0 (TID 1145)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 88.0 in stage 13.0 (TID 1124) in 466 ms on localhost (94/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 92.0 in stage 13.0 (TID 1128). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 110.0 in stage 13.0 (TID 1146, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 110.0 in stage 13.0 (TID 1146)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 92.0 in stage 13.0 (TID 1128) in 455 ms on localhost (95/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 93.0 in stage 13.0 (TID 1129). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 111.0 in stage 13.0 (TID 1147, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 111.0 in stage 13.0 (TID 1147)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 93.0 in stage 13.0 (TID 1129) in 466 ms on localhost (96/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:50 INFO Executor: Finished task 91.0 in stage 13.0 (TID 1127). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 112.0 in stage 13.0 (TID 1148, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Running task 112.0 in stage 13.0 (TID 1148)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 91.0 in stage 13.0 (TID 1127) in 477 ms on localhost (97/200)
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:50 INFO Executor: Finished task 96.0 in stage 13.0 (TID 1132). 1219 bytes result sent to driver
15/08/16 12:51:50 INFO TaskSetManager: Starting task 113.0 in stage 13.0 (TID 1149, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:50 INFO Executor: Running task 113.0 in stage 13.0 (TID 1149)
15/08/16 12:51:50 INFO TaskSetManager: Finished task 96.0 in stage 13.0 (TID 1132) in 466 ms on localhost (98/200)
15/08/16 12:51:51 INFO Executor: Finished task 94.0 in stage 13.0 (TID 1130). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Starting task 114.0 in stage 13.0 (TID 1150, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 114.0 in stage 13.0 (TID 1150)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Finished task 94.0 in stage 13.0 (TID 1130) in 482 ms on localhost (99/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 97.0 in stage 13.0 (TID 1133). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 115.0 in stage 13.0 (TID 1151, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 115.0 in stage 13.0 (TID 1151)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 97.0 in stage 13.0 (TID 1133) in 471 ms on localhost (100/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 98.0 in stage 13.0 (TID 1134). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 116.0 in stage 13.0 (TID 1152, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 116.0 in stage 13.0 (TID 1152)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 98.0 in stage 13.0 (TID 1134) in 358 ms on localhost (101/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO Executor: Finished task 113.0 in stage 13.0 (TID 1149). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO TaskSetManager: Starting task 117.0 in stage 13.0 (TID 1153, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 117.0 in stage 13.0 (TID 1153)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 113.0 in stage 13.0 (TID 1149) in 31 ms on localhost (102/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 115.0 in stage 13.0 (TID 1151). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 118.0 in stage 13.0 (TID 1154, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 118.0 in stage 13.0 (TID 1154)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 115.0 in stage 13.0 (TID 1151) in 27 ms on localhost (103/200)
15/08/16 12:51:51 INFO Executor: Finished task 99.0 in stage 13.0 (TID 1135). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 119.0 in stage 13.0 (TID 1155, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 119.0 in stage 13.0 (TID 1155)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Finished task 99.0 in stage 13.0 (TID 1135) in 344 ms on localhost (104/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 117.0 in stage 13.0 (TID 1153). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 120.0 in stage 13.0 (TID 1156, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 120.0 in stage 13.0 (TID 1156)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 117.0 in stage 13.0 (TID 1153) in 25 ms on localhost (105/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 100.0 in stage 13.0 (TID 1136). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO Executor: Finished task 118.0 in stage 13.0 (TID 1154). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 121.0 in stage 13.0 (TID 1157, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 121.0 in stage 13.0 (TID 1157)
15/08/16 12:51:51 INFO TaskSetManager: Starting task 122.0 in stage 13.0 (TID 1158, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 122.0 in stage 13.0 (TID 1158)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 100.0 in stage 13.0 (TID 1136) in 333 ms on localhost (106/200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 118.0 in stage 13.0 (TID 1154) in 29 ms on localhost (107/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 101.0 in stage 13.0 (TID 1137). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 123.0 in stage 13.0 (TID 1159, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 123.0 in stage 13.0 (TID 1159)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 101.0 in stage 13.0 (TID 1137) in 317 ms on localhost (108/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 102.0 in stage 13.0 (TID 1138). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 124.0 in stage 13.0 (TID 1160, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 124.0 in stage 13.0 (TID 1160)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 102.0 in stage 13.0 (TID 1138) in 305 ms on localhost (109/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 105.0 in stage 13.0 (TID 1141). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 125.0 in stage 13.0 (TID 1161, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 125.0 in stage 13.0 (TID 1161)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 105.0 in stage 13.0 (TID 1141) in 302 ms on localhost (110/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 106.0 in stage 13.0 (TID 1142). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 126.0 in stage 13.0 (TID 1162, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 126.0 in stage 13.0 (TID 1162)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 106.0 in stage 13.0 (TID 1142) in 301 ms on localhost (111/200)
15/08/16 12:51:51 INFO Executor: Finished task 107.0 in stage 13.0 (TID 1143). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 127.0 in stage 13.0 (TID 1163, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Running task 127.0 in stage 13.0 (TID 1163)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Finished task 107.0 in stage 13.0 (TID 1143) in 302 ms on localhost (112/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 108.0 in stage 13.0 (TID 1144). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 128.0 in stage 13.0 (TID 1164, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 128.0 in stage 13.0 (TID 1164)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 108.0 in stage 13.0 (TID 1144) in 302 ms on localhost (113/200)
15/08/16 12:51:51 INFO Executor: Finished task 109.0 in stage 13.0 (TID 1145). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 129.0 in stage 13.0 (TID 1165, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO Executor: Running task 129.0 in stage 13.0 (TID 1165)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Finished task 109.0 in stage 13.0 (TID 1145) in 301 ms on localhost (114/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 112.0 in stage 13.0 (TID 1148). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 130.0 in stage 13.0 (TID 1166, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 130.0 in stage 13.0 (TID 1166)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 112.0 in stage 13.0 (TID 1148) in 342 ms on localhost (115/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 114.0 in stage 13.0 (TID 1150). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 131.0 in stage 13.0 (TID 1167, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 131.0 in stage 13.0 (TID 1167)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 114.0 in stage 13.0 (TID 1150) in 335 ms on localhost (116/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 110.0 in stage 13.0 (TID 1146). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 132.0 in stage 13.0 (TID 1168, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 132.0 in stage 13.0 (TID 1168)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 110.0 in stage 13.0 (TID 1146) in 384 ms on localhost (117/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 111.0 in stage 13.0 (TID 1147). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 133.0 in stage 13.0 (TID 1169, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 133.0 in stage 13.0 (TID 1169)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 111.0 in stage 13.0 (TID 1147) in 377 ms on localhost (118/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 116.0 in stage 13.0 (TID 1152). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 134.0 in stage 13.0 (TID 1170, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 134.0 in stage 13.0 (TID 1170)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 116.0 in stage 13.0 (TID 1152) in 339 ms on localhost (119/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 119.0 in stage 13.0 (TID 1155). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 135.0 in stage 13.0 (TID 1171, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 135.0 in stage 13.0 (TID 1171)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 119.0 in stage 13.0 (TID 1155) in 347 ms on localhost (120/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 120.0 in stage 13.0 (TID 1156). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 136.0 in stage 13.0 (TID 1172, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 136.0 in stage 13.0 (TID 1172)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 120.0 in stage 13.0 (TID 1156) in 345 ms on localhost (121/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 122.0 in stage 13.0 (TID 1158). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO Executor: Finished task 121.0 in stage 13.0 (TID 1157). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 137.0 in stage 13.0 (TID 1173, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 137.0 in stage 13.0 (TID 1173)
15/08/16 12:51:51 INFO TaskSetManager: Starting task 138.0 in stage 13.0 (TID 1174, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 138.0 in stage 13.0 (TID 1174)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 122.0 in stage 13.0 (TID 1158) in 343 ms on localhost (122/200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 121.0 in stage 13.0 (TID 1157) in 345 ms on localhost (123/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 123.0 in stage 13.0 (TID 1159). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 139.0 in stage 13.0 (TID 1175, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 139.0 in stage 13.0 (TID 1175)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 123.0 in stage 13.0 (TID 1159) in 338 ms on localhost (124/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 124.0 in stage 13.0 (TID 1160). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 140.0 in stage 13.0 (TID 1176, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 140.0 in stage 13.0 (TID 1176)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 124.0 in stage 13.0 (TID 1160) in 336 ms on localhost (125/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 125.0 in stage 13.0 (TID 1161). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 141.0 in stage 13.0 (TID 1177, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 141.0 in stage 13.0 (TID 1177)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 125.0 in stage 13.0 (TID 1161) in 351 ms on localhost (126/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 127.0 in stage 13.0 (TID 1163). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 142.0 in stage 13.0 (TID 1178, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 142.0 in stage 13.0 (TID 1178)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 127.0 in stage 13.0 (TID 1163) in 347 ms on localhost (127/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 128.0 in stage 13.0 (TID 1164). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 143.0 in stage 13.0 (TID 1179, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 143.0 in stage 13.0 (TID 1179)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 128.0 in stage 13.0 (TID 1164) in 345 ms on localhost (128/200)
15/08/16 12:51:51 INFO Executor: Finished task 126.0 in stage 13.0 (TID 1162). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO Executor: Finished task 129.0 in stage 13.0 (TID 1165). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO TaskSetManager: Starting task 144.0 in stage 13.0 (TID 1180, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 144.0 in stage 13.0 (TID 1180)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO TaskSetManager: Starting task 145.0 in stage 13.0 (TID 1181, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Running task 145.0 in stage 13.0 (TID 1181)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 126.0 in stage 13.0 (TID 1162) in 368 ms on localhost (129/200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 129.0 in stage 13.0 (TID 1165) in 344 ms on localhost (130/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 130.0 in stage 13.0 (TID 1166). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 146.0 in stage 13.0 (TID 1182, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 146.0 in stage 13.0 (TID 1182)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 130.0 in stage 13.0 (TID 1166) in 332 ms on localhost (131/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 131.0 in stage 13.0 (TID 1167). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 147.0 in stage 13.0 (TID 1183, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 147.0 in stage 13.0 (TID 1183)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 131.0 in stage 13.0 (TID 1167) in 327 ms on localhost (132/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 132.0 in stage 13.0 (TID 1168). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 148.0 in stage 13.0 (TID 1184, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 148.0 in stage 13.0 (TID 1184)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 132.0 in stage 13.0 (TID 1168) in 345 ms on localhost (133/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 133.0 in stage 13.0 (TID 1169). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 149.0 in stage 13.0 (TID 1185, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 149.0 in stage 13.0 (TID 1185)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 133.0 in stage 13.0 (TID 1169) in 346 ms on localhost (134/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 134.0 in stage 13.0 (TID 1170). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 150.0 in stage 13.0 (TID 1186, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 150.0 in stage 13.0 (TID 1186)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 134.0 in stage 13.0 (TID 1170) in 344 ms on localhost (135/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 149.0 in stage 13.0 (TID 1185). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 151.0 in stage 13.0 (TID 1187, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 151.0 in stage 13.0 (TID 1187)
15/08/16 12:51:51 INFO Executor: Finished task 136.0 in stage 13.0 (TID 1172). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 152.0 in stage 13.0 (TID 1188, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 152.0 in stage 13.0 (TID 1188)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 149.0 in stage 13.0 (TID 1185) in 25 ms on localhost (136/200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 136.0 in stage 13.0 (TID 1172) in 330 ms on localhost (137/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 135.0 in stage 13.0 (TID 1171). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 153.0 in stage 13.0 (TID 1189, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 153.0 in stage 13.0 (TID 1189)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 135.0 in stage 13.0 (TID 1171) in 343 ms on localhost (138/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 137.0 in stage 13.0 (TID 1173). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 154.0 in stage 13.0 (TID 1190, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 154.0 in stage 13.0 (TID 1190)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 137.0 in stage 13.0 (TID 1173) in 339 ms on localhost (139/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 138.0 in stage 13.0 (TID 1174). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 155.0 in stage 13.0 (TID 1191, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 155.0 in stage 13.0 (TID 1191)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 138.0 in stage 13.0 (TID 1174) in 352 ms on localhost (140/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 139.0 in stage 13.0 (TID 1175). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 156.0 in stage 13.0 (TID 1192, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 156.0 in stage 13.0 (TID 1192)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 139.0 in stage 13.0 (TID 1175) in 332 ms on localhost (141/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 140.0 in stage 13.0 (TID 1176). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 157.0 in stage 13.0 (TID 1193, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 157.0 in stage 13.0 (TID 1193)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 140.0 in stage 13.0 (TID 1176) in 330 ms on localhost (142/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 141.0 in stage 13.0 (TID 1177). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 158.0 in stage 13.0 (TID 1194, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 158.0 in stage 13.0 (TID 1194)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 141.0 in stage 13.0 (TID 1177) in 338 ms on localhost (143/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:51 INFO Executor: Finished task 142.0 in stage 13.0 (TID 1178). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 159.0 in stage 13.0 (TID 1195, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 159.0 in stage 13.0 (TID 1195)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 142.0 in stage 13.0 (TID 1178) in 341 ms on localhost (144/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 144.0 in stage 13.0 (TID 1180). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 160.0 in stage 13.0 (TID 1196, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 160.0 in stage 13.0 (TID 1196)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 144.0 in stage 13.0 (TID 1180) in 349 ms on localhost (145/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 145.0 in stage 13.0 (TID 1181). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO TaskSetManager: Starting task 161.0 in stage 13.0 (TID 1197, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Running task 161.0 in stage 13.0 (TID 1197)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 145.0 in stage 13.0 (TID 1181) in 352 ms on localhost (146/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 143.0 in stage 13.0 (TID 1179). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 162.0 in stage 13.0 (TID 1198, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 162.0 in stage 13.0 (TID 1198)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO TaskSetManager: Finished task 143.0 in stage 13.0 (TID 1179) in 360 ms on localhost (147/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO Executor: Finished task 146.0 in stage 13.0 (TID 1182). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO Executor: Finished task 147.0 in stage 13.0 (TID 1183). 1219 bytes result sent to driver
15/08/16 12:51:51 INFO TaskSetManager: Starting task 163.0 in stage 13.0 (TID 1199, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 163.0 in stage 13.0 (TID 1199)
15/08/16 12:51:51 INFO TaskSetManager: Starting task 164.0 in stage 13.0 (TID 1200, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:51 INFO Executor: Running task 164.0 in stage 13.0 (TID 1200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 146.0 in stage 13.0 (TID 1182) in 333 ms on localhost (148/200)
15/08/16 12:51:51 INFO TaskSetManager: Finished task 147.0 in stage 13.0 (TID 1183) in 323 ms on localhost (149/200)
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 150.0 in stage 13.0 (TID 1186). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 165.0 in stage 13.0 (TID 1201, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Finished task 148.0 in stage 13.0 (TID 1184). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Running task 165.0 in stage 13.0 (TID 1201)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 166.0 in stage 13.0 (TID 1202, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 166.0 in stage 13.0 (TID 1202)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 150.0 in stage 13.0 (TID 1186) in 339 ms on localhost (150/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 148.0 in stage 13.0 (TID 1184) in 351 ms on localhost (151/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 166.0 in stage 13.0 (TID 1202). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 152.0 in stage 13.0 (TID 1188). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 167.0 in stage 13.0 (TID 1203, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 167.0 in stage 13.0 (TID 1203)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 168.0 in stage 13.0 (TID 1204, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 168.0 in stage 13.0 (TID 1204)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 166.0 in stage 13.0 (TID 1202) in 33 ms on localhost (152/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 152.0 in stage 13.0 (TID 1188) in 353 ms on localhost (153/200)
15/08/16 12:51:52 INFO Executor: Finished task 151.0 in stage 13.0 (TID 1187). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 169.0 in stage 13.0 (TID 1205, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Finished task 151.0 in stage 13.0 (TID 1187) in 356 ms on localhost (154/200)
15/08/16 12:51:52 INFO Executor: Running task 169.0 in stage 13.0 (TID 1205)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 153.0 in stage 13.0 (TID 1189). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 170.0 in stage 13.0 (TID 1206, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 170.0 in stage 13.0 (TID 1206)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Finished task 153.0 in stage 13.0 (TID 1189) in 354 ms on localhost (155/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 154.0 in stage 13.0 (TID 1190). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 171.0 in stage 13.0 (TID 1207, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 171.0 in stage 13.0 (TID 1207)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 154.0 in stage 13.0 (TID 1190) in 352 ms on localhost (156/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 155.0 in stage 13.0 (TID 1191). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 172.0 in stage 13.0 (TID 1208, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 172.0 in stage 13.0 (TID 1208)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 155.0 in stage 13.0 (TID 1191) in 351 ms on localhost (157/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 156.0 in stage 13.0 (TID 1192). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 173.0 in stage 13.0 (TID 1209, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 173.0 in stage 13.0 (TID 1209)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 156.0 in stage 13.0 (TID 1192) in 445 ms on localhost (158/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 157.0 in stage 13.0 (TID 1193). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 174.0 in stage 13.0 (TID 1210, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 174.0 in stage 13.0 (TID 1210)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 157.0 in stage 13.0 (TID 1193) in 449 ms on localhost (159/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 174.0 in stage 13.0 (TID 1210). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 175.0 in stage 13.0 (TID 1211, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 175.0 in stage 13.0 (TID 1211)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 174.0 in stage 13.0 (TID 1210) in 36 ms on localhost (160/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 158.0 in stage 13.0 (TID 1194). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 176.0 in stage 13.0 (TID 1212, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 176.0 in stage 13.0 (TID 1212)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 158.0 in stage 13.0 (TID 1194) in 432 ms on localhost (161/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 176.0 in stage 13.0 (TID 1212). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 177.0 in stage 13.0 (TID 1213, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 177.0 in stage 13.0 (TID 1213)
15/08/16 12:51:52 INFO Executor: Finished task 159.0 in stage 13.0 (TID 1195). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 178.0 in stage 13.0 (TID 1214, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 178.0 in stage 13.0 (TID 1214)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 176.0 in stage 13.0 (TID 1212) in 27 ms on localhost (162/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 159.0 in stage 13.0 (TID 1195) in 432 ms on localhost (163/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 160.0 in stage 13.0 (TID 1196). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 179.0 in stage 13.0 (TID 1215, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 179.0 in stage 13.0 (TID 1215)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 160.0 in stage 13.0 (TID 1196) in 442 ms on localhost (164/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 162.0 in stage 13.0 (TID 1198). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 180.0 in stage 13.0 (TID 1216, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 180.0 in stage 13.0 (TID 1216)
15/08/16 12:51:52 INFO Executor: Finished task 161.0 in stage 13.0 (TID 1197). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 162.0 in stage 13.0 (TID 1198) in 441 ms on localhost (165/200)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 181.0 in stage 13.0 (TID 1217, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 181.0 in stage 13.0 (TID 1217)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Finished task 161.0 in stage 13.0 (TID 1197) in 446 ms on localhost (166/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 164.0 in stage 13.0 (TID 1200). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 182.0 in stage 13.0 (TID 1218, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 182.0 in stage 13.0 (TID 1218)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 164.0 in stage 13.0 (TID 1200) in 449 ms on localhost (167/200)
15/08/16 12:51:52 INFO Executor: Finished task 163.0 in stage 13.0 (TID 1199). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 183.0 in stage 13.0 (TID 1219, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 183.0 in stage 13.0 (TID 1219)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Finished task 163.0 in stage 13.0 (TID 1199) in 454 ms on localhost (168/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 165.0 in stage 13.0 (TID 1201). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 184.0 in stage 13.0 (TID 1220, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 184.0 in stage 13.0 (TID 1220)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 165.0 in stage 13.0 (TID 1201) in 453 ms on localhost (169/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 167.0 in stage 13.0 (TID 1203). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 185.0 in stage 13.0 (TID 1221, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 185.0 in stage 13.0 (TID 1221)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 167.0 in stage 13.0 (TID 1203) in 483 ms on localhost (170/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 170.0 in stage 13.0 (TID 1206). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 186.0 in stage 13.0 (TID 1222, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Finished task 168.0 in stage 13.0 (TID 1204). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Running task 186.0 in stage 13.0 (TID 1222)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 187.0 in stage 13.0 (TID 1223, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 187.0 in stage 13.0 (TID 1223)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 170.0 in stage 13.0 (TID 1206) in 486 ms on localhost (171/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 168.0 in stage 13.0 (TID 1204) in 494 ms on localhost (172/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 171.0 in stage 13.0 (TID 1207). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 188.0 in stage 13.0 (TID 1224, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 188.0 in stage 13.0 (TID 1224)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 171.0 in stage 13.0 (TID 1207) in 483 ms on localhost (173/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 169.0 in stage 13.0 (TID 1205). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 185.0 in stage 13.0 (TID 1221). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 189.0 in stage 13.0 (TID 1225, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 189.0 in stage 13.0 (TID 1225)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 169.0 in stage 13.0 (TID 1205) in 507 ms on localhost (174/200)
15/08/16 12:51:52 INFO Executor: Finished task 172.0 in stage 13.0 (TID 1208). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 190.0 in stage 13.0 (TID 1226, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 190.0 in stage 13.0 (TID 1226)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 185.0 in stage 13.0 (TID 1221) in 31 ms on localhost (175/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Starting task 191.0 in stage 13.0 (TID 1227, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 191.0 in stage 13.0 (TID 1227)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO TaskSetManager: Finished task 172.0 in stage 13.0 (TID 1208) in 478 ms on localhost (176/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 173.0 in stage 13.0 (TID 1209). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 192.0 in stage 13.0 (TID 1228, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 192.0 in stage 13.0 (TID 1228)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 173.0 in stage 13.0 (TID 1209) in 368 ms on localhost (177/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 186.0 in stage 13.0 (TID 1222). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 193.0 in stage 13.0 (TID 1229, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 193.0 in stage 13.0 (TID 1229)
15/08/16 12:51:52 INFO Executor: Finished task 188.0 in stage 13.0 (TID 1224). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 194.0 in stage 13.0 (TID 1230, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 194.0 in stage 13.0 (TID 1230)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 186.0 in stage 13.0 (TID 1222) in 37 ms on localhost (178/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 188.0 in stage 13.0 (TID 1224) in 25 ms on localhost (179/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 189.0 in stage 13.0 (TID 1225). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 195.0 in stage 13.0 (TID 1231, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 195.0 in stage 13.0 (TID 1231)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 189.0 in stage 13.0 (TID 1225) in 25 ms on localhost (180/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 192.0 in stage 13.0 (TID 1228). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 196.0 in stage 13.0 (TID 1232, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 196.0 in stage 13.0 (TID 1232)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 192.0 in stage 13.0 (TID 1228) in 27 ms on localhost (181/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO Executor: Finished task 175.0 in stage 13.0 (TID 1211). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO TaskSetManager: Starting task 197.0 in stage 13.0 (TID 1233, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Running task 197.0 in stage 13.0 (TID 1233)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 175.0 in stage 13.0 (TID 1211) in 349 ms on localhost (182/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 195.0 in stage 13.0 (TID 1231). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 198.0 in stage 13.0 (TID 1234, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 198.0 in stage 13.0 (TID 1234)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 195.0 in stage 13.0 (TID 1231) in 27 ms on localhost (183/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 197.0 in stage 13.0 (TID 1233). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Starting task 199.0 in stage 13.0 (TID 1235, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 199.0 in stage 13.0 (TID 1235)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 197.0 in stage 13.0 (TID 1233) in 26 ms on localhost (184/200)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO Executor: Finished task 177.0 in stage 13.0 (TID 1213). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 178.0 in stage 13.0 (TID 1214). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 177.0 in stage 13.0 (TID 1213) in 346 ms on localhost (185/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 178.0 in stage 13.0 (TID 1214) in 344 ms on localhost (186/200)
15/08/16 12:51:52 INFO Executor: Finished task 179.0 in stage 13.0 (TID 1215). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 179.0 in stage 13.0 (TID 1215) in 337 ms on localhost (187/200)
15/08/16 12:51:52 INFO Executor: Finished task 180.0 in stage 13.0 (TID 1216). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 180.0 in stage 13.0 (TID 1216) in 339 ms on localhost (188/200)
15/08/16 12:51:52 INFO Executor: Finished task 181.0 in stage 13.0 (TID 1217). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 181.0 in stage 13.0 (TID 1217) in 353 ms on localhost (189/200)
15/08/16 12:51:52 INFO Executor: Finished task 182.0 in stage 13.0 (TID 1218). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 182.0 in stage 13.0 (TID 1218) in 317 ms on localhost (190/200)
15/08/16 12:51:52 INFO Executor: Finished task 183.0 in stage 13.0 (TID 1219). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 183.0 in stage 13.0 (TID 1219) in 323 ms on localhost (191/200)
15/08/16 12:51:52 INFO Executor: Finished task 184.0 in stage 13.0 (TID 1220). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 184.0 in stage 13.0 (TID 1220) in 296 ms on localhost (192/200)
15/08/16 12:51:52 INFO Executor: Finished task 187.0 in stage 13.0 (TID 1223). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 187.0 in stage 13.0 (TID 1223) in 256 ms on localhost (193/200)
15/08/16 12:51:52 INFO Executor: Finished task 191.0 in stage 13.0 (TID 1227). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 190.0 in stage 13.0 (TID 1226). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 191.0 in stage 13.0 (TID 1227) in 260 ms on localhost (194/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 190.0 in stage 13.0 (TID 1226) in 261 ms on localhost (195/200)
15/08/16 12:51:52 INFO Executor: Finished task 194.0 in stage 13.0 (TID 1230). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 194.0 in stage 13.0 (TID 1230) in 251 ms on localhost (196/200)
15/08/16 12:51:52 INFO Executor: Finished task 193.0 in stage 13.0 (TID 1229). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 196.0 in stage 13.0 (TID 1232). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 193.0 in stage 13.0 (TID 1229) in 256 ms on localhost (197/200)
15/08/16 12:51:52 INFO Executor: Finished task 198.0 in stage 13.0 (TID 1234). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO Executor: Finished task 199.0 in stage 13.0 (TID 1235). 1219 bytes result sent to driver
15/08/16 12:51:52 INFO TaskSetManager: Finished task 196.0 in stage 13.0 (TID 1232) in 241 ms on localhost (198/200)
15/08/16 12:51:52 INFO TaskSetManager: Finished task 199.0 in stage 13.0 (TID 1235) in 212 ms on localhost (199/200)
15/08/16 12:51:52 INFO DAGScheduler: ShuffleMapStage 13 (processCmd at CliDriver.java:423) finished in 4.309 s
15/08/16 12:51:52 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:52 INFO DAGScheduler: running: Set()
15/08/16 12:51:52 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16, ShuffleMapStage 14)
15/08/16 12:51:52 INFO DAGScheduler: failed: Set()
15/08/16 12:51:52 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@462bbb02
15/08/16 12:51:52 INFO TaskSetManager: Finished task 198.0 in stage 13.0 (TID 1234) in 227 ms on localhost (200/200)
15/08/16 12:51:52 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/08/16 12:51:52 INFO StatsReportListener: task runtime:(count: 200, mean: 340.365000, stdev: 157.790468, max: 677.000000, min: 25.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	25.0 ms	27.0 ms	33.0 ms	302.0 ms	353.0 ms	445.0 ms	484.0 ms	571.0 ms	677.0 ms
15/08/16 12:51:52 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 17779.450000, stdev: 10155.165535, max: 41878.000000, min: 0.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	12.5 KB	18.5 KB	24.1 KB	29.4 KB	34.0 KB	40.9 KB
15/08/16 12:51:52 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.065000, stdev: 0.266036, max: 2.000000, min: 0.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms
15/08/16 12:51:52 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List(ShuffleMapStage 14)
15/08/16 12:51:52 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:51:52 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:52 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/16 12:51:52 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 93.926598, stdev: 8.830797, max: 98.893805, min: 61.290323)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	61 %	72 %	76 %	97 %	98 %	98 %	98 %	99 %	99 %
15/08/16 12:51:52 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.072055, stdev: 0.476923, max: 4.000000, min: 0.000000)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 4 %
15/08/16 12:51:52 INFO DAGScheduler: Missing parents for ShuffleMapStage 14: List()
15/08/16 12:51:52 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[67] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:51:52 INFO StatsReportListener: other time pct: (count: 200, mean: 6.001346, stdev: 8.698371, max: 38.709677, min: 1.106195)
15/08/16 12:51:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:52 INFO StatsReportListener: 	 1 %	 1 %	 2 %	 2 %	 2 %	 3 %	24 %	28 %	39 %
15/08/16 12:51:52 INFO MemoryStore: ensureFreeSpace(15336) called with curMem=508678287, maxMem=3333968363
15/08/16 12:51:52 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 2.6 GB)
15/08/16 12:51:52 INFO MemoryStore: ensureFreeSpace(7247) called with curMem=508693623, maxMem=3333968363
15/08/16 12:51:52 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.1 KB, free 2.6 GB)
15/08/16 12:51:52 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:36543 (size: 7.1 KB, free: 3.1 GB)
15/08/16 12:51:52 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:52 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[67] at processCmd at CliDriver.java:423)
15/08/16 12:51:52 INFO TaskSchedulerImpl: Adding task set 14.0 with 200 tasks
15/08/16 12:51:52 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 1236, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 1237, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 1238, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 1239, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 1240, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 1241, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 1242, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 1243, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 1244, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 1245, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 1246, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 1247, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 1248, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 1249, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 1250, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 1251, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:52 INFO Executor: Running task 0.0 in stage 14.0 (TID 1236)
15/08/16 12:51:52 INFO Executor: Running task 6.0 in stage 14.0 (TID 1242)
15/08/16 12:51:52 INFO Executor: Running task 4.0 in stage 14.0 (TID 1240)
15/08/16 12:51:52 INFO Executor: Running task 2.0 in stage 14.0 (TID 1238)
15/08/16 12:51:52 INFO Executor: Running task 1.0 in stage 14.0 (TID 1237)
15/08/16 12:51:52 INFO Executor: Running task 5.0 in stage 14.0 (TID 1241)
15/08/16 12:51:52 INFO Executor: Running task 3.0 in stage 14.0 (TID 1239)
15/08/16 12:51:52 INFO Executor: Running task 12.0 in stage 14.0 (TID 1248)
15/08/16 12:51:52 INFO Executor: Running task 15.0 in stage 14.0 (TID 1251)
15/08/16 12:51:52 INFO Executor: Running task 11.0 in stage 14.0 (TID 1247)
15/08/16 12:51:52 INFO Executor: Running task 8.0 in stage 14.0 (TID 1244)
15/08/16 12:51:52 INFO Executor: Running task 10.0 in stage 14.0 (TID 1246)
15/08/16 12:51:52 INFO Executor: Running task 13.0 in stage 14.0 (TID 1249)
15/08/16 12:51:52 INFO Executor: Running task 9.0 in stage 14.0 (TID 1245)
15/08/16 12:51:52 INFO Executor: Running task 14.0 in stage 14.0 (TID 1250)
15/08/16 12:51:52 INFO Executor: Running task 7.0 in stage 14.0 (TID 1243)
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO Executor: Finished task 8.0 in stage 14.0 (TID 1244). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 1252, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 16.0 in stage 14.0 (TID 1252)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 1244) in 549 ms on localhost (1/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 15.0 in stage 14.0 (TID 1251). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 1253, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 17.0 in stage 14.0 (TID 1253)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 1251) in 573 ms on localhost (2/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 12.0 in stage 14.0 (TID 1248). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 1254, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 18.0 in stage 14.0 (TID 1254)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 1248) in 579 ms on localhost (3/200)
15/08/16 12:51:53 INFO Executor: Finished task 1.0 in stage 14.0 (TID 1237). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 1255, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Running task 19.0 in stage 14.0 (TID 1255)
15/08/16 12:51:53 INFO Executor: Finished task 11.0 in stage 14.0 (TID 1247). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 1256, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 1237) in 584 ms on localhost (4/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO Executor: Running task 20.0 in stage 14.0 (TID 1256)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 1247) in 585 ms on localhost (5/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 3.0 in stage 14.0 (TID 1239). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 1257, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 21.0 in stage 14.0 (TID 1257)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 1239) in 603 ms on localhost (6/200)
15/08/16 12:51:53 INFO Executor: Finished task 4.0 in stage 14.0 (TID 1240). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 1258, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 22.0 in stage 14.0 (TID 1258)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 1240) in 608 ms on localhost (7/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 14.0 in stage 14.0 (TID 1250). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 1259, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 23.0 in stage 14.0 (TID 1259)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 1250) in 619 ms on localhost (8/200)
15/08/16 12:51:53 INFO Executor: Finished task 10.0 in stage 14.0 (TID 1246). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 1260, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 1246) in 624 ms on localhost (9/200)
15/08/16 12:51:53 INFO Executor: Running task 24.0 in stage 14.0 (TID 1260)
15/08/16 12:51:53 INFO Executor: Finished task 9.0 in stage 14.0 (TID 1245). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 1261, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Finished task 13.0 in stage 14.0 (TID 1249). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO Executor: Running task 25.0 in stage 14.0 (TID 1261)
15/08/16 12:51:53 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 1262, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 26.0 in stage 14.0 (TID 1262)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 1245) in 630 ms on localhost (10/200)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 1249) in 630 ms on localhost (11/200)
15/08/16 12:51:53 INFO Executor: Finished task 7.0 in stage 14.0 (TID 1243). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 1263, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 27.0 in stage 14.0 (TID 1263)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 0.0 in stage 14.0 (TID 1236). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 1243) in 634 ms on localhost (12/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 1264, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 28.0 in stage 14.0 (TID 1264)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 1236) in 638 ms on localhost (13/200)
15/08/16 12:51:53 INFO Executor: Finished task 5.0 in stage 14.0 (TID 1241). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO Executor: Finished task 2.0 in stage 14.0 (TID 1238). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO Executor: Finished task 6.0 in stage 14.0 (TID 1242). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 1265, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 29.0 in stage 14.0 (TID 1265)
15/08/16 12:51:53 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 1266, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 1267, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 30.0 in stage 14.0 (TID 1266)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 1241) in 660 ms on localhost (14/200)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 1238) in 661 ms on localhost (15/200)
15/08/16 12:51:53 INFO Executor: Running task 31.0 in stage 14.0 (TID 1267)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 1242) in 661 ms on localhost (16/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 16.0 in stage 14.0 (TID 1252). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 1268, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 32.0 in stage 14.0 (TID 1268)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 1252) in 134 ms on localhost (17/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 17.0 in stage 14.0 (TID 1253). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 1269, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 33.0 in stage 14.0 (TID 1269)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 1253) in 164 ms on localhost (18/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO Executor: Finished task 20.0 in stage 14.0 (TID 1256). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 1270, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 34.0 in stage 14.0 (TID 1270)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 1256) in 362 ms on localhost (19/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO Executor: Finished task 18.0 in stage 14.0 (TID 1254). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 1271, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 35.0 in stage 14.0 (TID 1271)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 1254) in 383 ms on localhost (20/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:53 INFO Executor: Finished task 19.0 in stage 14.0 (TID 1255). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 1272, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 36.0 in stage 14.0 (TID 1272)
15/08/16 12:51:53 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 1255) in 510 ms on localhost (21/200)
15/08/16 12:51:53 INFO Executor: Finished task 21.0 in stage 14.0 (TID 1257). 1219 bytes result sent to driver
15/08/16 12:51:53 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 1273, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:53 INFO Executor: Running task 37.0 in stage 14.0 (TID 1273)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 1257) in 498 ms on localhost (22/200)
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 26.0 in stage 14.0 (TID 1262). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 1274, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 38.0 in stage 14.0 (TID 1274)
15/08/16 12:51:54 INFO Executor: Finished task 22.0 in stage 14.0 (TID 1258). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 1262) in 492 ms on localhost (23/200)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 1275, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 39.0 in stage 14.0 (TID 1275)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 1258) in 519 ms on localhost (24/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 27.0 in stage 14.0 (TID 1263). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 1276, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 40.0 in stage 14.0 (TID 1276)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 1263) in 503 ms on localhost (25/200)
15/08/16 12:51:54 INFO Executor: Finished task 28.0 in stage 14.0 (TID 1264). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 1277, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Finished task 25.0 in stage 14.0 (TID 1261). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO Executor: Running task 41.0 in stage 14.0 (TID 1277)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 1278, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 42.0 in stage 14.0 (TID 1278)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 1264) in 504 ms on localhost (26/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 1261) in 508 ms on localhost (27/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 31.0 in stage 14.0 (TID 1267). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 1279, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 43.0 in stage 14.0 (TID 1279)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 1267) in 488 ms on localhost (28/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 30.0 in stage 14.0 (TID 1266). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO Executor: Finished task 23.0 in stage 14.0 (TID 1259). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 1280, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 44.0 in stage 14.0 (TID 1280)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 1281, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 45.0 in stage 14.0 (TID 1281)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 1266) in 502 ms on localhost (29/200)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 1259) in 541 ms on localhost (30/200)
15/08/16 12:51:54 INFO Executor: Finished task 32.0 in stage 14.0 (TID 1268). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 1282, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 46.0 in stage 14.0 (TID 1282)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 1268) in 485 ms on localhost (31/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 33.0 in stage 14.0 (TID 1269). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 1283, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 47.0 in stage 14.0 (TID 1283)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 1269) in 432 ms on localhost (32/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO Executor: Finished task 29.0 in stage 14.0 (TID 1265). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO Executor: Finished task 24.0 in stage 14.0 (TID 1260). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 1284, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 48.0 in stage 14.0 (TID 1284)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 1285, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 1265) in 524 ms on localhost (33/200)
15/08/16 12:51:54 INFO Executor: Running task 49.0 in stage 14.0 (TID 1285)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 1260) in 560 ms on localhost (34/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 178 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 35.0 in stage 14.0 (TID 1271). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 50.0 in stage 14.0 (TID 1286, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 50.0 in stage 14.0 (TID 1286)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 1271) in 426 ms on localhost (35/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 34.0 in stage 14.0 (TID 1270). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 51.0 in stage 14.0 (TID 1287, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 51.0 in stage 14.0 (TID 1287)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 1270) in 457 ms on localhost (36/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 37.0 in stage 14.0 (TID 1273). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 52.0 in stage 14.0 (TID 1288, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 52.0 in stage 14.0 (TID 1288)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 1273) in 543 ms on localhost (37/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 36.0 in stage 14.0 (TID 1272). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 53.0 in stage 14.0 (TID 1289, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 53.0 in stage 14.0 (TID 1289)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 1272) in 573 ms on localhost (38/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 39.0 in stage 14.0 (TID 1275). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 54.0 in stage 14.0 (TID 1290, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Finished task 38.0 in stage 14.0 (TID 1274). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO Executor: Running task 54.0 in stage 14.0 (TID 1290)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 55.0 in stage 14.0 (TID 1291, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 55.0 in stage 14.0 (TID 1291)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 1275) in 595 ms on localhost (39/200)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 1274) in 598 ms on localhost (40/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 40.0 in stage 14.0 (TID 1276). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 56.0 in stage 14.0 (TID 1292, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 56.0 in stage 14.0 (TID 1292)
15/08/16 12:51:54 INFO Executor: Finished task 41.0 in stage 14.0 (TID 1277). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 1276) in 615 ms on localhost (41/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO TaskSetManager: Starting task 57.0 in stage 14.0 (TID 1293, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Running task 57.0 in stage 14.0 (TID 1293)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 1277) in 614 ms on localhost (42/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 42.0 in stage 14.0 (TID 1278). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 58.0 in stage 14.0 (TID 1294, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 58.0 in stage 14.0 (TID 1294)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 1278) in 619 ms on localhost (43/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 43.0 in stage 14.0 (TID 1279). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 59.0 in stage 14.0 (TID 1295, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 59.0 in stage 14.0 (TID 1295)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 1279) in 639 ms on localhost (44/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO Executor: Finished task 44.0 in stage 14.0 (TID 1280). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 60.0 in stage 14.0 (TID 1296, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 60.0 in stage 14.0 (TID 1296)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 1280) in 630 ms on localhost (45/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 46.0 in stage 14.0 (TID 1282). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 61.0 in stage 14.0 (TID 1297, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 61.0 in stage 14.0 (TID 1297)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 1282) in 636 ms on localhost (46/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO Executor: Finished task 45.0 in stage 14.0 (TID 1281). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 62.0 in stage 14.0 (TID 1298, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 62.0 in stage 14.0 (TID 1298)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 1281) in 645 ms on localhost (47/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 47.0 in stage 14.0 (TID 1283). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 63.0 in stage 14.0 (TID 1299, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 63.0 in stage 14.0 (TID 1299)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 1283) in 662 ms on localhost (48/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 48.0 in stage 14.0 (TID 1284). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 64.0 in stage 14.0 (TID 1300, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 64.0 in stage 14.0 (TID 1300)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 1284) in 656 ms on localhost (49/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 49.0 in stage 14.0 (TID 1285). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 65.0 in stage 14.0 (TID 1301, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 65.0 in stage 14.0 (TID 1301)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 1285) in 660 ms on localhost (50/200)
15/08/16 12:51:54 INFO Executor: Finished task 50.0 in stage 14.0 (TID 1286). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO Executor: Finished task 51.0 in stage 14.0 (TID 1287). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 66.0 in stage 14.0 (TID 1302, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 66.0 in stage 14.0 (TID 1302)
15/08/16 12:51:54 INFO TaskSetManager: Starting task 67.0 in stage 14.0 (TID 1303, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 67.0 in stage 14.0 (TID 1303)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO TaskSetManager: Finished task 50.0 in stage 14.0 (TID 1286) in 462 ms on localhost (51/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO TaskSetManager: Finished task 51.0 in stage 14.0 (TID 1287) in 446 ms on localhost (52/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:54 INFO Executor: Finished task 52.0 in stage 14.0 (TID 1288). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 68.0 in stage 14.0 (TID 1304, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 68.0 in stage 14.0 (TID 1304)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 52.0 in stage 14.0 (TID 1288) in 401 ms on localhost (53/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO Executor: Finished task 53.0 in stage 14.0 (TID 1289). 1219 bytes result sent to driver
15/08/16 12:51:54 INFO TaskSetManager: Starting task 69.0 in stage 14.0 (TID 1305, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:54 INFO Executor: Running task 69.0 in stage 14.0 (TID 1305)
15/08/16 12:51:54 INFO TaskSetManager: Finished task 53.0 in stage 14.0 (TID 1289) in 389 ms on localhost (54/200)
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 54.0 in stage 14.0 (TID 1290). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 55.0 in stage 14.0 (TID 1291). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 70.0 in stage 14.0 (TID 1306, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 70.0 in stage 14.0 (TID 1306)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 71.0 in stage 14.0 (TID 1307, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 71.0 in stage 14.0 (TID 1307)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 54.0 in stage 14.0 (TID 1290) in 409 ms on localhost (55/200)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 55.0 in stage 14.0 (TID 1291) in 408 ms on localhost (56/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 56.0 in stage 14.0 (TID 1292). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 72.0 in stage 14.0 (TID 1308, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 72.0 in stage 14.0 (TID 1308)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 56.0 in stage 14.0 (TID 1292) in 392 ms on localhost (57/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 57.0 in stage 14.0 (TID 1293). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 73.0 in stage 14.0 (TID 1309, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 73.0 in stage 14.0 (TID 1309)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 57.0 in stage 14.0 (TID 1293) in 398 ms on localhost (58/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 58.0 in stage 14.0 (TID 1294). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 74.0 in stage 14.0 (TID 1310, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 74.0 in stage 14.0 (TID 1310)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 58.0 in stage 14.0 (TID 1294) in 416 ms on localhost (59/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 60.0 in stage 14.0 (TID 1296). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 59.0 in stage 14.0 (TID 1295). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 75.0 in stage 14.0 (TID 1311, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 75.0 in stage 14.0 (TID 1311)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 76.0 in stage 14.0 (TID 1312, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 76.0 in stage 14.0 (TID 1312)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 60.0 in stage 14.0 (TID 1296) in 440 ms on localhost (60/200)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 59.0 in stage 14.0 (TID 1295) in 445 ms on localhost (61/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 61.0 in stage 14.0 (TID 1297). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO TaskSetManager: Starting task 77.0 in stage 14.0 (TID 1313, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 77.0 in stage 14.0 (TID 1313)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 61.0 in stage 14.0 (TID 1297) in 436 ms on localhost (62/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 62.0 in stage 14.0 (TID 1298). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 78.0 in stage 14.0 (TID 1314, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 78.0 in stage 14.0 (TID 1314)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 62.0 in stage 14.0 (TID 1298) in 435 ms on localhost (63/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 63.0 in stage 14.0 (TID 1299). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 79.0 in stage 14.0 (TID 1315, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 79.0 in stage 14.0 (TID 1315)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 63.0 in stage 14.0 (TID 1299) in 435 ms on localhost (64/200)
15/08/16 12:51:55 INFO Executor: Finished task 64.0 in stage 14.0 (TID 1300). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO TaskSetManager: Starting task 80.0 in stage 14.0 (TID 1316, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 80.0 in stage 14.0 (TID 1316)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 64.0 in stage 14.0 (TID 1300) in 431 ms on localhost (65/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 66.0 in stage 14.0 (TID 1302). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 65.0 in stage 14.0 (TID 1301). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 81.0 in stage 14.0 (TID 1317, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 82.0 in stage 14.0 (TID 1318, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 82.0 in stage 14.0 (TID 1318)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 66.0 in stage 14.0 (TID 1302) in 432 ms on localhost (66/200)
15/08/16 12:51:55 INFO Executor: Running task 81.0 in stage 14.0 (TID 1317)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 65.0 in stage 14.0 (TID 1301) in 438 ms on localhost (67/200)
15/08/16 12:51:55 INFO Executor: Finished task 67.0 in stage 14.0 (TID 1303). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 83.0 in stage 14.0 (TID 1319, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 83.0 in stage 14.0 (TID 1319)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 67.0 in stage 14.0 (TID 1303) in 436 ms on localhost (68/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 68.0 in stage 14.0 (TID 1304). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 84.0 in stage 14.0 (TID 1320, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 84.0 in stage 14.0 (TID 1320)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 68.0 in stage 14.0 (TID 1304) in 427 ms on localhost (69/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 69.0 in stage 14.0 (TID 1305). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 85.0 in stage 14.0 (TID 1321, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 85.0 in stage 14.0 (TID 1321)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 69.0 in stage 14.0 (TID 1305) in 420 ms on localhost (70/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 71.0 in stage 14.0 (TID 1307). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 86.0 in stage 14.0 (TID 1322, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 86.0 in stage 14.0 (TID 1322)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 71.0 in stage 14.0 (TID 1307) in 424 ms on localhost (71/200)
15/08/16 12:51:55 INFO Executor: Finished task 70.0 in stage 14.0 (TID 1306). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 87.0 in stage 14.0 (TID 1323, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 87.0 in stage 14.0 (TID 1323)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 70.0 in stage 14.0 (TID 1306) in 428 ms on localhost (72/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 72.0 in stage 14.0 (TID 1308). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 88.0 in stage 14.0 (TID 1324, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 88.0 in stage 14.0 (TID 1324)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 72.0 in stage 14.0 (TID 1308) in 422 ms on localhost (73/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 73.0 in stage 14.0 (TID 1309). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 89.0 in stage 14.0 (TID 1325, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 89.0 in stage 14.0 (TID 1325)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 73.0 in stage 14.0 (TID 1309) in 432 ms on localhost (74/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 74.0 in stage 14.0 (TID 1310). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 90.0 in stage 14.0 (TID 1326, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 90.0 in stage 14.0 (TID 1326)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 74.0 in stage 14.0 (TID 1310) in 434 ms on localhost (75/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 76.0 in stage 14.0 (TID 1312). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 75.0 in stage 14.0 (TID 1311). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 91.0 in stage 14.0 (TID 1327, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 91.0 in stage 14.0 (TID 1327)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 76.0 in stage 14.0 (TID 1312) in 464 ms on localhost (76/200)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 92.0 in stage 14.0 (TID 1328, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 75.0 in stage 14.0 (TID 1311) in 466 ms on localhost (77/200)
15/08/16 12:51:55 INFO Executor: Running task 92.0 in stage 14.0 (TID 1328)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 77.0 in stage 14.0 (TID 1313). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 78.0 in stage 14.0 (TID 1314). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 93.0 in stage 14.0 (TID 1329, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 93.0 in stage 14.0 (TID 1329)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 94.0 in stage 14.0 (TID 1330, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 94.0 in stage 14.0 (TID 1330)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 77.0 in stage 14.0 (TID 1313) in 482 ms on localhost (78/200)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 78.0 in stage 14.0 (TID 1314) in 476 ms on localhost (79/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 79.0 in stage 14.0 (TID 1315). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 95.0 in stage 14.0 (TID 1331, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 95.0 in stage 14.0 (TID 1331)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 79.0 in stage 14.0 (TID 1315) in 474 ms on localhost (80/200)
15/08/16 12:51:55 INFO Executor: Finished task 80.0 in stage 14.0 (TID 1316). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 96.0 in stage 14.0 (TID 1332, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 96.0 in stage 14.0 (TID 1332)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 80.0 in stage 14.0 (TID 1316) in 473 ms on localhost (81/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 81.0 in stage 14.0 (TID 1317). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 97.0 in stage 14.0 (TID 1333, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 97.0 in stage 14.0 (TID 1333)
15/08/16 12:51:55 INFO Executor: Finished task 83.0 in stage 14.0 (TID 1319). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 98.0 in stage 14.0 (TID 1334, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 98.0 in stage 14.0 (TID 1334)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 81.0 in stage 14.0 (TID 1317) in 471 ms on localhost (82/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 83.0 in stage 14.0 (TID 1319) in 469 ms on localhost (83/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 82.0 in stage 14.0 (TID 1318). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO TaskSetManager: Starting task 99.0 in stage 14.0 (TID 1335, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 99.0 in stage 14.0 (TID 1335)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO TaskSetManager: Finished task 82.0 in stage 14.0 (TID 1318) in 476 ms on localhost (84/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO Executor: Finished task 84.0 in stage 14.0 (TID 1320). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 100.0 in stage 14.0 (TID 1336, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 100.0 in stage 14.0 (TID 1336)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 84.0 in stage 14.0 (TID 1320) in 378 ms on localhost (85/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 85.0 in stage 14.0 (TID 1321). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 101.0 in stage 14.0 (TID 1337, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 101.0 in stage 14.0 (TID 1337)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 85.0 in stage 14.0 (TID 1321) in 378 ms on localhost (86/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 87.0 in stage 14.0 (TID 1323). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 88.0 in stage 14.0 (TID 1324). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO Executor: Finished task 86.0 in stage 14.0 (TID 1322). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 102.0 in stage 14.0 (TID 1338, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 103.0 in stage 14.0 (TID 1339, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 103.0 in stage 14.0 (TID 1339)
15/08/16 12:51:55 INFO Executor: Running task 102.0 in stage 14.0 (TID 1338)
15/08/16 12:51:55 INFO TaskSetManager: Starting task 104.0 in stage 14.0 (TID 1340, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 104.0 in stage 14.0 (TID 1340)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 87.0 in stage 14.0 (TID 1323) in 412 ms on localhost (87/200)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 88.0 in stage 14.0 (TID 1324) in 406 ms on localhost (88/200)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 86.0 in stage 14.0 (TID 1322) in 417 ms on localhost (89/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 89.0 in stage 14.0 (TID 1325). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 105.0 in stage 14.0 (TID 1341, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 105.0 in stage 14.0 (TID 1341)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 89.0 in stage 14.0 (TID 1325) in 404 ms on localhost (90/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 90.0 in stage 14.0 (TID 1326). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 106.0 in stage 14.0 (TID 1342, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 106.0 in stage 14.0 (TID 1342)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 90.0 in stage 14.0 (TID 1326) in 395 ms on localhost (91/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 92.0 in stage 14.0 (TID 1328). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 107.0 in stage 14.0 (TID 1343, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 107.0 in stage 14.0 (TID 1343)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 92.0 in stage 14.0 (TID 1328) in 398 ms on localhost (92/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:55 INFO Executor: Finished task 91.0 in stage 14.0 (TID 1327). 1219 bytes result sent to driver
15/08/16 12:51:55 INFO TaskSetManager: Starting task 108.0 in stage 14.0 (TID 1344, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:55 INFO Executor: Running task 108.0 in stage 14.0 (TID 1344)
15/08/16 12:51:55 INFO TaskSetManager: Finished task 91.0 in stage 14.0 (TID 1327) in 409 ms on localhost (93/200)
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 94.0 in stage 14.0 (TID 1330). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 109.0 in stage 14.0 (TID 1345, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 109.0 in stage 14.0 (TID 1345)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 94.0 in stage 14.0 (TID 1330) in 412 ms on localhost (94/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 93.0 in stage 14.0 (TID 1329). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 110.0 in stage 14.0 (TID 1346, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 110.0 in stage 14.0 (TID 1346)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 93.0 in stage 14.0 (TID 1329) in 418 ms on localhost (95/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 95.0 in stage 14.0 (TID 1331). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 111.0 in stage 14.0 (TID 1347, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 111.0 in stage 14.0 (TID 1347)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 95.0 in stage 14.0 (TID 1331) in 425 ms on localhost (96/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 98.0 in stage 14.0 (TID 1334). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO Executor: Finished task 99.0 in stage 14.0 (TID 1335). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 112.0 in stage 14.0 (TID 1348, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 112.0 in stage 14.0 (TID 1348)
15/08/16 12:51:56 INFO TaskSetManager: Starting task 113.0 in stage 14.0 (TID 1349, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 113.0 in stage 14.0 (TID 1349)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 98.0 in stage 14.0 (TID 1334) in 419 ms on localhost (97/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 99.0 in stage 14.0 (TID 1335) in 415 ms on localhost (98/200)
15/08/16 12:51:56 INFO Executor: Finished task 97.0 in stage 14.0 (TID 1333). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO TaskSetManager: Starting task 114.0 in stage 14.0 (TID 1350, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Running task 114.0 in stage 14.0 (TID 1350)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO TaskSetManager: Finished task 97.0 in stage 14.0 (TID 1333) in 423 ms on localhost (99/200)
15/08/16 12:51:56 INFO Executor: Finished task 96.0 in stage 14.0 (TID 1332). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 115.0 in stage 14.0 (TID 1351, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 115.0 in stage 14.0 (TID 1351)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 96.0 in stage 14.0 (TID 1332) in 434 ms on localhost (100/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 101.0 in stage 14.0 (TID 1337). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 116.0 in stage 14.0 (TID 1352, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 116.0 in stage 14.0 (TID 1352)
15/08/16 12:51:56 INFO Executor: Finished task 100.0 in stage 14.0 (TID 1336). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 117.0 in stage 14.0 (TID 1353, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 117.0 in stage 14.0 (TID 1353)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 101.0 in stage 14.0 (TID 1337) in 351 ms on localhost (101/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 100.0 in stage 14.0 (TID 1336) in 362 ms on localhost (102/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 102.0 in stage 14.0 (TID 1338). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO Executor: Finished task 104.0 in stage 14.0 (TID 1340). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 118.0 in stage 14.0 (TID 1354, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 118.0 in stage 14.0 (TID 1354)
15/08/16 12:51:56 INFO TaskSetManager: Starting task 119.0 in stage 14.0 (TID 1355, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 119.0 in stage 14.0 (TID 1355)
15/08/16 12:51:56 INFO Executor: Finished task 103.0 in stage 14.0 (TID 1339). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 120.0 in stage 14.0 (TID 1356, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 120.0 in stage 14.0 (TID 1356)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 104.0 in stage 14.0 (TID 1340) in 513 ms on localhost (103/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 102.0 in stage 14.0 (TID 1338) in 514 ms on localhost (104/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 103.0 in stage 14.0 (TID 1339) in 515 ms on localhost (105/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 105.0 in stage 14.0 (TID 1341). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 121.0 in stage 14.0 (TID 1357, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 121.0 in stage 14.0 (TID 1357)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 105.0 in stage 14.0 (TID 1341) in 511 ms on localhost (106/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 106.0 in stage 14.0 (TID 1342). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 122.0 in stage 14.0 (TID 1358, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 122.0 in stage 14.0 (TID 1358)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 106.0 in stage 14.0 (TID 1342) in 505 ms on localhost (107/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 107.0 in stage 14.0 (TID 1343). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 123.0 in stage 14.0 (TID 1359, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 123.0 in stage 14.0 (TID 1359)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 107.0 in stage 14.0 (TID 1343) in 596 ms on localhost (108/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 109.0 in stage 14.0 (TID 1345). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 124.0 in stage 14.0 (TID 1360, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 124.0 in stage 14.0 (TID 1360)
15/08/16 12:51:56 INFO Executor: Finished task 108.0 in stage 14.0 (TID 1344). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 125.0 in stage 14.0 (TID 1361, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 125.0 in stage 14.0 (TID 1361)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 109.0 in stage 14.0 (TID 1345) in 594 ms on localhost (109/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 108.0 in stage 14.0 (TID 1344) in 620 ms on localhost (110/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 110.0 in stage 14.0 (TID 1346). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 126.0 in stage 14.0 (TID 1362, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 126.0 in stage 14.0 (TID 1362)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 110.0 in stage 14.0 (TID 1346) in 599 ms on localhost (111/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO Executor: Finished task 111.0 in stage 14.0 (TID 1347). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 127.0 in stage 14.0 (TID 1363, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 127.0 in stage 14.0 (TID 1363)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 111.0 in stage 14.0 (TID 1347) in 625 ms on localhost (112/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO Executor: Finished task 112.0 in stage 14.0 (TID 1348). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 128.0 in stage 14.0 (TID 1364, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 128.0 in stage 14.0 (TID 1364)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 112.0 in stage 14.0 (TID 1348) in 630 ms on localhost (113/200)
15/08/16 12:51:56 INFO Executor: Finished task 113.0 in stage 14.0 (TID 1349). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 129.0 in stage 14.0 (TID 1365, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 129.0 in stage 14.0 (TID 1365)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO TaskSetManager: Finished task 113.0 in stage 14.0 (TID 1349) in 631 ms on localhost (114/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 114.0 in stage 14.0 (TID 1350). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 130.0 in stage 14.0 (TID 1366, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 130.0 in stage 14.0 (TID 1366)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO TaskSetManager: Finished task 114.0 in stage 14.0 (TID 1350) in 632 ms on localhost (115/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 117.0 in stage 14.0 (TID 1353). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 131.0 in stage 14.0 (TID 1367, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 131.0 in stage 14.0 (TID 1367)
15/08/16 12:51:56 INFO Executor: Finished task 116.0 in stage 14.0 (TID 1352). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Finished task 117.0 in stage 14.0 (TID 1353) in 614 ms on localhost (116/200)
15/08/16 12:51:56 INFO TaskSetManager: Starting task 132.0 in stage 14.0 (TID 1368, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 132.0 in stage 14.0 (TID 1368)
15/08/16 12:51:56 INFO Executor: Finished task 115.0 in stage 14.0 (TID 1351). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO TaskSetManager: Starting task 133.0 in stage 14.0 (TID 1369, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 133.0 in stage 14.0 (TID 1369)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 116.0 in stage 14.0 (TID 1352) in 617 ms on localhost (117/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 115.0 in stage 14.0 (TID 1351) in 645 ms on localhost (118/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 119.0 in stage 14.0 (TID 1355). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO Executor: Finished task 120.0 in stage 14.0 (TID 1356). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 134.0 in stage 14.0 (TID 1370, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 134.0 in stage 14.0 (TID 1370)
15/08/16 12:51:56 INFO TaskSetManager: Starting task 135.0 in stage 14.0 (TID 1371, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 119.0 in stage 14.0 (TID 1355) in 424 ms on localhost (119/200)
15/08/16 12:51:56 INFO Executor: Running task 135.0 in stage 14.0 (TID 1371)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 120.0 in stage 14.0 (TID 1356) in 423 ms on localhost (120/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO Executor: Finished task 118.0 in stage 14.0 (TID 1354). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO TaskSetManager: Starting task 136.0 in stage 14.0 (TID 1372, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Running task 136.0 in stage 14.0 (TID 1372)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO TaskSetManager: Finished task 118.0 in stage 14.0 (TID 1354) in 430 ms on localhost (121/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 121.0 in stage 14.0 (TID 1357). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO Executor: Finished task 122.0 in stage 14.0 (TID 1358). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 137.0 in stage 14.0 (TID 1373, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 137.0 in stage 14.0 (TID 1373)
15/08/16 12:51:56 INFO TaskSetManager: Starting task 138.0 in stage 14.0 (TID 1374, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 138.0 in stage 14.0 (TID 1374)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 121.0 in stage 14.0 (TID 1357) in 426 ms on localhost (122/200)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 122.0 in stage 14.0 (TID 1358) in 417 ms on localhost (123/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:56 INFO Executor: Finished task 123.0 in stage 14.0 (TID 1359). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 139.0 in stage 14.0 (TID 1375, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 139.0 in stage 14.0 (TID 1375)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 123.0 in stage 14.0 (TID 1359) in 378 ms on localhost (124/200)
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:56 INFO Executor: Finished task 124.0 in stage 14.0 (TID 1360). 1219 bytes result sent to driver
15/08/16 12:51:56 INFO TaskSetManager: Starting task 140.0 in stage 14.0 (TID 1376, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:56 INFO Executor: Running task 140.0 in stage 14.0 (TID 1376)
15/08/16 12:51:56 INFO TaskSetManager: Finished task 124.0 in stage 14.0 (TID 1360) in 396 ms on localhost (125/200)
15/08/16 12:51:57 INFO Executor: Finished task 125.0 in stage 14.0 (TID 1361). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 141.0 in stage 14.0 (TID 1377, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO Executor: Running task 141.0 in stage 14.0 (TID 1377)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 125.0 in stage 14.0 (TID 1361) in 411 ms on localhost (126/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 126.0 in stage 14.0 (TID 1362). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 142.0 in stage 14.0 (TID 1378, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 142.0 in stage 14.0 (TID 1378)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 126.0 in stage 14.0 (TID 1362) in 413 ms on localhost (127/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 127.0 in stage 14.0 (TID 1363). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 143.0 in stage 14.0 (TID 1379, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 143.0 in stage 14.0 (TID 1379)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 127.0 in stage 14.0 (TID 1363) in 427 ms on localhost (128/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 128.0 in stage 14.0 (TID 1364). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 144.0 in stage 14.0 (TID 1380, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 144.0 in stage 14.0 (TID 1380)
15/08/16 12:51:57 INFO Executor: Finished task 129.0 in stage 14.0 (TID 1365). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Finished task 128.0 in stage 14.0 (TID 1364) in 437 ms on localhost (129/200)
15/08/16 12:51:57 INFO TaskSetManager: Starting task 145.0 in stage 14.0 (TID 1381, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 145.0 in stage 14.0 (TID 1381)
15/08/16 12:51:57 INFO Executor: Finished task 130.0 in stage 14.0 (TID 1366). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Finished task 129.0 in stage 14.0 (TID 1365) in 437 ms on localhost (130/200)
15/08/16 12:51:57 INFO TaskSetManager: Starting task 146.0 in stage 14.0 (TID 1382, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Running task 146.0 in stage 14.0 (TID 1382)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 130.0 in stage 14.0 (TID 1366) in 438 ms on localhost (131/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 131.0 in stage 14.0 (TID 1367). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 147.0 in stage 14.0 (TID 1383, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 147.0 in stage 14.0 (TID 1383)
15/08/16 12:51:57 INFO Executor: Finished task 133.0 in stage 14.0 (TID 1369). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Finished task 131.0 in stage 14.0 (TID 1367) in 437 ms on localhost (132/200)
15/08/16 12:51:57 INFO Executor: Finished task 132.0 in stage 14.0 (TID 1368). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 148.0 in stage 14.0 (TID 1384, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 148.0 in stage 14.0 (TID 1384)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 149.0 in stage 14.0 (TID 1385, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 149.0 in stage 14.0 (TID 1385)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 133.0 in stage 14.0 (TID 1369) in 436 ms on localhost (133/200)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 132.0 in stage 14.0 (TID 1368) in 438 ms on localhost (134/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 134.0 in stage 14.0 (TID 1370). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 150.0 in stage 14.0 (TID 1386, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 150.0 in stage 14.0 (TID 1386)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 134.0 in stage 14.0 (TID 1370) in 433 ms on localhost (135/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 135.0 in stage 14.0 (TID 1371). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 151.0 in stage 14.0 (TID 1387, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 151.0 in stage 14.0 (TID 1387)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 135.0 in stage 14.0 (TID 1371) in 443 ms on localhost (136/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO Executor: Finished task 137.0 in stage 14.0 (TID 1373). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO Executor: Finished task 136.0 in stage 14.0 (TID 1372). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 152.0 in stage 14.0 (TID 1388, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 152.0 in stage 14.0 (TID 1388)
15/08/16 12:51:57 INFO TaskSetManager: Starting task 153.0 in stage 14.0 (TID 1389, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 153.0 in stage 14.0 (TID 1389)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 137.0 in stage 14.0 (TID 1373) in 433 ms on localhost (137/200)
15/08/16 12:51:57 INFO Executor: Finished task 138.0 in stage 14.0 (TID 1374). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 154.0 in stage 14.0 (TID 1390, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 154.0 in stage 14.0 (TID 1390)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 136.0 in stage 14.0 (TID 1372) in 445 ms on localhost (138/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 138.0 in stage 14.0 (TID 1374) in 436 ms on localhost (139/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 139.0 in stage 14.0 (TID 1375). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 155.0 in stage 14.0 (TID 1391, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 155.0 in stage 14.0 (TID 1391)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 139.0 in stage 14.0 (TID 1375) in 427 ms on localhost (140/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 141.0 in stage 14.0 (TID 1377). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 156.0 in stage 14.0 (TID 1392, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 156.0 in stage 14.0 (TID 1392)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 141.0 in stage 14.0 (TID 1377) in 428 ms on localhost (141/200)
15/08/16 12:51:57 INFO Executor: Finished task 140.0 in stage 14.0 (TID 1376). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 157.0 in stage 14.0 (TID 1393, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Running task 157.0 in stage 14.0 (TID 1393)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 140.0 in stage 14.0 (TID 1376) in 450 ms on localhost (142/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO Executor: Finished task 142.0 in stage 14.0 (TID 1378). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 158.0 in stage 14.0 (TID 1394, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 158.0 in stage 14.0 (TID 1394)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 142.0 in stage 14.0 (TID 1378) in 426 ms on localhost (143/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 143.0 in stage 14.0 (TID 1379). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 159.0 in stage 14.0 (TID 1395, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 159.0 in stage 14.0 (TID 1395)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 143.0 in stage 14.0 (TID 1379) in 445 ms on localhost (144/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 146.0 in stage 14.0 (TID 1382). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO Executor: Finished task 144.0 in stage 14.0 (TID 1380). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 160.0 in stage 14.0 (TID 1396, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 160.0 in stage 14.0 (TID 1396)
15/08/16 12:51:57 INFO TaskSetManager: Starting task 161.0 in stage 14.0 (TID 1397, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 161.0 in stage 14.0 (TID 1397)
15/08/16 12:51:57 INFO Executor: Finished task 145.0 in stage 14.0 (TID 1381). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 162.0 in stage 14.0 (TID 1398, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 162.0 in stage 14.0 (TID 1398)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 146.0 in stage 14.0 (TID 1382) in 474 ms on localhost (145/200)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 144.0 in stage 14.0 (TID 1380) in 480 ms on localhost (146/200)
15/08/16 12:51:57 INFO Executor: Finished task 147.0 in stage 14.0 (TID 1383). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 163.0 in stage 14.0 (TID 1399, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 145.0 in stage 14.0 (TID 1381) in 479 ms on localhost (147/200)
15/08/16 12:51:57 INFO Executor: Running task 163.0 in stage 14.0 (TID 1399)
15/08/16 12:51:57 INFO Executor: Finished task 148.0 in stage 14.0 (TID 1384). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 164.0 in stage 14.0 (TID 1400, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 164.0 in stage 14.0 (TID 1400)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 147.0 in stage 14.0 (TID 1383) in 469 ms on localhost (148/200)
15/08/16 12:51:57 INFO Executor: Finished task 149.0 in stage 14.0 (TID 1385). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO TaskSetManager: Starting task 165.0 in stage 14.0 (TID 1401, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Running task 165.0 in stage 14.0 (TID 1401)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 148.0 in stage 14.0 (TID 1384) in 466 ms on localhost (149/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 149.0 in stage 14.0 (TID 1385) in 466 ms on localhost (150/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 150.0 in stage 14.0 (TID 1386). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 166.0 in stage 14.0 (TID 1402, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 166.0 in stage 14.0 (TID 1402)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 150.0 in stage 14.0 (TID 1386) in 437 ms on localhost (151/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 151.0 in stage 14.0 (TID 1387). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 167.0 in stage 14.0 (TID 1403, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 167.0 in stage 14.0 (TID 1403)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 151.0 in stage 14.0 (TID 1387) in 438 ms on localhost (152/200)
15/08/16 12:51:57 INFO Executor: Finished task 152.0 in stage 14.0 (TID 1388). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 168.0 in stage 14.0 (TID 1404, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 168.0 in stage 14.0 (TID 1404)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 152.0 in stage 14.0 (TID 1388) in 435 ms on localhost (153/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 154.0 in stage 14.0 (TID 1390). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO Executor: Finished task 153.0 in stage 14.0 (TID 1389). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 169.0 in stage 14.0 (TID 1405, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 169.0 in stage 14.0 (TID 1405)
15/08/16 12:51:57 INFO TaskSetManager: Starting task 170.0 in stage 14.0 (TID 1406, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 170.0 in stage 14.0 (TID 1406)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 154.0 in stage 14.0 (TID 1390) in 439 ms on localhost (154/200)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 153.0 in stage 14.0 (TID 1389) in 441 ms on localhost (155/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 155.0 in stage 14.0 (TID 1391). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 171.0 in stage 14.0 (TID 1407, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 171.0 in stage 14.0 (TID 1407)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 155.0 in stage 14.0 (TID 1391) in 363 ms on localhost (156/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 157.0 in stage 14.0 (TID 1393). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 172.0 in stage 14.0 (TID 1408, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 172.0 in stage 14.0 (TID 1408)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 157.0 in stage 14.0 (TID 1393) in 368 ms on localhost (157/200)
15/08/16 12:51:57 INFO Executor: Finished task 158.0 in stage 14.0 (TID 1394). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 173.0 in stage 14.0 (TID 1409, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 173.0 in stage 14.0 (TID 1409)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Finished task 158.0 in stage 14.0 (TID 1394) in 366 ms on localhost (158/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 156.0 in stage 14.0 (TID 1392). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO TaskSetManager: Starting task 174.0 in stage 14.0 (TID 1410, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 174.0 in stage 14.0 (TID 1410)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 156.0 in stage 14.0 (TID 1392) in 380 ms on localhost (159/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO Executor: Finished task 159.0 in stage 14.0 (TID 1395). 1219 bytes result sent to driver
15/08/16 12:51:57 INFO TaskSetManager: Starting task 175.0 in stage 14.0 (TID 1411, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:57 INFO Executor: Running task 175.0 in stage 14.0 (TID 1411)
15/08/16 12:51:57 INFO TaskSetManager: Finished task 159.0 in stage 14.0 (TID 1395) in 395 ms on localhost (160/200)
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 161.0 in stage 14.0 (TID 1397). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 176.0 in stage 14.0 (TID 1412, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 176.0 in stage 14.0 (TID 1412)
15/08/16 12:51:58 INFO Executor: Finished task 162.0 in stage 14.0 (TID 1398). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 177.0 in stage 14.0 (TID 1413, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 177.0 in stage 14.0 (TID 1413)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 161.0 in stage 14.0 (TID 1397) in 463 ms on localhost (161/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 162.0 in stage 14.0 (TID 1398) in 463 ms on localhost (162/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 163.0 in stage 14.0 (TID 1399). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO TaskSetManager: Starting task 178.0 in stage 14.0 (TID 1414, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Running task 178.0 in stage 14.0 (TID 1414)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 163.0 in stage 14.0 (TID 1399) in 464 ms on localhost (163/200)
15/08/16 12:51:58 INFO Executor: Finished task 160.0 in stage 14.0 (TID 1396). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Starting task 179.0 in stage 14.0 (TID 1415, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Running task 179.0 in stage 14.0 (TID 1415)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 160.0 in stage 14.0 (TID 1396) in 471 ms on localhost (164/200)
15/08/16 12:51:58 INFO Executor: Finished task 165.0 in stage 14.0 (TID 1401). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Starting task 180.0 in stage 14.0 (TID 1416, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO Executor: Running task 180.0 in stage 14.0 (TID 1416)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Finished task 165.0 in stage 14.0 (TID 1401) in 469 ms on localhost (165/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO Executor: Finished task 164.0 in stage 14.0 (TID 1400). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO TaskSetManager: Starting task 181.0 in stage 14.0 (TID 1417, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 181.0 in stage 14.0 (TID 1417)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 164.0 in stage 14.0 (TID 1400) in 475 ms on localhost (166/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 166.0 in stage 14.0 (TID 1402). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 182.0 in stage 14.0 (TID 1418, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 182.0 in stage 14.0 (TID 1418)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 166.0 in stage 14.0 (TID 1402) in 471 ms on localhost (167/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 167.0 in stage 14.0 (TID 1403). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 183.0 in stage 14.0 (TID 1419, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 183.0 in stage 14.0 (TID 1419)
15/08/16 12:51:58 INFO Executor: Finished task 168.0 in stage 14.0 (TID 1404). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 184.0 in stage 14.0 (TID 1420, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 167.0 in stage 14.0 (TID 1403) in 471 ms on localhost (168/200)
15/08/16 12:51:58 INFO Executor: Running task 184.0 in stage 14.0 (TID 1420)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 168.0 in stage 14.0 (TID 1404) in 468 ms on localhost (169/200)
15/08/16 12:51:58 INFO Executor: Finished task 169.0 in stage 14.0 (TID 1405). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Starting task 185.0 in stage 14.0 (TID 1421, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 185.0 in stage 14.0 (TID 1421)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Finished task 169.0 in stage 14.0 (TID 1405) in 464 ms on localhost (170/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 170.0 in stage 14.0 (TID 1406). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 186.0 in stage 14.0 (TID 1422, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 186.0 in stage 14.0 (TID 1422)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 170.0 in stage 14.0 (TID 1406) in 467 ms on localhost (171/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 171.0 in stage 14.0 (TID 1407). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 187.0 in stage 14.0 (TID 1423, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 187.0 in stage 14.0 (TID 1423)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 171.0 in stage 14.0 (TID 1407) in 417 ms on localhost (172/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 173.0 in stage 14.0 (TID 1409). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 188.0 in stage 14.0 (TID 1424, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Finished task 172.0 in stage 14.0 (TID 1408). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Running task 188.0 in stage 14.0 (TID 1424)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 189.0 in stage 14.0 (TID 1425, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 189.0 in stage 14.0 (TID 1425)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 173.0 in stage 14.0 (TID 1409) in 385 ms on localhost (173/200)
15/08/16 12:51:58 INFO Executor: Finished task 174.0 in stage 14.0 (TID 1410). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 190.0 in stage 14.0 (TID 1426, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 172.0 in stage 14.0 (TID 1408) in 390 ms on localhost (174/200)
15/08/16 12:51:58 INFO Executor: Running task 190.0 in stage 14.0 (TID 1426)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 174.0 in stage 14.0 (TID 1410) in 384 ms on localhost (175/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 175.0 in stage 14.0 (TID 1411). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 191.0 in stage 14.0 (TID 1427, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 191.0 in stage 14.0 (TID 1427)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 175.0 in stage 14.0 (TID 1411) in 582 ms on localhost (176/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 177.0 in stage 14.0 (TID 1413). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 176.0 in stage 14.0 (TID 1412). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 192.0 in stage 14.0 (TID 1428, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 192.0 in stage 14.0 (TID 1428)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 193.0 in stage 14.0 (TID 1429, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 193.0 in stage 14.0 (TID 1429)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 177.0 in stage 14.0 (TID 1413) in 602 ms on localhost (177/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 176.0 in stage 14.0 (TID 1412) in 605 ms on localhost (178/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 178.0 in stage 14.0 (TID 1414). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 194.0 in stage 14.0 (TID 1430, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 194.0 in stage 14.0 (TID 1430)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 178.0 in stage 14.0 (TID 1414) in 605 ms on localhost (179/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO Executor: Finished task 179.0 in stage 14.0 (TID 1415). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 195.0 in stage 14.0 (TID 1431, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 195.0 in stage 14.0 (TID 1431)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 179.0 in stage 14.0 (TID 1415) in 607 ms on localhost (180/200)
15/08/16 12:51:58 INFO Executor: Finished task 180.0 in stage 14.0 (TID 1416). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 196.0 in stage 14.0 (TID 1432, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Running task 196.0 in stage 14.0 (TID 1432)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Finished task 180.0 in stage 14.0 (TID 1416) in 606 ms on localhost (181/200)
15/08/16 12:51:58 INFO Executor: Finished task 181.0 in stage 14.0 (TID 1417). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Starting task 197.0 in stage 14.0 (TID 1433, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 197.0 in stage 14.0 (TID 1433)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO TaskSetManager: Finished task 181.0 in stage 14.0 (TID 1417) in 604 ms on localhost (182/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 183.0 in stage 14.0 (TID 1419). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 182.0 in stage 14.0 (TID 1418). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Starting task 198.0 in stage 14.0 (TID 1434, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 198.0 in stage 14.0 (TID 1434)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 199.0 in stage 14.0 (TID 1435, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 199.0 in stage 14.0 (TID 1435)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 182.0 in stage 14.0 (TID 1418) in 614 ms on localhost (183/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 183.0 in stage 14.0 (TID 1419) in 603 ms on localhost (184/200)
15/08/16 12:51:58 INFO Executor: Finished task 184.0 in stage 14.0 (TID 1420). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 185.0 in stage 14.0 (TID 1421). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 184.0 in stage 14.0 (TID 1420) in 605 ms on localhost (185/200)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 168 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO Executor: Finished task 186.0 in stage 14.0 (TID 1422). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 185.0 in stage 14.0 (TID 1421) in 605 ms on localhost (186/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 186.0 in stage 14.0 (TID 1422) in 601 ms on localhost (187/200)
15/08/16 12:51:58 INFO Executor: Finished task 187.0 in stage 14.0 (TID 1423). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 187.0 in stage 14.0 (TID 1423) in 594 ms on localhost (188/200)
15/08/16 12:51:58 INFO Executor: Finished task 188.0 in stage 14.0 (TID 1424). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 189.0 in stage 14.0 (TID 1425). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 190.0 in stage 14.0 (TID 1426). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 188.0 in stage 14.0 (TID 1424) in 594 ms on localhost (189/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 189.0 in stage 14.0 (TID 1425) in 593 ms on localhost (190/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 190.0 in stage 14.0 (TID 1426) in 594 ms on localhost (191/200)
15/08/16 12:51:58 INFO Executor: Finished task 191.0 in stage 14.0 (TID 1427). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 191.0 in stage 14.0 (TID 1427) in 322 ms on localhost (192/200)
15/08/16 12:51:58 INFO Executor: Finished task 194.0 in stage 14.0 (TID 1430). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 192.0 in stage 14.0 (TID 1428). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 195.0 in stage 14.0 (TID 1431). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 193.0 in stage 14.0 (TID 1429). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 197.0 in stage 14.0 (TID 1433). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 196.0 in stage 14.0 (TID 1432). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 194.0 in stage 14.0 (TID 1430) in 243 ms on localhost (193/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 195.0 in stage 14.0 (TID 1431) in 239 ms on localhost (194/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 193.0 in stage 14.0 (TID 1429) in 250 ms on localhost (195/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 196.0 in stage 14.0 (TID 1432) in 238 ms on localhost (196/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 197.0 in stage 14.0 (TID 1433) in 235 ms on localhost (197/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 192.0 in stage 14.0 (TID 1428) in 252 ms on localhost (198/200)
15/08/16 12:51:58 INFO Executor: Finished task 199.0 in stage 14.0 (TID 1435). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO Executor: Finished task 198.0 in stage 14.0 (TID 1434). 1219 bytes result sent to driver
15/08/16 12:51:58 INFO TaskSetManager: Finished task 199.0 in stage 14.0 (TID 1435) in 183 ms on localhost (199/200)
15/08/16 12:51:58 INFO TaskSetManager: Finished task 198.0 in stage 14.0 (TID 1434) in 183 ms on localhost (200/200)
15/08/16 12:51:58 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/08/16 12:51:58 INFO DAGScheduler: ShuffleMapStage 14 (processCmd at CliDriver.java:423) finished in 6.026 s
15/08/16 12:51:58 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:51:58 INFO DAGScheduler: running: Set()
15/08/16 12:51:58 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
15/08/16 12:51:58 INFO DAGScheduler: failed: Set()
15/08/16 12:51:58 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@65fd9ddb
15/08/16 12:51:58 INFO StatsReportListener: task runtime:(count: 200, mean: 477.650000, stdev: 105.552676, max: 662.000000, min: 134.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	134.0 ms	322.0 ms	380.0 ms	422.0 ms	463.0 ms	585.0 ms	624.0 ms	638.0 ms	662.0 ms
15/08/16 12:51:58 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 8645.880000, stdev: 303.175635, max: 9374.000000, min: 7801.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	7.6 KB	7.9 KB	8.1 KB	8.2 KB	8.4 KB	8.7 KB	8.8 KB	8.9 KB	9.2 KB
15/08/16 12:51:58 INFO DAGScheduler: Missing parents for ShuffleMapStage 15: List()
15/08/16 12:51:58 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.410000, stdev: 0.686950, max: 3.000000, min: 0.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	3.0 ms
15/08/16 12:51:58 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:51:58 INFO DAGScheduler: Missing parents for ResultStage 16: List(ShuffleMapStage 15)
15/08/16 12:51:58 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/16 12:51:58 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[74] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:51:58 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.703714, stdev: 2.037798, max: 99.030695, min: 72.408537)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	72 %	96 %	97 %	97 %	98 %	98 %	99 %	99 %	99 %
15/08/16 12:51:58 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.084795, stdev: 0.145663, max: 0.828729, min: 0.000000)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/16 12:51:58 INFO MemoryStore: ensureFreeSpace(18336) called with curMem=508700870, maxMem=3333968363
15/08/16 12:51:58 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 17.9 KB, free 2.6 GB)
15/08/16 12:51:58 INFO StatsReportListener: other time pct: (count: 200, mean: 2.211491, stdev: 2.021259, max: 27.439024, min: 0.807754)
15/08/16 12:51:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:51:58 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 3 %	 3 %	27 %
15/08/16 12:51:58 INFO MemoryStore: ensureFreeSpace(8328) called with curMem=508719206, maxMem=3333968363
15/08/16 12:51:58 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KB, free 2.6 GB)
15/08/16 12:51:58 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:36543 (size: 8.1 KB, free: 3.1 GB)
15/08/16 12:51:58 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:874
15/08/16 12:51:58 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[74] at processCmd at CliDriver.java:423)
15/08/16 12:51:58 INFO TaskSchedulerImpl: Adding task set 15.0 with 200 tasks
15/08/16 12:51:58 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 1436, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 1437, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 1438, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 1439, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 1440, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 1441, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 6.0 in stage 15.0 (TID 1442, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 7.0 in stage 15.0 (TID 1443, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 8.0 in stage 15.0 (TID 1444, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 9.0 in stage 15.0 (TID 1445, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 10.0 in stage 15.0 (TID 1446, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 11.0 in stage 15.0 (TID 1447, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 12.0 in stage 15.0 (TID 1448, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 13.0 in stage 15.0 (TID 1449, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 14.0 in stage 15.0 (TID 1450, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO TaskSetManager: Starting task 15.0 in stage 15.0 (TID 1451, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:58 INFO Executor: Running task 0.0 in stage 15.0 (TID 1436)
15/08/16 12:51:58 INFO Executor: Running task 6.0 in stage 15.0 (TID 1442)
15/08/16 12:51:58 INFO Executor: Running task 11.0 in stage 15.0 (TID 1447)
15/08/16 12:51:58 INFO Executor: Running task 10.0 in stage 15.0 (TID 1446)
15/08/16 12:51:58 INFO Executor: Running task 7.0 in stage 15.0 (TID 1443)
15/08/16 12:51:58 INFO Executor: Running task 5.0 in stage 15.0 (TID 1441)
15/08/16 12:51:58 INFO Executor: Running task 1.0 in stage 15.0 (TID 1437)
15/08/16 12:51:58 INFO Executor: Running task 8.0 in stage 15.0 (TID 1444)
15/08/16 12:51:58 INFO Executor: Running task 4.0 in stage 15.0 (TID 1440)
15/08/16 12:51:58 INFO Executor: Running task 12.0 in stage 15.0 (TID 1448)
15/08/16 12:51:58 INFO Executor: Running task 3.0 in stage 15.0 (TID 1439)
15/08/16 12:51:58 INFO Executor: Running task 2.0 in stage 15.0 (TID 1438)
15/08/16 12:51:58 INFO Executor: Running task 9.0 in stage 15.0 (TID 1445)
15/08/16 12:51:58 INFO Executor: Running task 13.0 in stage 15.0 (TID 1449)
15/08/16 12:51:58 INFO Executor: Running task 14.0 in stage 15.0 (TID 1450)
15/08/16 12:51:58 INFO Executor: Running task 15.0 in stage 15.0 (TID 1451)
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:51:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 8.0 in stage 15.0 (TID 1444). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 16.0 in stage 15.0 (TID 1452, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 16.0 in stage 15.0 (TID 1452)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 8.0 in stage 15.0 (TID 1444) in 202 ms on localhost (1/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 14.0 in stage 15.0 (TID 1450). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 17.0 in stage 15.0 (TID 1453, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 17.0 in stage 15.0 (TID 1453)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 14.0 in stage 15.0 (TID 1450) in 243 ms on localhost (2/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO Executor: Finished task 3.0 in stage 15.0 (TID 1439). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 18.0 in stage 15.0 (TID 1454, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 18.0 in stage 15.0 (TID 1454)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 1439) in 478 ms on localhost (3/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 10.0 in stage 15.0 (TID 1446). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO Executor: Finished task 7.0 in stage 15.0 (TID 1443). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 19.0 in stage 15.0 (TID 1455, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 19.0 in stage 15.0 (TID 1455)
15/08/16 12:51:59 INFO TaskSetManager: Starting task 20.0 in stage 15.0 (TID 1456, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 20.0 in stage 15.0 (TID 1456)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 10.0 in stage 15.0 (TID 1446) in 517 ms on localhost (4/200)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 7.0 in stage 15.0 (TID 1443) in 518 ms on localhost (5/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO Executor: Finished task 4.0 in stage 15.0 (TID 1440). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Starting task 21.0 in stage 15.0 (TID 1457, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 21.0 in stage 15.0 (TID 1457)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 1440) in 524 ms on localhost (6/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1436). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO TaskSetManager: Starting task 22.0 in stage 15.0 (TID 1458, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 22.0 in stage 15.0 (TID 1458)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1436) in 534 ms on localhost (7/200)
15/08/16 12:51:59 INFO Executor: Finished task 5.0 in stage 15.0 (TID 1441). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 23.0 in stage 15.0 (TID 1459, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 23.0 in stage 15.0 (TID 1459)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 1441) in 534 ms on localhost (8/200)
15/08/16 12:51:59 INFO Executor: Finished task 11.0 in stage 15.0 (TID 1447). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 24.0 in stage 15.0 (TID 1460, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 24.0 in stage 15.0 (TID 1460)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 11.0 in stage 15.0 (TID 1447) in 536 ms on localhost (9/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 15.0 in stage 15.0 (TID 1451). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 25.0 in stage 15.0 (TID 1461, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 25.0 in stage 15.0 (TID 1461)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 15.0 in stage 15.0 (TID 1451) in 558 ms on localhost (10/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO Executor: Finished task 9.0 in stage 15.0 (TID 1445). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 26.0 in stage 15.0 (TID 1462, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Finished task 6.0 in stage 15.0 (TID 1442). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO Executor: Running task 26.0 in stage 15.0 (TID 1462)
15/08/16 12:51:59 INFO TaskSetManager: Starting task 27.0 in stage 15.0 (TID 1463, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Finished task 13.0 in stage 15.0 (TID 1449). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO Executor: Running task 27.0 in stage 15.0 (TID 1463)
15/08/16 12:51:59 INFO Executor: Finished task 12.0 in stage 15.0 (TID 1448). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 28.0 in stage 15.0 (TID 1464, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 6.0 in stage 15.0 (TID 1442) in 568 ms on localhost (11/200)
15/08/16 12:51:59 INFO Executor: Running task 28.0 in stage 15.0 (TID 1464)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 9.0 in stage 15.0 (TID 1445) in 568 ms on localhost (12/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO TaskSetManager: Starting task 29.0 in stage 15.0 (TID 1465, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 13.0 in stage 15.0 (TID 1449) in 568 ms on localhost (13/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 1.0 in stage 15.0 (TID 1437). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO TaskSetManager: Starting task 30.0 in stage 15.0 (TID 1466, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 12.0 in stage 15.0 (TID 1448) in 571 ms on localhost (14/200)
15/08/16 12:51:59 INFO Executor: Finished task 2.0 in stage 15.0 (TID 1438). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO Executor: Running task 30.0 in stage 15.0 (TID 1466)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 1437) in 575 ms on localhost (15/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Starting task 31.0 in stage 15.0 (TID 1467, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO Executor: Running task 31.0 in stage 15.0 (TID 1467)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 1438) in 578 ms on localhost (16/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO Executor: Running task 29.0 in stage 15.0 (TID 1465)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 17.0 in stage 15.0 (TID 1453). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 32.0 in stage 15.0 (TID 1468, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 32.0 in stage 15.0 (TID 1468)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 17.0 in stage 15.0 (TID 1453) in 339 ms on localhost (17/200)
15/08/16 12:51:59 INFO Executor: Finished task 16.0 in stage 15.0 (TID 1452). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO TaskSetManager: Starting task 33.0 in stage 15.0 (TID 1469, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO TaskSetManager: Finished task 16.0 in stage 15.0 (TID 1452) in 384 ms on localhost (18/200)
15/08/16 12:51:59 INFO Executor: Running task 33.0 in stage 15.0 (TID 1469)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 18.0 in stage 15.0 (TID 1454). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 34.0 in stage 15.0 (TID 1470, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 34.0 in stage 15.0 (TID 1470)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 18.0 in stage 15.0 (TID 1454) in 130 ms on localhost (19/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 20.0 in stage 15.0 (TID 1456). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 35.0 in stage 15.0 (TID 1471, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 35.0 in stage 15.0 (TID 1471)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 20.0 in stage 15.0 (TID 1456) in 131 ms on localhost (20/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO Executor: Finished task 19.0 in stage 15.0 (TID 1455). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 36.0 in stage 15.0 (TID 1472, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 36.0 in stage 15.0 (TID 1472)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 19.0 in stage 15.0 (TID 1455) in 177 ms on localhost (21/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:51:59 INFO Executor: Finished task 21.0 in stage 15.0 (TID 1457). 1219 bytes result sent to driver
15/08/16 12:51:59 INFO TaskSetManager: Starting task 37.0 in stage 15.0 (TID 1473, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:51:59 INFO Executor: Running task 37.0 in stage 15.0 (TID 1473)
15/08/16 12:51:59 INFO TaskSetManager: Finished task 21.0 in stage 15.0 (TID 1457) in 349 ms on localhost (22/200)
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:51:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 23.0 in stage 15.0 (TID 1459). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Finished task 24.0 in stage 15.0 (TID 1460). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 38.0 in stage 15.0 (TID 1474, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 38.0 in stage 15.0 (TID 1474)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 39.0 in stage 15.0 (TID 1475, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 39.0 in stage 15.0 (TID 1475)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 23.0 in stage 15.0 (TID 1459) in 524 ms on localhost (23/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 24.0 in stage 15.0 (TID 1460) in 522 ms on localhost (24/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 22.0 in stage 15.0 (TID 1458). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Finished task 26.0 in stage 15.0 (TID 1462). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 40.0 in stage 15.0 (TID 1476, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 40.0 in stage 15.0 (TID 1476)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 41.0 in stage 15.0 (TID 1477, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 41.0 in stage 15.0 (TID 1477)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 22.0 in stage 15.0 (TID 1458) in 559 ms on localhost (25/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 26.0 in stage 15.0 (TID 1462) in 524 ms on localhost (26/200)
15/08/16 12:52:00 INFO Executor: Finished task 31.0 in stage 15.0 (TID 1467). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Starting task 42.0 in stage 15.0 (TID 1478, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 42.0 in stage 15.0 (TID 1478)
15/08/16 12:52:00 INFO Executor: Finished task 25.0 in stage 15.0 (TID 1461). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Finished task 31.0 in stage 15.0 (TID 1467) in 519 ms on localhost (27/200)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 43.0 in stage 15.0 (TID 1479, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO TaskSetManager: Finished task 25.0 in stage 15.0 (TID 1461) in 541 ms on localhost (28/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO Executor: Running task 43.0 in stage 15.0 (TID 1479)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 28.0 in stage 15.0 (TID 1464). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Starting task 44.0 in stage 15.0 (TID 1480, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 44.0 in stage 15.0 (TID 1480)
15/08/16 12:52:00 INFO Executor: Finished task 32.0 in stage 15.0 (TID 1468). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 45.0 in stage 15.0 (TID 1481, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 45.0 in stage 15.0 (TID 1481)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 28.0 in stage 15.0 (TID 1464) in 536 ms on localhost (29/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 32.0 in stage 15.0 (TID 1468) in 525 ms on localhost (30/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO Executor: Finished task 27.0 in stage 15.0 (TID 1463). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 46.0 in stage 15.0 (TID 1482, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 46.0 in stage 15.0 (TID 1482)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Finished task 27.0 in stage 15.0 (TID 1463) in 546 ms on localhost (31/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 34.0 in stage 15.0 (TID 1470). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Starting task 47.0 in stage 15.0 (TID 1483, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 47.0 in stage 15.0 (TID 1483)
15/08/16 12:52:00 INFO Executor: Finished task 30.0 in stage 15.0 (TID 1466). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 48.0 in stage 15.0 (TID 1484, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 48.0 in stage 15.0 (TID 1484)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 34.0 in stage 15.0 (TID 1470) in 520 ms on localhost (32/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 30.0 in stage 15.0 (TID 1466) in 548 ms on localhost (33/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 33.0 in stage 15.0 (TID 1469). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 49.0 in stage 15.0 (TID 1485, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 49.0 in stage 15.0 (TID 1485)
15/08/16 12:52:00 INFO Executor: Finished task 29.0 in stage 15.0 (TID 1465). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Starting task 50.0 in stage 15.0 (TID 1486, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 50.0 in stage 15.0 (TID 1486)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 33.0 in stage 15.0 (TID 1469) in 543 ms on localhost (34/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 29.0 in stage 15.0 (TID 1465) in 558 ms on localhost (35/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 35.0 in stage 15.0 (TID 1471). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 51.0 in stage 15.0 (TID 1487, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 51.0 in stage 15.0 (TID 1487)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Finished task 35.0 in stage 15.0 (TID 1471) in 496 ms on localhost (36/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 36.0 in stage 15.0 (TID 1472). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 52.0 in stage 15.0 (TID 1488, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 52.0 in stage 15.0 (TID 1488)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 36.0 in stage 15.0 (TID 1472) in 458 ms on localhost (37/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 37.0 in stage 15.0 (TID 1473). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 53.0 in stage 15.0 (TID 1489, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 53.0 in stage 15.0 (TID 1489)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 37.0 in stage 15.0 (TID 1473) in 308 ms on localhost (38/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 38.0 in stage 15.0 (TID 1474). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Finished task 39.0 in stage 15.0 (TID 1475). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 54.0 in stage 15.0 (TID 1490, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 54.0 in stage 15.0 (TID 1490)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 55.0 in stage 15.0 (TID 1491, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 55.0 in stage 15.0 (TID 1491)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 38.0 in stage 15.0 (TID 1474) in 143 ms on localhost (39/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 39.0 in stage 15.0 (TID 1475) in 142 ms on localhost (40/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 43.0 in stage 15.0 (TID 1479). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 56.0 in stage 15.0 (TID 1492, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 56.0 in stage 15.0 (TID 1492)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 43.0 in stage 15.0 (TID 1479) in 652 ms on localhost (41/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 40.0 in stage 15.0 (TID 1476). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 57.0 in stage 15.0 (TID 1493, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 57.0 in stage 15.0 (TID 1493)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 40.0 in stage 15.0 (TID 1476) in 695 ms on localhost (42/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 41.0 in stage 15.0 (TID 1477). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 58.0 in stage 15.0 (TID 1494, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 58.0 in stage 15.0 (TID 1494)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 41.0 in stage 15.0 (TID 1477) in 706 ms on localhost (43/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 42.0 in stage 15.0 (TID 1478). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 59.0 in stage 15.0 (TID 1495, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 59.0 in stage 15.0 (TID 1495)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 42.0 in stage 15.0 (TID 1478) in 710 ms on localhost (44/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 45.0 in stage 15.0 (TID 1481). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 60.0 in stage 15.0 (TID 1496, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 60.0 in stage 15.0 (TID 1496)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 45.0 in stage 15.0 (TID 1481) in 731 ms on localhost (45/200)
15/08/16 12:52:00 INFO Executor: Finished task 44.0 in stage 15.0 (TID 1480). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 61.0 in stage 15.0 (TID 1497, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 61.0 in stage 15.0 (TID 1497)
15/08/16 12:52:00 INFO Executor: Finished task 47.0 in stage 15.0 (TID 1483). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 62.0 in stage 15.0 (TID 1498, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 62.0 in stage 15.0 (TID 1498)
15/08/16 12:52:00 INFO Executor: Finished task 46.0 in stage 15.0 (TID 1482). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 63.0 in stage 15.0 (TID 1499, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 63.0 in stage 15.0 (TID 1499)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 47.0 in stage 15.0 (TID 1483) in 723 ms on localhost (46/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 44.0 in stage 15.0 (TID 1480) in 738 ms on localhost (47/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Finished task 46.0 in stage 15.0 (TID 1482) in 731 ms on localhost (48/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO Executor: Finished task 48.0 in stage 15.0 (TID 1484). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 64.0 in stage 15.0 (TID 1500, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 64.0 in stage 15.0 (TID 1500)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 48.0 in stage 15.0 (TID 1484) in 738 ms on localhost (49/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 49.0 in stage 15.0 (TID 1485). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 65.0 in stage 15.0 (TID 1501, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Finished task 51.0 in stage 15.0 (TID 1487). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Running task 65.0 in stage 15.0 (TID 1501)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 66.0 in stage 15.0 (TID 1502, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 49.0 in stage 15.0 (TID 1485) in 754 ms on localhost (50/200)
15/08/16 12:52:00 INFO Executor: Running task 66.0 in stage 15.0 (TID 1502)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 51.0 in stage 15.0 (TID 1487) in 741 ms on localhost (51/200)
15/08/16 12:52:00 INFO Executor: Finished task 50.0 in stage 15.0 (TID 1486). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO TaskSetManager: Starting task 67.0 in stage 15.0 (TID 1503, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Finished task 52.0 in stage 15.0 (TID 1488). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Running task 67.0 in stage 15.0 (TID 1503)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 68.0 in stage 15.0 (TID 1504, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Running task 68.0 in stage 15.0 (TID 1504)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO TaskSetManager: Finished task 52.0 in stage 15.0 (TID 1488) in 743 ms on localhost (52/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 50.0 in stage 15.0 (TID 1486) in 765 ms on localhost (53/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 53.0 in stage 15.0 (TID 1489). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO Executor: Finished task 55.0 in stage 15.0 (TID 1491). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 69.0 in stage 15.0 (TID 1505, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 69.0 in stage 15.0 (TID 1505)
15/08/16 12:52:00 INFO Executor: Finished task 54.0 in stage 15.0 (TID 1490). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 70.0 in stage 15.0 (TID 1506, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 70.0 in stage 15.0 (TID 1506)
15/08/16 12:52:00 INFO TaskSetManager: Starting task 71.0 in stage 15.0 (TID 1507, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 53.0 in stage 15.0 (TID 1489) in 726 ms on localhost (54/200)
15/08/16 12:52:00 INFO Executor: Running task 71.0 in stage 15.0 (TID 1507)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 54.0 in stage 15.0 (TID 1490) in 706 ms on localhost (55/200)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 55.0 in stage 15.0 (TID 1491) in 706 ms on localhost (56/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 56.0 in stage 15.0 (TID 1492). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 72.0 in stage 15.0 (TID 1508, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 72.0 in stage 15.0 (TID 1508)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 56.0 in stage 15.0 (TID 1492) in 230 ms on localhost (57/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO Executor: Finished task 57.0 in stage 15.0 (TID 1493). 1219 bytes result sent to driver
15/08/16 12:52:00 INFO TaskSetManager: Starting task 73.0 in stage 15.0 (TID 1509, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:00 INFO Executor: Running task 73.0 in stage 15.0 (TID 1509)
15/08/16 12:52:00 INFO TaskSetManager: Finished task 57.0 in stage 15.0 (TID 1493) in 242 ms on localhost (58/200)
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 58.0 in stage 15.0 (TID 1494). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 74.0 in stage 15.0 (TID 1510, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 74.0 in stage 15.0 (TID 1510)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 58.0 in stage 15.0 (TID 1494) in 346 ms on localhost (59/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 59.0 in stage 15.0 (TID 1495). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO TaskSetManager: Starting task 75.0 in stage 15.0 (TID 1511, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 75.0 in stage 15.0 (TID 1511)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 59.0 in stage 15.0 (TID 1495) in 347 ms on localhost (60/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 60.0 in stage 15.0 (TID 1496). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 76.0 in stage 15.0 (TID 1512, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 76.0 in stage 15.0 (TID 1512)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 60.0 in stage 15.0 (TID 1496) in 428 ms on localhost (61/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 63.0 in stage 15.0 (TID 1499). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 77.0 in stage 15.0 (TID 1513, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 77.0 in stage 15.0 (TID 1513)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 63.0 in stage 15.0 (TID 1499) in 432 ms on localhost (62/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO Executor: Finished task 61.0 in stage 15.0 (TID 1497). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 78.0 in stage 15.0 (TID 1514, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 78.0 in stage 15.0 (TID 1514)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 61.0 in stage 15.0 (TID 1497) in 439 ms on localhost (63/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 62.0 in stage 15.0 (TID 1498). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 79.0 in stage 15.0 (TID 1515, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 79.0 in stage 15.0 (TID 1515)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 62.0 in stage 15.0 (TID 1498) in 448 ms on localhost (64/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 66.0 in stage 15.0 (TID 1502). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 80.0 in stage 15.0 (TID 1516, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 80.0 in stage 15.0 (TID 1516)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 66.0 in stage 15.0 (TID 1502) in 429 ms on localhost (65/200)
15/08/16 12:52:01 INFO Executor: Finished task 68.0 in stage 15.0 (TID 1504). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO TaskSetManager: Starting task 81.0 in stage 15.0 (TID 1517, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Running task 81.0 in stage 15.0 (TID 1517)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 68.0 in stage 15.0 (TID 1504) in 432 ms on localhost (66/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO Executor: Finished task 65.0 in stage 15.0 (TID 1501). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO TaskSetManager: Starting task 82.0 in stage 15.0 (TID 1518, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 82.0 in stage 15.0 (TID 1518)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 65.0 in stage 15.0 (TID 1501) in 448 ms on localhost (67/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 67.0 in stage 15.0 (TID 1503). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 83.0 in stage 15.0 (TID 1519, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 83.0 in stage 15.0 (TID 1519)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 67.0 in stage 15.0 (TID 1503) in 447 ms on localhost (68/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 64.0 in stage 15.0 (TID 1500). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 84.0 in stage 15.0 (TID 1520, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 84.0 in stage 15.0 (TID 1520)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 69.0 in stage 15.0 (TID 1505). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Finished task 64.0 in stage 15.0 (TID 1500) in 483 ms on localhost (69/200)
15/08/16 12:52:01 INFO TaskSetManager: Starting task 85.0 in stage 15.0 (TID 1521, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Finished task 70.0 in stage 15.0 (TID 1506). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO Executor: Running task 85.0 in stage 15.0 (TID 1521)
15/08/16 12:52:01 INFO TaskSetManager: Starting task 86.0 in stage 15.0 (TID 1522, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 86.0 in stage 15.0 (TID 1522)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 69.0 in stage 15.0 (TID 1505) in 444 ms on localhost (70/200)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 70.0 in stage 15.0 (TID 1506) in 445 ms on localhost (71/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 71.0 in stage 15.0 (TID 1507). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 87.0 in stage 15.0 (TID 1523, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 87.0 in stage 15.0 (TID 1523)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 71.0 in stage 15.0 (TID 1507) in 459 ms on localhost (72/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 72.0 in stage 15.0 (TID 1508). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 88.0 in stage 15.0 (TID 1524, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 88.0 in stage 15.0 (TID 1524)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 72.0 in stage 15.0 (TID 1508) in 416 ms on localhost (73/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO Executor: Finished task 73.0 in stage 15.0 (TID 1509). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 89.0 in stage 15.0 (TID 1525, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 89.0 in stage 15.0 (TID 1525)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 73.0 in stage 15.0 (TID 1509) in 387 ms on localhost (74/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 75.0 in stage 15.0 (TID 1511). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 90.0 in stage 15.0 (TID 1526, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 90.0 in stage 15.0 (TID 1526)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 75.0 in stage 15.0 (TID 1511) in 311 ms on localhost (75/200)
15/08/16 12:52:01 INFO Executor: Finished task 74.0 in stage 15.0 (TID 1510). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 91.0 in stage 15.0 (TID 1527, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 91.0 in stage 15.0 (TID 1527)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 74.0 in stage 15.0 (TID 1510) in 326 ms on localhost (76/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 78.0 in stage 15.0 (TID 1514). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 92.0 in stage 15.0 (TID 1528, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 92.0 in stage 15.0 (TID 1528)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 78.0 in stage 15.0 (TID 1514) in 413 ms on localhost (77/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 76.0 in stage 15.0 (TID 1512). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 93.0 in stage 15.0 (TID 1529, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 93.0 in stage 15.0 (TID 1529)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 76.0 in stage 15.0 (TID 1512) in 442 ms on localhost (78/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 77.0 in stage 15.0 (TID 1513). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 94.0 in stage 15.0 (TID 1530, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 94.0 in stage 15.0 (TID 1530)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 77.0 in stage 15.0 (TID 1513) in 449 ms on localhost (79/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 80.0 in stage 15.0 (TID 1516). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 95.0 in stage 15.0 (TID 1531, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 95.0 in stage 15.0 (TID 1531)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 80.0 in stage 15.0 (TID 1516) in 465 ms on localhost (80/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 79.0 in stage 15.0 (TID 1515). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 96.0 in stage 15.0 (TID 1532, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 96.0 in stage 15.0 (TID 1532)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 79.0 in stage 15.0 (TID 1515) in 502 ms on localhost (81/200)
15/08/16 12:52:01 INFO Executor: Finished task 81.0 in stage 15.0 (TID 1517). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 97.0 in stage 15.0 (TID 1533, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 97.0 in stage 15.0 (TID 1533)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO TaskSetManager: Finished task 81.0 in stage 15.0 (TID 1517) in 479 ms on localhost (82/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 82.0 in stage 15.0 (TID 1518). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 98.0 in stage 15.0 (TID 1534, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 98.0 in stage 15.0 (TID 1534)
15/08/16 12:52:01 INFO Executor: Finished task 83.0 in stage 15.0 (TID 1519). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 99.0 in stage 15.0 (TID 1535, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 99.0 in stage 15.0 (TID 1535)
15/08/16 12:52:01 INFO Executor: Finished task 85.0 in stage 15.0 (TID 1521). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Finished task 82.0 in stage 15.0 (TID 1518) in 486 ms on localhost (83/200)
15/08/16 12:52:01 INFO TaskSetManager: Starting task 100.0 in stage 15.0 (TID 1536, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 100.0 in stage 15.0 (TID 1536)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 83.0 in stage 15.0 (TID 1519) in 482 ms on localhost (84/200)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 85.0 in stage 15.0 (TID 1521) in 469 ms on localhost (85/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 87.0 in stage 15.0 (TID 1523). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 101.0 in stage 15.0 (TID 1537, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 101.0 in stage 15.0 (TID 1537)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO Executor: Finished task 84.0 in stage 15.0 (TID 1520). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO TaskSetManager: Finished task 87.0 in stage 15.0 (TID 1523) in 459 ms on localhost (86/200)
15/08/16 12:52:01 INFO TaskSetManager: Starting task 102.0 in stage 15.0 (TID 1538, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 102.0 in stage 15.0 (TID 1538)
15/08/16 12:52:01 INFO Executor: Finished task 86.0 in stage 15.0 (TID 1522). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 103.0 in stage 15.0 (TID 1539, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO TaskSetManager: Finished task 84.0 in stage 15.0 (TID 1520) in 482 ms on localhost (87/200)
15/08/16 12:52:01 INFO Executor: Running task 103.0 in stage 15.0 (TID 1539)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 86.0 in stage 15.0 (TID 1522) in 479 ms on localhost (88/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO Executor: Finished task 88.0 in stage 15.0 (TID 1524). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 104.0 in stage 15.0 (TID 1540, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 104.0 in stage 15.0 (TID 1540)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 88.0 in stage 15.0 (TID 1524) in 472 ms on localhost (89/200)
15/08/16 12:52:01 INFO Executor: Finished task 89.0 in stage 15.0 (TID 1525). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO TaskSetManager: Starting task 105.0 in stage 15.0 (TID 1541, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 105.0 in stage 15.0 (TID 1541)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 89.0 in stage 15.0 (TID 1525) in 458 ms on localhost (90/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO Executor: Finished task 91.0 in stage 15.0 (TID 1527). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 106.0 in stage 15.0 (TID 1542, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 106.0 in stage 15.0 (TID 1542)
15/08/16 12:52:01 INFO Executor: Finished task 90.0 in stage 15.0 (TID 1526). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 107.0 in stage 15.0 (TID 1543, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 107.0 in stage 15.0 (TID 1543)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 91.0 in stage 15.0 (TID 1527) in 421 ms on localhost (91/200)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 90.0 in stage 15.0 (TID 1526) in 426 ms on localhost (92/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:01 INFO Executor: Finished task 92.0 in stage 15.0 (TID 1528). 1219 bytes result sent to driver
15/08/16 12:52:01 INFO TaskSetManager: Starting task 108.0 in stage 15.0 (TID 1544, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:01 INFO Executor: Running task 108.0 in stage 15.0 (TID 1544)
15/08/16 12:52:01 INFO TaskSetManager: Finished task 92.0 in stage 15.0 (TID 1528) in 340 ms on localhost (93/200)
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 94.0 in stage 15.0 (TID 1530). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 109.0 in stage 15.0 (TID 1545, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 109.0 in stage 15.0 (TID 1545)
15/08/16 12:52:02 INFO Executor: Finished task 93.0 in stage 15.0 (TID 1529). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 110.0 in stage 15.0 (TID 1546, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 110.0 in stage 15.0 (TID 1546)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 94.0 in stage 15.0 (TID 1530) in 342 ms on localhost (94/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 93.0 in stage 15.0 (TID 1529) in 360 ms on localhost (95/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 95.0 in stage 15.0 (TID 1531). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 111.0 in stage 15.0 (TID 1547, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 111.0 in stage 15.0 (TID 1547)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 95.0 in stage 15.0 (TID 1531) in 356 ms on localhost (96/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 97.0 in stage 15.0 (TID 1533). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO Executor: Finished task 96.0 in stage 15.0 (TID 1532). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 112.0 in stage 15.0 (TID 1548, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 112.0 in stage 15.0 (TID 1548)
15/08/16 12:52:02 INFO TaskSetManager: Starting task 113.0 in stage 15.0 (TID 1549, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 113.0 in stage 15.0 (TID 1549)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 97.0 in stage 15.0 (TID 1533) in 356 ms on localhost (97/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 96.0 in stage 15.0 (TID 1532) in 361 ms on localhost (98/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 100.0 in stage 15.0 (TID 1536). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO Executor: Finished task 99.0 in stage 15.0 (TID 1535). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 114.0 in stage 15.0 (TID 1550, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 114.0 in stage 15.0 (TID 1550)
15/08/16 12:52:02 INFO TaskSetManager: Starting task 115.0 in stage 15.0 (TID 1551, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 115.0 in stage 15.0 (TID 1551)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 100.0 in stage 15.0 (TID 1536) in 401 ms on localhost (99/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 99.0 in stage 15.0 (TID 1535) in 403 ms on localhost (100/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 101.0 in stage 15.0 (TID 1537). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO TaskSetManager: Starting task 116.0 in stage 15.0 (TID 1552, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 116.0 in stage 15.0 (TID 1552)
15/08/16 12:52:02 INFO Executor: Finished task 102.0 in stage 15.0 (TID 1538). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO TaskSetManager: Starting task 117.0 in stage 15.0 (TID 1553, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 117.0 in stage 15.0 (TID 1553)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 101.0 in stage 15.0 (TID 1537) in 405 ms on localhost (101/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 102.0 in stage 15.0 (TID 1538) in 404 ms on localhost (102/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 98.0 in stage 15.0 (TID 1534). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 118.0 in stage 15.0 (TID 1554, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO Executor: Running task 118.0 in stage 15.0 (TID 1554)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO TaskSetManager: Finished task 98.0 in stage 15.0 (TID 1534) in 423 ms on localhost (103/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 103.0 in stage 15.0 (TID 1539). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 119.0 in stage 15.0 (TID 1555, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 103.0 in stage 15.0 (TID 1539) in 687 ms on localhost (104/200)
15/08/16 12:52:02 INFO Executor: Running task 119.0 in stage 15.0 (TID 1555)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 104.0 in stage 15.0 (TID 1540). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO TaskSetManager: Starting task 120.0 in stage 15.0 (TID 1556, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 120.0 in stage 15.0 (TID 1556)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 104.0 in stage 15.0 (TID 1540) in 666 ms on localhost (105/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 106.0 in stage 15.0 (TID 1542). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 121.0 in stage 15.0 (TID 1557, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 121.0 in stage 15.0 (TID 1557)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 106.0 in stage 15.0 (TID 1542) in 652 ms on localhost (106/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 105.0 in stage 15.0 (TID 1541). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 122.0 in stage 15.0 (TID 1558, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 122.0 in stage 15.0 (TID 1558)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 105.0 in stage 15.0 (TID 1541) in 702 ms on localhost (107/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 107.0 in stage 15.0 (TID 1543). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 123.0 in stage 15.0 (TID 1559, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 123.0 in stage 15.0 (TID 1559)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 107.0 in stage 15.0 (TID 1543) in 696 ms on localhost (108/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 108.0 in stage 15.0 (TID 1544). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 124.0 in stage 15.0 (TID 1560, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 124.0 in stage 15.0 (TID 1560)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 108.0 in stage 15.0 (TID 1544) in 625 ms on localhost (109/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO Executor: Finished task 109.0 in stage 15.0 (TID 1545). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 125.0 in stage 15.0 (TID 1561, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 125.0 in stage 15.0 (TID 1561)
15/08/16 12:52:02 INFO Executor: Finished task 110.0 in stage 15.0 (TID 1546). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 126.0 in stage 15.0 (TID 1562, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 126.0 in stage 15.0 (TID 1562)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 109.0 in stage 15.0 (TID 1545) in 640 ms on localhost (110/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 110.0 in stage 15.0 (TID 1546) in 636 ms on localhost (111/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 111.0 in stage 15.0 (TID 1547). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 127.0 in stage 15.0 (TID 1563, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 127.0 in stage 15.0 (TID 1563)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 111.0 in stage 15.0 (TID 1547) in 641 ms on localhost (112/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 112.0 in stage 15.0 (TID 1548). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 128.0 in stage 15.0 (TID 1564, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 128.0 in stage 15.0 (TID 1564)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 112.0 in stage 15.0 (TID 1548) in 644 ms on localhost (113/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 113.0 in stage 15.0 (TID 1549). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 129.0 in stage 15.0 (TID 1565, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 129.0 in stage 15.0 (TID 1565)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 113.0 in stage 15.0 (TID 1549) in 696 ms on localhost (114/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:02 INFO Executor: Finished task 115.0 in stage 15.0 (TID 1551). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO Executor: Finished task 114.0 in stage 15.0 (TID 1550). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 130.0 in stage 15.0 (TID 1566, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 130.0 in stage 15.0 (TID 1566)
15/08/16 12:52:02 INFO TaskSetManager: Starting task 131.0 in stage 15.0 (TID 1567, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 115.0 in stage 15.0 (TID 1551) in 678 ms on localhost (115/200)
15/08/16 12:52:02 INFO Executor: Running task 131.0 in stage 15.0 (TID 1567)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 114.0 in stage 15.0 (TID 1550) in 680 ms on localhost (116/200)
15/08/16 12:52:02 INFO Executor: Finished task 116.0 in stage 15.0 (TID 1552). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 132.0 in stage 15.0 (TID 1568, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 132.0 in stage 15.0 (TID 1568)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 116.0 in stage 15.0 (TID 1552) in 672 ms on localhost (117/200)
15/08/16 12:52:02 INFO Executor: Finished task 117.0 in stage 15.0 (TID 1553). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 133.0 in stage 15.0 (TID 1569, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 133.0 in stage 15.0 (TID 1569)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO TaskSetManager: Finished task 117.0 in stage 15.0 (TID 1553) in 673 ms on localhost (118/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 120.0 in stage 15.0 (TID 1556). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO Executor: Finished task 119.0 in stage 15.0 (TID 1555). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 134.0 in stage 15.0 (TID 1570, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 134.0 in stage 15.0 (TID 1570)
15/08/16 12:52:02 INFO TaskSetManager: Starting task 135.0 in stage 15.0 (TID 1571, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 135.0 in stage 15.0 (TID 1571)
15/08/16 12:52:02 INFO Executor: Finished task 118.0 in stage 15.0 (TID 1554). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Finished task 120.0 in stage 15.0 (TID 1556) in 413 ms on localhost (119/200)
15/08/16 12:52:02 INFO TaskSetManager: Starting task 136.0 in stage 15.0 (TID 1572, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 136.0 in stage 15.0 (TID 1572)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 119.0 in stage 15.0 (TID 1555) in 427 ms on localhost (120/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 118.0 in stage 15.0 (TID 1554) in 708 ms on localhost (121/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO Executor: Finished task 121.0 in stage 15.0 (TID 1557). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 137.0 in stage 15.0 (TID 1573, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 137.0 in stage 15.0 (TID 1573)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 121.0 in stage 15.0 (TID 1557) in 412 ms on localhost (122/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 122.0 in stage 15.0 (TID 1558). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 138.0 in stage 15.0 (TID 1574, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 138.0 in stage 15.0 (TID 1574)
15/08/16 12:52:02 INFO Executor: Finished task 123.0 in stage 15.0 (TID 1559). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 139.0 in stage 15.0 (TID 1575, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 139.0 in stage 15.0 (TID 1575)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 122.0 in stage 15.0 (TID 1558) in 395 ms on localhost (123/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 123.0 in stage 15.0 (TID 1559) in 382 ms on localhost (124/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 124.0 in stage 15.0 (TID 1560). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 140.0 in stage 15.0 (TID 1576, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 140.0 in stage 15.0 (TID 1576)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 124.0 in stage 15.0 (TID 1560) in 349 ms on localhost (125/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 126.0 in stage 15.0 (TID 1562). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 141.0 in stage 15.0 (TID 1577, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 141.0 in stage 15.0 (TID 1577)
15/08/16 12:52:02 INFO Executor: Finished task 125.0 in stage 15.0 (TID 1561). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 142.0 in stage 15.0 (TID 1578, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 142.0 in stage 15.0 (TID 1578)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 126.0 in stage 15.0 (TID 1562) in 319 ms on localhost (126/200)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 125.0 in stage 15.0 (TID 1561) in 324 ms on localhost (127/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO Executor: Finished task 127.0 in stage 15.0 (TID 1563). 1219 bytes result sent to driver
15/08/16 12:52:02 INFO TaskSetManager: Starting task 143.0 in stage 15.0 (TID 1579, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:02 INFO Executor: Running task 143.0 in stage 15.0 (TID 1579)
15/08/16 12:52:02 INFO TaskSetManager: Finished task 127.0 in stage 15.0 (TID 1563) in 275 ms on localhost (128/200)
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 128.0 in stage 15.0 (TID 1564). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 144.0 in stage 15.0 (TID 1580, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 144.0 in stage 15.0 (TID 1580)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 128.0 in stage 15.0 (TID 1564) in 285 ms on localhost (129/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 129.0 in stage 15.0 (TID 1565). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 145.0 in stage 15.0 (TID 1581, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 145.0 in stage 15.0 (TID 1581)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 129.0 in stage 15.0 (TID 1565) in 279 ms on localhost (130/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 133.0 in stage 15.0 (TID 1569). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 130.0 in stage 15.0 (TID 1566). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 146.0 in stage 15.0 (TID 1582, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 146.0 in stage 15.0 (TID 1582)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 147.0 in stage 15.0 (TID 1583, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 147.0 in stage 15.0 (TID 1583)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 133.0 in stage 15.0 (TID 1569) in 310 ms on localhost (131/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 130.0 in stage 15.0 (TID 1566) in 316 ms on localhost (132/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 132.0 in stage 15.0 (TID 1568). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 148.0 in stage 15.0 (TID 1584, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 148.0 in stage 15.0 (TID 1584)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 132.0 in stage 15.0 (TID 1568) in 429 ms on localhost (133/200)
15/08/16 12:52:03 INFO Executor: Finished task 134.0 in stage 15.0 (TID 1570). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 135.0 in stage 15.0 (TID 1571). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 149.0 in stage 15.0 (TID 1585, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO Executor: Finished task 131.0 in stage 15.0 (TID 1567). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Running task 149.0 in stage 15.0 (TID 1585)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO TaskSetManager: Starting task 150.0 in stage 15.0 (TID 1586, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 150.0 in stage 15.0 (TID 1586)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 151.0 in stage 15.0 (TID 1587, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 151.0 in stage 15.0 (TID 1587)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 134.0 in stage 15.0 (TID 1570) in 393 ms on localhost (134/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 135.0 in stage 15.0 (TID 1571) in 392 ms on localhost (135/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 131.0 in stage 15.0 (TID 1567) in 437 ms on localhost (136/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 136.0 in stage 15.0 (TID 1572). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 152.0 in stage 15.0 (TID 1588, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 137.0 in stage 15.0 (TID 1573). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Running task 152.0 in stage 15.0 (TID 1588)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Starting task 153.0 in stage 15.0 (TID 1589, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 153.0 in stage 15.0 (TID 1589)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO TaskSetManager: Finished task 136.0 in stage 15.0 (TID 1572) in 394 ms on localhost (137/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 137.0 in stage 15.0 (TID 1573) in 387 ms on localhost (138/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 139.0 in stage 15.0 (TID 1575). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 138.0 in stage 15.0 (TID 1574). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 154.0 in stage 15.0 (TID 1590, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 154.0 in stage 15.0 (TID 1590)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 155.0 in stage 15.0 (TID 1591, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 155.0 in stage 15.0 (TID 1591)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 139.0 in stage 15.0 (TID 1575) in 403 ms on localhost (139/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 138.0 in stage 15.0 (TID 1574) in 406 ms on localhost (140/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 140.0 in stage 15.0 (TID 1576). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 156.0 in stage 15.0 (TID 1592, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 156.0 in stage 15.0 (TID 1592)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 140.0 in stage 15.0 (TID 1576) in 402 ms on localhost (141/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 142.0 in stage 15.0 (TID 1578). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 157.0 in stage 15.0 (TID 1593, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 157.0 in stage 15.0 (TID 1593)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 142.0 in stage 15.0 (TID 1578) in 392 ms on localhost (142/200)
15/08/16 12:52:03 INFO Executor: Finished task 141.0 in stage 15.0 (TID 1577). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 158.0 in stage 15.0 (TID 1594, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 158.0 in stage 15.0 (TID 1594)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 141.0 in stage 15.0 (TID 1577) in 406 ms on localhost (143/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 143.0 in stage 15.0 (TID 1579). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 159.0 in stage 15.0 (TID 1595, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 159.0 in stage 15.0 (TID 1595)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 143.0 in stage 15.0 (TID 1579) in 409 ms on localhost (144/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 144.0 in stage 15.0 (TID 1580). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 160.0 in stage 15.0 (TID 1596, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 160.0 in stage 15.0 (TID 1596)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 144.0 in stage 15.0 (TID 1580) in 381 ms on localhost (145/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 145.0 in stage 15.0 (TID 1581). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 161.0 in stage 15.0 (TID 1597, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 161.0 in stage 15.0 (TID 1597)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 145.0 in stage 15.0 (TID 1581) in 377 ms on localhost (146/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 146.0 in stage 15.0 (TID 1582). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 147.0 in stage 15.0 (TID 1583). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 162.0 in stage 15.0 (TID 1598, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 162.0 in stage 15.0 (TID 1598)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 163.0 in stage 15.0 (TID 1599, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 163.0 in stage 15.0 (TID 1599)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 146.0 in stage 15.0 (TID 1582) in 381 ms on localhost (147/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 147.0 in stage 15.0 (TID 1583) in 381 ms on localhost (148/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 148.0 in stage 15.0 (TID 1584). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 164.0 in stage 15.0 (TID 1600, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 164.0 in stage 15.0 (TID 1600)
15/08/16 12:52:03 INFO Executor: Finished task 149.0 in stage 15.0 (TID 1585). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 165.0 in stage 15.0 (TID 1601, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 165.0 in stage 15.0 (TID 1601)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 148.0 in stage 15.0 (TID 1584) in 380 ms on localhost (149/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 149.0 in stage 15.0 (TID 1585) in 376 ms on localhost (150/200)
15/08/16 12:52:03 INFO Executor: Finished task 153.0 in stage 15.0 (TID 1589). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 166.0 in stage 15.0 (TID 1602, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 166.0 in stage 15.0 (TID 1602)
15/08/16 12:52:03 INFO Executor: Finished task 152.0 in stage 15.0 (TID 1588). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Starting task 167.0 in stage 15.0 (TID 1603, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 167.0 in stage 15.0 (TID 1603)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 153.0 in stage 15.0 (TID 1589) in 377 ms on localhost (151/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 152.0 in stage 15.0 (TID 1588) in 380 ms on localhost (152/200)
15/08/16 12:52:03 INFO Executor: Finished task 150.0 in stage 15.0 (TID 1586). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Starting task 168.0 in stage 15.0 (TID 1604, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 150.0 in stage 15.0 (TID 1586) in 386 ms on localhost (153/200)
15/08/16 12:52:03 INFO Executor: Running task 168.0 in stage 15.0 (TID 1604)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 151.0 in stage 15.0 (TID 1587). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO TaskSetManager: Starting task 169.0 in stage 15.0 (TID 1605, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Running task 169.0 in stage 15.0 (TID 1605)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 151.0 in stage 15.0 (TID 1587) in 390 ms on localhost (154/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 155.0 in stage 15.0 (TID 1591). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 170.0 in stage 15.0 (TID 1606, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 170.0 in stage 15.0 (TID 1606)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 155.0 in stage 15.0 (TID 1591) in 381 ms on localhost (155/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 154.0 in stage 15.0 (TID 1590). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Starting task 171.0 in stage 15.0 (TID 1607, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 171.0 in stage 15.0 (TID 1607)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 154.0 in stage 15.0 (TID 1590) in 394 ms on localhost (156/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 156.0 in stage 15.0 (TID 1592). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 172.0 in stage 15.0 (TID 1608, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 172.0 in stage 15.0 (TID 1608)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 156.0 in stage 15.0 (TID 1592) in 383 ms on localhost (157/200)
15/08/16 12:52:03 INFO Executor: Finished task 158.0 in stage 15.0 (TID 1594). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 173.0 in stage 15.0 (TID 1609, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 173.0 in stage 15.0 (TID 1609)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 158.0 in stage 15.0 (TID 1594) in 373 ms on localhost (158/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 157.0 in stage 15.0 (TID 1593). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 174.0 in stage 15.0 (TID 1610, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 174.0 in stage 15.0 (TID 1610)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 157.0 in stage 15.0 (TID 1593) in 408 ms on localhost (159/200)
15/08/16 12:52:03 INFO Executor: Finished task 160.0 in stage 15.0 (TID 1596). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 175.0 in stage 15.0 (TID 1611, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 175.0 in stage 15.0 (TID 1611)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO TaskSetManager: Finished task 160.0 in stage 15.0 (TID 1596) in 371 ms on localhost (160/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 159.0 in stage 15.0 (TID 1595). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 176.0 in stage 15.0 (TID 1612, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 176.0 in stage 15.0 (TID 1612)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 159.0 in stage 15.0 (TID 1595) in 390 ms on localhost (161/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO Executor: Finished task 161.0 in stage 15.0 (TID 1597). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 177.0 in stage 15.0 (TID 1613, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 177.0 in stage 15.0 (TID 1613)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 161.0 in stage 15.0 (TID 1597) in 360 ms on localhost (162/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 163.0 in stage 15.0 (TID 1599). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 162.0 in stage 15.0 (TID 1598). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 178.0 in stage 15.0 (TID 1614, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 178.0 in stage 15.0 (TID 1614)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 179.0 in stage 15.0 (TID 1615, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 179.0 in stage 15.0 (TID 1615)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 163.0 in stage 15.0 (TID 1599) in 341 ms on localhost (163/200)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 162.0 in stage 15.0 (TID 1598) in 342 ms on localhost (164/200)
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:03 INFO Executor: Finished task 165.0 in stage 15.0 (TID 1601). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO Executor: Finished task 166.0 in stage 15.0 (TID 1602). 1219 bytes result sent to driver
15/08/16 12:52:03 INFO TaskSetManager: Starting task 180.0 in stage 15.0 (TID 1616, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 180.0 in stage 15.0 (TID 1616)
15/08/16 12:52:03 INFO TaskSetManager: Starting task 181.0 in stage 15.0 (TID 1617, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:03 INFO Executor: Running task 181.0 in stage 15.0 (TID 1617)
15/08/16 12:52:03 INFO TaskSetManager: Finished task 165.0 in stage 15.0 (TID 1601) in 346 ms on localhost (165/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 166.0 in stage 15.0 (TID 1602) in 352 ms on localhost (166/200)
15/08/16 12:52:04 INFO Executor: Finished task 164.0 in stage 15.0 (TID 1600). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 182.0 in stage 15.0 (TID 1618, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 182.0 in stage 15.0 (TID 1618)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 164.0 in stage 15.0 (TID 1600) in 363 ms on localhost (167/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 168.0 in stage 15.0 (TID 1604). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 183.0 in stage 15.0 (TID 1619, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 183.0 in stage 15.0 (TID 1619)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 168.0 in stage 15.0 (TID 1604) in 582 ms on localhost (168/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 169.0 in stage 15.0 (TID 1605). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 184.0 in stage 15.0 (TID 1620, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 184.0 in stage 15.0 (TID 1620)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 169.0 in stage 15.0 (TID 1605) in 593 ms on localhost (169/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 170.0 in stage 15.0 (TID 1606). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 185.0 in stage 15.0 (TID 1621, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 185.0 in stage 15.0 (TID 1621)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 170.0 in stage 15.0 (TID 1606) in 581 ms on localhost (170/200)
15/08/16 12:52:04 INFO Executor: Finished task 167.0 in stage 15.0 (TID 1603). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 186.0 in stage 15.0 (TID 1622, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 186.0 in stage 15.0 (TID 1622)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 167.0 in stage 15.0 (TID 1603) in 622 ms on localhost (171/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 173.0 in stage 15.0 (TID 1609). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 172.0 in stage 15.0 (TID 1608). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 187.0 in stage 15.0 (TID 1623, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 187.0 in stage 15.0 (TID 1623)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 188.0 in stage 15.0 (TID 1624, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 188.0 in stage 15.0 (TID 1624)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 172.0 in stage 15.0 (TID 1608) in 587 ms on localhost (172/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 173.0 in stage 15.0 (TID 1609) in 583 ms on localhost (173/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 171.0 in stage 15.0 (TID 1607). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 189.0 in stage 15.0 (TID 1625, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 189.0 in stage 15.0 (TID 1625)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 171.0 in stage 15.0 (TID 1607) in 620 ms on localhost (174/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 175.0 in stage 15.0 (TID 1611). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 190.0 in stage 15.0 (TID 1626, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 190.0 in stage 15.0 (TID 1626)
15/08/16 12:52:04 INFO Executor: Finished task 174.0 in stage 15.0 (TID 1610). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 191.0 in stage 15.0 (TID 1627, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 191.0 in stage 15.0 (TID 1627)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 175.0 in stage 15.0 (TID 1611) in 566 ms on localhost (175/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 174.0 in stage 15.0 (TID 1610) in 574 ms on localhost (176/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 176.0 in stage 15.0 (TID 1612). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 192.0 in stage 15.0 (TID 1628, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Running task 192.0 in stage 15.0 (TID 1628)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 176.0 in stage 15.0 (TID 1612) in 564 ms on localhost (177/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 177.0 in stage 15.0 (TID 1613). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 193.0 in stage 15.0 (TID 1629, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 193.0 in stage 15.0 (TID 1629)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 177.0 in stage 15.0 (TID 1613) in 559 ms on localhost (178/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 178.0 in stage 15.0 (TID 1614). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 194.0 in stage 15.0 (TID 1630, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 194.0 in stage 15.0 (TID 1630)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 178.0 in stage 15.0 (TID 1614) in 552 ms on localhost (179/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 179.0 in stage 15.0 (TID 1615). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 195.0 in stage 15.0 (TID 1631, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 195.0 in stage 15.0 (TID 1631)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 179.0 in stage 15.0 (TID 1615) in 560 ms on localhost (180/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 180.0 in stage 15.0 (TID 1616). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 196.0 in stage 15.0 (TID 1632, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 196.0 in stage 15.0 (TID 1632)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 180.0 in stage 15.0 (TID 1616) in 592 ms on localhost (181/200)
15/08/16 12:52:04 INFO Executor: Finished task 182.0 in stage 15.0 (TID 1618). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 197.0 in stage 15.0 (TID 1633, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 197.0 in stage 15.0 (TID 1633)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 182.0 in stage 15.0 (TID 1618) in 580 ms on localhost (182/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 181.0 in stage 15.0 (TID 1617). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 198.0 in stage 15.0 (TID 1634, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 198.0 in stage 15.0 (TID 1634)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 181.0 in stage 15.0 (TID 1617) in 602 ms on localhost (183/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 183.0 in stage 15.0 (TID 1619). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 199.0 in stage 15.0 (TID 1635, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/16 12:52:04 INFO Executor: Running task 199.0 in stage 15.0 (TID 1635)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 183.0 in stage 15.0 (TID 1619) in 383 ms on localhost (184/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 184.0 in stage 15.0 (TID 1620). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 184.0 in stage 15.0 (TID 1620) in 378 ms on localhost (185/200)
15/08/16 12:52:04 INFO Executor: Finished task 186.0 in stage 15.0 (TID 1622). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 185.0 in stage 15.0 (TID 1621). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 186.0 in stage 15.0 (TID 1622) in 381 ms on localhost (186/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 185.0 in stage 15.0 (TID 1621) in 387 ms on localhost (187/200)
15/08/16 12:52:04 INFO Executor: Finished task 188.0 in stage 15.0 (TID 1624). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 188.0 in stage 15.0 (TID 1624) in 375 ms on localhost (188/200)
15/08/16 12:52:04 INFO Executor: Finished task 189.0 in stage 15.0 (TID 1625). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 189.0 in stage 15.0 (TID 1625) in 374 ms on localhost (189/200)
15/08/16 12:52:04 INFO Executor: Finished task 187.0 in stage 15.0 (TID 1623). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 187.0 in stage 15.0 (TID 1623) in 393 ms on localhost (190/200)
15/08/16 12:52:04 INFO Executor: Finished task 191.0 in stage 15.0 (TID 1627). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 191.0 in stage 15.0 (TID 1627) in 372 ms on localhost (191/200)
15/08/16 12:52:04 INFO Executor: Finished task 190.0 in stage 15.0 (TID 1626). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 190.0 in stage 15.0 (TID 1626) in 379 ms on localhost (192/200)
15/08/16 12:52:04 INFO Executor: Finished task 192.0 in stage 15.0 (TID 1628). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 192.0 in stage 15.0 (TID 1628) in 372 ms on localhost (193/200)
15/08/16 12:52:04 INFO Executor: Finished task 193.0 in stage 15.0 (TID 1629). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 193.0 in stage 15.0 (TID 1629) in 366 ms on localhost (194/200)
15/08/16 12:52:04 INFO Executor: Finished task 194.0 in stage 15.0 (TID 1630). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 195.0 in stage 15.0 (TID 1631). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 194.0 in stage 15.0 (TID 1630) in 345 ms on localhost (195/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 195.0 in stage 15.0 (TID 1631) in 336 ms on localhost (196/200)
15/08/16 12:52:04 INFO Executor: Finished task 197.0 in stage 15.0 (TID 1633). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 198.0 in stage 15.0 (TID 1634). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 196.0 in stage 15.0 (TID 1632). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 199.0 in stage 15.0 (TID 1635). 1219 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 197.0 in stage 15.0 (TID 1633) in 200 ms on localhost (197/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 198.0 in stage 15.0 (TID 1634) in 192 ms on localhost (198/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 196.0 in stage 15.0 (TID 1632) in 206 ms on localhost (199/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 199.0 in stage 15.0 (TID 1635) in 169 ms on localhost (200/200)
15/08/16 12:52:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/08/16 12:52:04 INFO DAGScheduler: ShuffleMapStage 15 (processCmd at CliDriver.java:423) finished in 5.836 s
15/08/16 12:52:04 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:52:04 INFO DAGScheduler: running: Set()
15/08/16 12:52:04 INFO DAGScheduler: waiting: Set(ResultStage 16)
15/08/16 12:52:04 INFO DAGScheduler: failed: Set()
15/08/16 12:52:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3f89adda
15/08/16 12:52:04 INFO StatsReportListener: task runtime:(count: 200, mean: 463.515000, stdev: 141.591983, max: 765.000000, min: 130.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	130.0 ms	230.0 ms	319.0 ms	377.0 ms	432.0 ms	566.0 ms	687.0 ms	723.0 ms	765.0 ms
15/08/16 12:52:04 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 3900.030000, stdev: 160.552325, max: 4434.000000, min: 3531.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	3.4 KB	3.6 KB	3.6 KB	3.7 KB	3.8 KB	3.9 KB	4.0 KB	4.1 KB	4.3 KB
15/08/16 12:52:04 INFO DAGScheduler: Missing parents for ResultStage 16: List()
15/08/16 12:52:04 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[80] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:52:04 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.515000, stdev: 0.727856, max: 4.000000, min: 0.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	4.0 ms
15/08/16 12:52:04 INFO MemoryStore: ensureFreeSpace(18312) called with curMem=508727534, maxMem=3333968363
15/08/16 12:52:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.9 KB, free 2.6 GB)
15/08/16 12:52:04 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:52:04 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/16 12:52:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.291477, stdev: 1.375069, max: 99.154930, min: 90.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	90 %	95 %	96 %	97 %	98 %	98 %	99 %	99 %	99 %
15/08/16 12:52:04 INFO MemoryStore: ensureFreeSpace(8382) called with curMem=508745846, maxMem=3333968363
15/08/16 12:52:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.2 KB, free 2.6 GB)
15/08/16 12:52:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:36543 (size: 8.2 KB, free: 3.1 GB)
15/08/16 12:52:04 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.125015, stdev: 0.218879, max: 2.112676, min: 0.000000)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %
15/08/16 12:52:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:874
15/08/16 12:52:04 INFO StatsReportListener: other time pct: (count: 200, mean: 2.583508, stdev: 1.332924, max: 10.000000, min: 0.845070)
15/08/16 12:52:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:04 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	 4 %	 5 %	10 %
15/08/16 12:52:04 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 16 (MapPartitionsRDD[80] at processCmd at CliDriver.java:423)
15/08/16 12:52:04 INFO TaskSchedulerImpl: Adding task set 16.0 with 200 tasks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 1636, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 1637, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 1638, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 1639, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 1640, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 1641, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 1642, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 1643, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 1644, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 1645, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 1646, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 1647, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 12.0 in stage 16.0 (TID 1648, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 13.0 in stage 16.0 (TID 1649, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 14.0 in stage 16.0 (TID 1650, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 15.0 in stage 16.0 (TID 1651, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 0.0 in stage 16.0 (TID 1636)
15/08/16 12:52:04 INFO Executor: Running task 4.0 in stage 16.0 (TID 1640)
15/08/16 12:52:04 INFO Executor: Running task 5.0 in stage 16.0 (TID 1641)
15/08/16 12:52:04 INFO Executor: Running task 6.0 in stage 16.0 (TID 1642)
15/08/16 12:52:04 INFO Executor: Running task 12.0 in stage 16.0 (TID 1648)
15/08/16 12:52:04 INFO Executor: Running task 2.0 in stage 16.0 (TID 1638)
15/08/16 12:52:04 INFO Executor: Running task 3.0 in stage 16.0 (TID 1639)
15/08/16 12:52:04 INFO Executor: Running task 13.0 in stage 16.0 (TID 1649)
15/08/16 12:52:04 INFO Executor: Running task 1.0 in stage 16.0 (TID 1637)
15/08/16 12:52:04 INFO Executor: Running task 14.0 in stage 16.0 (TID 1650)
15/08/16 12:52:04 INFO Executor: Running task 15.0 in stage 16.0 (TID 1651)
15/08/16 12:52:04 INFO Executor: Running task 11.0 in stage 16.0 (TID 1647)
15/08/16 12:52:04 INFO Executor: Running task 10.0 in stage 16.0 (TID 1646)
15/08/16 12:52:04 INFO Executor: Running task 7.0 in stage 16.0 (TID 1643)
15/08/16 12:52:04 INFO Executor: Running task 8.0 in stage 16.0 (TID 1644)
15/08/16 12:52:04 INFO Executor: Running task 9.0 in stage 16.0 (TID 1645)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 12.0 in stage 16.0 (TID 1648). 895 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 9.0 in stage 16.0 (TID 1645). 1146 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 14.0 in stage 16.0 (TID 1650). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 2.0 in stage 16.0 (TID 1638). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 1.0 in stage 16.0 (TID 1637). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 4.0 in stage 16.0 (TID 1640). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 7.0 in stage 16.0 (TID 1643). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 15.0 in stage 16.0 (TID 1651). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 11.0 in stage 16.0 (TID 1647). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 16.0 in stage 16.0 (TID 1652, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 0.0 in stage 16.0 (TID 1636). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 3.0 in stage 16.0 (TID 1639). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 16.0 in stage 16.0 (TID 1652)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 17.0 in stage 16.0 (TID 1653, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 17.0 in stage 16.0 (TID 1653)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 18.0 in stage 16.0 (TID 1654, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 10.0 in stage 16.0 (TID 1646). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 19.0 in stage 16.0 (TID 1655, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 12.0 in stage 16.0 (TID 1648) in 32 ms on localhost (1/200)
15/08/16 12:52:04 INFO Executor: Running task 19.0 in stage 16.0 (TID 1655)
15/08/16 12:52:04 INFO Executor: Running task 18.0 in stage 16.0 (TID 1654)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 20.0 in stage 16.0 (TID 1656, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 20.0 in stage 16.0 (TID 1656)
15/08/16 12:52:04 INFO Executor: Finished task 5.0 in stage 16.0 (TID 1641). 1146 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 21.0 in stage 16.0 (TID 1657, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 21.0 in stage 16.0 (TID 1657)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 1645) in 44 ms on localhost (2/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 1637) in 47 ms on localhost (3/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 22.0 in stage 16.0 (TID 1658, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 22.0 in stage 16.0 (TID 1658)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 14.0 in stage 16.0 (TID 1650) in 44 ms on localhost (4/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 23.0 in stage 16.0 (TID 1659, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 23.0 in stage 16.0 (TID 1659)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 24.0 in stage 16.0 (TID 1660, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 1638) in 49 ms on localhost (5/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 1640) in 49 ms on localhost (6/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 1647) in 49 ms on localhost (7/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 24.0 in stage 16.0 (TID 1660)
15/08/16 12:52:04 INFO Executor: Finished task 6.0 in stage 16.0 (TID 1642). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 25.0 in stage 16.0 (TID 1661, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 25.0 in stage 16.0 (TID 1661)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 1643) in 50 ms on localhost (8/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 15.0 in stage 16.0 (TID 1651) in 49 ms on localhost (9/200)
15/08/16 12:52:04 INFO Executor: Finished task 13.0 in stage 16.0 (TID 1649). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 26.0 in stage 16.0 (TID 1662, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 27.0 in stage 16.0 (TID 1663, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 27.0 in stage 16.0 (TID 1663)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 28.0 in stage 16.0 (TID 1664, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 26.0 in stage 16.0 (TID 1662)
15/08/16 12:52:04 INFO Executor: Running task 28.0 in stage 16.0 (TID 1664)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 29.0 in stage 16.0 (TID 1665, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Running task 29.0 in stage 16.0 (TID 1665)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 30.0 in stage 16.0 (TID 1666, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 8.0 in stage 16.0 (TID 1644). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 31.0 in stage 16.0 (TID 1667, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 1646) in 53 ms on localhost (10/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 1639) in 54 ms on localhost (11/200)
15/08/16 12:52:04 INFO Executor: Running task 31.0 in stage 16.0 (TID 1667)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 1636) in 56 ms on localhost (12/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 1641) in 56 ms on localhost (13/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 1644) in 56 ms on localhost (14/200)
15/08/16 12:52:04 INFO Executor: Running task 30.0 in stage 16.0 (TID 1666)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 1642) in 57 ms on localhost (15/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 13.0 in stage 16.0 (TID 1649) in 59 ms on localhost (16/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 20.0 in stage 16.0 (TID 1656). 895 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:52:04 INFO Executor: Finished task 17.0 in stage 16.0 (TID 1653). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 32.0 in stage 16.0 (TID 1668, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 16.0 in stage 16.0 (TID 1652). 895 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 32.0 in stage 16.0 (TID 1668)
15/08/16 12:52:04 INFO Executor: Finished task 21.0 in stage 16.0 (TID 1657). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 33.0 in stage 16.0 (TID 1669, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 33.0 in stage 16.0 (TID 1669)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 34.0 in stage 16.0 (TID 1670, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 35.0 in stage 16.0 (TID 1671, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 34.0 in stage 16.0 (TID 1670)
15/08/16 12:52:04 INFO Executor: Running task 35.0 in stage 16.0 (TID 1671)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 20.0 in stage 16.0 (TID 1656) in 32 ms on localhost (17/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 16.0 in stage 16.0 (TID 1652) in 35 ms on localhost (18/200)
15/08/16 12:52:04 INFO Executor: Finished task 24.0 in stage 16.0 (TID 1660). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 36.0 in stage 16.0 (TID 1672, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 17.0 in stage 16.0 (TID 1653) in 35 ms on localhost (19/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 23.0 in stage 16.0 (TID 1659). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 36.0 in stage 16.0 (TID 1672)
15/08/16 12:52:04 INFO Executor: Finished task 18.0 in stage 16.0 (TID 1654). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 24.0 in stage 16.0 (TID 1660) in 21 ms on localhost (20/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 21.0 in stage 16.0 (TID 1657) in 27 ms on localhost (21/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 37.0 in stage 16.0 (TID 1673, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 38.0 in stage 16.0 (TID 1674, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 19.0 in stage 16.0 (TID 1655). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 38.0 in stage 16.0 (TID 1674)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 23.0 in stage 16.0 (TID 1659) in 26 ms on localhost (22/200)
15/08/16 12:52:04 INFO Executor: Running task 37.0 in stage 16.0 (TID 1673)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 18.0 in stage 16.0 (TID 1654) in 40 ms on localhost (23/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 39.0 in stage 16.0 (TID 1675, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 39.0 in stage 16.0 (TID 1675)
15/08/16 12:52:04 INFO Executor: Finished task 28.0 in stage 16.0 (TID 1664). 895 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 26.0 in stage 16.0 (TID 1662). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 40.0 in stage 16.0 (TID 1676, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 40.0 in stage 16.0 (TID 1676)
15/08/16 12:52:04 INFO Executor: Finished task 25.0 in stage 16.0 (TID 1661). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 41.0 in stage 16.0 (TID 1677, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 29.0 in stage 16.0 (TID 1665). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 19.0 in stage 16.0 (TID 1655) in 43 ms on localhost (24/200)
15/08/16 12:52:04 INFO Executor: Running task 41.0 in stage 16.0 (TID 1677)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 42.0 in stage 16.0 (TID 1678, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Running task 42.0 in stage 16.0 (TID 1678)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 22.0 in stage 16.0 (TID 1658). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 28.0 in stage 16.0 (TID 1664) in 24 ms on localhost (25/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 27.0 in stage 16.0 (TID 1663). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 32.0 in stage 16.0 (TID 1668). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 43.0 in stage 16.0 (TID 1679, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 34.0 in stage 16.0 (TID 1670). 895 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 26.0 in stage 16.0 (TID 1662) in 29 ms on localhost (26/200)
15/08/16 12:52:04 INFO Executor: Running task 43.0 in stage 16.0 (TID 1679)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 25.0 in stage 16.0 (TID 1661) in 30 ms on localhost (27/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 29.0 in stage 16.0 (TID 1665) in 28 ms on localhost (28/200)
15/08/16 12:52:04 INFO Executor: Finished task 33.0 in stage 16.0 (TID 1669). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 44.0 in stage 16.0 (TID 1680, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 44.0 in stage 16.0 (TID 1680)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 45.0 in stage 16.0 (TID 1681, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 45.0 in stage 16.0 (TID 1681)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 46.0 in stage 16.0 (TID 1682, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 46.0 in stage 16.0 (TID 1682)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 47.0 in stage 16.0 (TID 1683, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 35.0 in stage 16.0 (TID 1671). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 22.0 in stage 16.0 (TID 1658) in 37 ms on localhost (29/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 27.0 in stage 16.0 (TID 1663) in 32 ms on localhost (30/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 47.0 in stage 16.0 (TID 1683)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 34.0 in stage 16.0 (TID 1670) in 20 ms on localhost (31/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 32.0 in stage 16.0 (TID 1668) in 21 ms on localhost (32/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 36.0 in stage 16.0 (TID 1672). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 48.0 in stage 16.0 (TID 1684, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 48.0 in stage 16.0 (TID 1684)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 33.0 in stage 16.0 (TID 1669) in 22 ms on localhost (33/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 31.0 in stage 16.0 (TID 1667). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 49.0 in stage 16.0 (TID 1685, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 49.0 in stage 16.0 (TID 1685)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 50.0 in stage 16.0 (TID 1686, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 50.0 in stage 16.0 (TID 1686)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 35.0 in stage 16.0 (TID 1671) in 25 ms on localhost (34/200)
15/08/16 12:52:04 INFO Executor: Finished task 30.0 in stage 16.0 (TID 1666). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 36.0 in stage 16.0 (TID 1672) in 25 ms on localhost (35/200)
15/08/16 12:52:04 INFO Executor: Finished task 39.0 in stage 16.0 (TID 1675). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 51.0 in stage 16.0 (TID 1687, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 51.0 in stage 16.0 (TID 1687)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 52.0 in stage 16.0 (TID 1688, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 40.0 in stage 16.0 (TID 1676). 1146 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 53.0 in stage 16.0 (TID 1689, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Finished task 41.0 in stage 16.0 (TID 1677). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 53.0 in stage 16.0 (TID 1689)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 31.0 in stage 16.0 (TID 1667) in 40 ms on localhost (36/200)
15/08/16 12:52:04 INFO Executor: Running task 52.0 in stage 16.0 (TID 1688)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 54.0 in stage 16.0 (TID 1690, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 44.0 in stage 16.0 (TID 1680). 895 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 42.0 in stage 16.0 (TID 1678). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 30.0 in stage 16.0 (TID 1666) in 43 ms on localhost (37/200)
15/08/16 12:52:04 INFO Executor: Finished task 43.0 in stage 16.0 (TID 1679). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 54.0 in stage 16.0 (TID 1690)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 55.0 in stage 16.0 (TID 1691, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 55.0 in stage 16.0 (TID 1691)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 56.0 in stage 16.0 (TID 1692, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 56.0 in stage 16.0 (TID 1692)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 57.0 in stage 16.0 (TID 1693, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 45.0 in stage 16.0 (TID 1681). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 57.0 in stage 16.0 (TID 1693)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 58.0 in stage 16.0 (TID 1694, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 58.0 in stage 16.0 (TID 1694)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 59.0 in stage 16.0 (TID 1695, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 59.0 in stage 16.0 (TID 1695)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 39.0 in stage 16.0 (TID 1675) in 26 ms on localhost (38/200)
15/08/16 12:52:04 INFO Executor: Finished task 47.0 in stage 16.0 (TID 1683). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 44.0 in stage 16.0 (TID 1680) in 19 ms on localhost (39/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 42.0 in stage 16.0 (TID 1678) in 25 ms on localhost (40/200)
15/08/16 12:52:04 INFO Executor: Finished task 48.0 in stage 16.0 (TID 1684). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 60.0 in stage 16.0 (TID 1696, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Finished task 46.0 in stage 16.0 (TID 1682). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 61.0 in stage 16.0 (TID 1697, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Running task 61.0 in stage 16.0 (TID 1697)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 43.0 in stage 16.0 (TID 1679) in 24 ms on localhost (41/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 45.0 in stage 16.0 (TID 1681) in 21 ms on localhost (42/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Running task 60.0 in stage 16.0 (TID 1696)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:04 INFO Executor: Finished task 38.0 in stage 16.0 (TID 1674). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 41.0 in stage 16.0 (TID 1677) in 29 ms on localhost (43/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 62.0 in stage 16.0 (TID 1698, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 63.0 in stage 16.0 (TID 1699, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 63.0 in stage 16.0 (TID 1699)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 48.0 in stage 16.0 (TID 1684) in 21 ms on localhost (44/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 40.0 in stage 16.0 (TID 1676) in 31 ms on localhost (45/200)
15/08/16 12:52:04 INFO Executor: Finished task 37.0 in stage 16.0 (TID 1673). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 38.0 in stage 16.0 (TID 1674) in 36 ms on localhost (46/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 64.0 in stage 16.0 (TID 1700, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 62.0 in stage 16.0 (TID 1698)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 46.0 in stage 16.0 (TID 1682) in 26 ms on localhost (47/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 47.0 in stage 16.0 (TID 1683) in 26 ms on localhost (48/200)
15/08/16 12:52:04 INFO Executor: Running task 64.0 in stage 16.0 (TID 1700)
15/08/16 12:52:04 INFO Executor: Finished task 51.0 in stage 16.0 (TID 1687). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 37.0 in stage 16.0 (TID 1673) in 40 ms on localhost (49/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 65.0 in stage 16.0 (TID 1701, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 65.0 in stage 16.0 (TID 1701)
15/08/16 12:52:04 INFO Executor: Finished task 52.0 in stage 16.0 (TID 1688). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 66.0 in stage 16.0 (TID 1702, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 66.0 in stage 16.0 (TID 1702)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 54.0 in stage 16.0 (TID 1690). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 52.0 in stage 16.0 (TID 1688) in 23 ms on localhost (50/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 67.0 in stage 16.0 (TID 1703, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 55.0 in stage 16.0 (TID 1691). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 58.0 in stage 16.0 (TID 1694). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 56.0 in stage 16.0 (TID 1692). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 67.0 in stage 16.0 (TID 1703)
15/08/16 12:52:04 INFO Executor: Finished task 49.0 in stage 16.0 (TID 1685). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 50.0 in stage 16.0 (TID 1686). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 57.0 in stage 16.0 (TID 1693). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 61.0 in stage 16.0 (TID 1697). 1031 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 68.0 in stage 16.0 (TID 1704, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 59.0 in stage 16.0 (TID 1695). 895 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 68.0 in stage 16.0 (TID 1704)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 54.0 in stage 16.0 (TID 1690) in 25 ms on localhost (51/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 51.0 in stage 16.0 (TID 1687) in 29 ms on localhost (52/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 69.0 in stage 16.0 (TID 1705, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 70.0 in stage 16.0 (TID 1706, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 69.0 in stage 16.0 (TID 1705)
15/08/16 12:52:04 INFO Executor: Running task 70.0 in stage 16.0 (TID 1706)
15/08/16 12:52:04 INFO Executor: Finished task 63.0 in stage 16.0 (TID 1699). 1118 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 71.0 in stage 16.0 (TID 1707, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 71.0 in stage 16.0 (TID 1707)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 72.0 in stage 16.0 (TID 1708, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 53.0 in stage 16.0 (TID 1689). 1059 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 72.0 in stage 16.0 (TID 1708)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 55.0 in stage 16.0 (TID 1691) in 27 ms on localhost (53/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 56.0 in stage 16.0 (TID 1692) in 28 ms on localhost (54/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 73.0 in stage 16.0 (TID 1709, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO Executor: Finished task 64.0 in stage 16.0 (TID 1700). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 58.0 in stage 16.0 (TID 1694) in 28 ms on localhost (55/200)
15/08/16 12:52:04 INFO Executor: Running task 73.0 in stage 16.0 (TID 1709)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 74.0 in stage 16.0 (TID 1710, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 74.0 in stage 16.0 (TID 1710)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 75.0 in stage 16.0 (TID 1711, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 75.0 in stage 16.0 (TID 1711)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 50.0 in stage 16.0 (TID 1686) in 40 ms on localhost (56/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 57.0 in stage 16.0 (TID 1693) in 31 ms on localhost (57/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 49.0 in stage 16.0 (TID 1685) in 41 ms on localhost (58/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 60.0 in stage 16.0 (TID 1696). 1146 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 59.0 in stage 16.0 (TID 1695) in 31 ms on localhost (59/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 61.0 in stage 16.0 (TID 1697) in 27 ms on localhost (60/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 63.0 in stage 16.0 (TID 1699) in 24 ms on localhost (61/200)
15/08/16 12:52:04 INFO Executor: Finished task 65.0 in stage 16.0 (TID 1701). 1147 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 76.0 in stage 16.0 (TID 1712, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Running task 76.0 in stage 16.0 (TID 1712)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 77.0 in stage 16.0 (TID 1713, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 77.0 in stage 16.0 (TID 1713)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 78.0 in stage 16.0 (TID 1714, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 66.0 in stage 16.0 (TID 1702). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 78.0 in stage 16.0 (TID 1714)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 79.0 in stage 16.0 (TID 1715, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 80.0 in stage 16.0 (TID 1716, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 79.0 in stage 16.0 (TID 1715)
15/08/16 12:52:04 INFO Executor: Finished task 62.0 in stage 16.0 (TID 1698). 1087 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 80.0 in stage 16.0 (TID 1716)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 81.0 in stage 16.0 (TID 1717, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 81.0 in stage 16.0 (TID 1717)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Starting task 82.0 in stage 16.0 (TID 1718, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 82.0 in stage 16.0 (TID 1718)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 53.0 in stage 16.0 (TID 1689) in 42 ms on localhost (62/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Finished task 68.0 in stage 16.0 (TID 1704). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 65.0 in stage 16.0 (TID 1701) in 24 ms on localhost (63/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 60.0 in stage 16.0 (TID 1696) in 34 ms on localhost (64/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 83.0 in stage 16.0 (TID 1719, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 83.0 in stage 16.0 (TID 1719)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 66.0 in stage 16.0 (TID 1702) in 23 ms on localhost (65/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 68.0 in stage 16.0 (TID 1704) in 20 ms on localhost (66/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 67.0 in stage 16.0 (TID 1703). 896 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 62.0 in stage 16.0 (TID 1698) in 34 ms on localhost (67/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Finished task 71.0 in stage 16.0 (TID 1707). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 64.0 in stage 16.0 (TID 1700) in 32 ms on localhost (68/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 70.0 in stage 16.0 (TID 1706). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 84.0 in stage 16.0 (TID 1720, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 84.0 in stage 16.0 (TID 1720)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 67.0 in stage 16.0 (TID 1703) in 25 ms on localhost (69/200)
15/08/16 12:52:04 INFO Executor: Finished task 74.0 in stage 16.0 (TID 1710). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 71.0 in stage 16.0 (TID 1707) in 20 ms on localhost (70/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 85.0 in stage 16.0 (TID 1721, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 85.0 in stage 16.0 (TID 1721)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 86.0 in stage 16.0 (TID 1722, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 87.0 in stage 16.0 (TID 1723, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 87.0 in stage 16.0 (TID 1723)
15/08/16 12:52:04 INFO Executor: Running task 86.0 in stage 16.0 (TID 1722)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 70.0 in stage 16.0 (TID 1706) in 23 ms on localhost (71/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 74.0 in stage 16.0 (TID 1710) in 18 ms on localhost (72/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 76.0 in stage 16.0 (TID 1712). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 88.0 in stage 16.0 (TID 1724, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 73.0 in stage 16.0 (TID 1709). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 88.0 in stage 16.0 (TID 1724)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 89.0 in stage 16.0 (TID 1725, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 69.0 in stage 16.0 (TID 1705). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 78.0 in stage 16.0 (TID 1714). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 76.0 in stage 16.0 (TID 1712) in 20 ms on localhost (73/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 73.0 in stage 16.0 (TID 1709) in 25 ms on localhost (74/200)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 90.0 in stage 16.0 (TID 1726, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 91.0 in stage 16.0 (TID 1727, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 72.0 in stage 16.0 (TID 1708). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 89.0 in stage 16.0 (TID 1725)
15/08/16 12:52:04 INFO Executor: Running task 90.0 in stage 16.0 (TID 1726)
15/08/16 12:52:04 INFO Executor: Running task 91.0 in stage 16.0 (TID 1727)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 92.0 in stage 16.0 (TID 1728, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 92.0 in stage 16.0 (TID 1728)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 69.0 in stage 16.0 (TID 1705) in 32 ms on localhost (75/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 78.0 in stage 16.0 (TID 1714) in 21 ms on localhost (76/200)
15/08/16 12:52:04 INFO Executor: Finished task 82.0 in stage 16.0 (TID 1718). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 79.0 in stage 16.0 (TID 1715). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 77.0 in stage 16.0 (TID 1713). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 72.0 in stage 16.0 (TID 1708) in 30 ms on localhost (77/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 93.0 in stage 16.0 (TID 1729, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 93.0 in stage 16.0 (TID 1729)
15/08/16 12:52:04 INFO Executor: Finished task 75.0 in stage 16.0 (TID 1711). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 94.0 in stage 16.0 (TID 1730, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 95.0 in stage 16.0 (TID 1731, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 95.0 in stage 16.0 (TID 1731)
15/08/16 12:52:04 INFO Executor: Running task 94.0 in stage 16.0 (TID 1730)
15/08/16 12:52:04 INFO Executor: Finished task 85.0 in stage 16.0 (TID 1721). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 82.0 in stage 16.0 (TID 1718) in 24 ms on localhost (78/200)
15/08/16 12:52:04 INFO Executor: Finished task 83.0 in stage 16.0 (TID 1719). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 79.0 in stage 16.0 (TID 1715) in 26 ms on localhost (79/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 96.0 in stage 16.0 (TID 1732, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 77.0 in stage 16.0 (TID 1713) in 28 ms on localhost (80/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 80.0 in stage 16.0 (TID 1716). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 96.0 in stage 16.0 (TID 1732)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 75.0 in stage 16.0 (TID 1711) in 34 ms on localhost (81/200)
15/08/16 12:52:04 INFO Executor: Finished task 87.0 in stage 16.0 (TID 1723). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 97.0 in stage 16.0 (TID 1733, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 97.0 in stage 16.0 (TID 1733)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 98.0 in stage 16.0 (TID 1734, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 99.0 in stage 16.0 (TID 1735, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 99.0 in stage 16.0 (TID 1735)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 100.0 in stage 16.0 (TID 1736, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 98.0 in stage 16.0 (TID 1734)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 85.0 in stage 16.0 (TID 1721) in 21 ms on localhost (82/200)
15/08/16 12:52:04 INFO Executor: Running task 100.0 in stage 16.0 (TID 1736)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 83.0 in stage 16.0 (TID 1719) in 27 ms on localhost (83/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 80.0 in stage 16.0 (TID 1716) in 32 ms on localhost (84/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 86.0 in stage 16.0 (TID 1722). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 84.0 in stage 16.0 (TID 1720). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Finished task 81.0 in stage 16.0 (TID 1717). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 101.0 in stage 16.0 (TID 1737, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 101.0 in stage 16.0 (TID 1737)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 102.0 in stage 16.0 (TID 1738, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 102.0 in stage 16.0 (TID 1738)
15/08/16 12:52:04 INFO Executor: Finished task 90.0 in stage 16.0 (TID 1726). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Starting task 103.0 in stage 16.0 (TID 1739, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 104.0 in stage 16.0 (TID 1740, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 103.0 in stage 16.0 (TID 1739)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Running task 104.0 in stage 16.0 (TID 1740)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 87.0 in stage 16.0 (TID 1723) in 29 ms on localhost (85/200)
15/08/16 12:52:04 INFO Executor: Finished task 93.0 in stage 16.0 (TID 1729). 896 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO TaskSetManager: Finished task 81.0 in stage 16.0 (TID 1717) in 41 ms on localhost (86/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 86.0 in stage 16.0 (TID 1722) in 31 ms on localhost (87/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 90.0 in stage 16.0 (TID 1726) in 23 ms on localhost (88/200)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 84.0 in stage 16.0 (TID 1720) in 33 ms on localhost (89/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 95.0 in stage 16.0 (TID 1731). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 105.0 in stage 16.0 (TID 1741, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 106.0 in stage 16.0 (TID 1742, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 93.0 in stage 16.0 (TID 1729) in 21 ms on localhost (90/200)
15/08/16 12:52:04 INFO Executor: Finished task 88.0 in stage 16.0 (TID 1724). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 105.0 in stage 16.0 (TID 1741)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 95.0 in stage 16.0 (TID 1731) in 20 ms on localhost (91/200)
15/08/16 12:52:04 INFO Executor: Finished task 89.0 in stage 16.0 (TID 1725). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 106.0 in stage 16.0 (TID 1742)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 107.0 in stage 16.0 (TID 1743, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 108.0 in stage 16.0 (TID 1744, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO Executor: Running task 107.0 in stage 16.0 (TID 1743)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO Executor: Finished task 92.0 in stage 16.0 (TID 1728). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 88.0 in stage 16.0 (TID 1724) in 34 ms on localhost (92/200)
15/08/16 12:52:04 INFO Executor: Running task 108.0 in stage 16.0 (TID 1744)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 109.0 in stage 16.0 (TID 1745, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 109.0 in stage 16.0 (TID 1745)
15/08/16 12:52:04 INFO TaskSetManager: Finished task 89.0 in stage 16.0 (TID 1725) in 33 ms on localhost (93/200)
15/08/16 12:52:04 INFO Executor: Finished task 91.0 in stage 16.0 (TID 1727). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 98.0 in stage 16.0 (TID 1734). 1060 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Finished task 100.0 in stage 16.0 (TID 1736). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:04 INFO TaskSetManager: Finished task 92.0 in stage 16.0 (TID 1728) in 33 ms on localhost (94/200)
15/08/16 12:52:04 INFO Executor: Finished task 97.0 in stage 16.0 (TID 1733). 1088 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Starting task 110.0 in stage 16.0 (TID 1746, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Finished task 99.0 in stage 16.0 (TID 1735). 1119 bytes result sent to driver
15/08/16 12:52:04 INFO Executor: Running task 110.0 in stage 16.0 (TID 1746)
15/08/16 12:52:04 INFO TaskSetManager: Starting task 111.0 in stage 16.0 (TID 1747, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:04 INFO Executor: Running task 111.0 in stage 16.0 (TID 1747)
15/08/16 12:52:04 INFO Executor: Finished task 101.0 in stage 16.0 (TID 1737). 1032 bytes result sent to driver
15/08/16 12:52:04 INFO TaskSetManager: Finished task 91.0 in stage 16.0 (TID 1727) in 35 ms on localhost (95/200)
15/08/16 12:52:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 112.0 in stage 16.0 (TID 1748, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 113.0 in stage 16.0 (TID 1749, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 113.0 in stage 16.0 (TID 1749)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 98.0 in stage 16.0 (TID 1734) in 37 ms on localhost (96/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 97.0 in stage 16.0 (TID 1733) in 38 ms on localhost (97/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 114.0 in stage 16.0 (TID 1750, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 114.0 in stage 16.0 (TID 1750)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 115.0 in stage 16.0 (TID 1751, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 115.0 in stage 16.0 (TID 1751)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 100.0 in stage 16.0 (TID 1736) in 38 ms on localhost (98/200)
15/08/16 12:52:05 INFO Executor: Finished task 103.0 in stage 16.0 (TID 1739). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 104.0 in stage 16.0 (TID 1740). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 116.0 in stage 16.0 (TID 1752, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 112.0 in stage 16.0 (TID 1748)
15/08/16 12:52:05 INFO Executor: Finished task 96.0 in stage 16.0 (TID 1732). 1147 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 116.0 in stage 16.0 (TID 1752)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 94.0 in stage 16.0 (TID 1730). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 106.0 in stage 16.0 (TID 1742). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 117.0 in stage 16.0 (TID 1753, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 118.0 in stage 16.0 (TID 1754, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 102.0 in stage 16.0 (TID 1738). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Running task 118.0 in stage 16.0 (TID 1754)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 119.0 in stage 16.0 (TID 1755, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 120.0 in stage 16.0 (TID 1756, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 101.0 in stage 16.0 (TID 1737) in 42 ms on localhost (99/200)
15/08/16 12:52:05 INFO Executor: Running task 120.0 in stage 16.0 (TID 1756)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 96.0 in stage 16.0 (TID 1732) in 49 ms on localhost (100/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 99.0 in stage 16.0 (TID 1735) in 46 ms on localhost (101/200)
15/08/16 12:52:05 INFO Executor: Running task 117.0 in stage 16.0 (TID 1753)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 106.0 in stage 16.0 (TID 1742) in 35 ms on localhost (102/200)
15/08/16 12:52:05 INFO Executor: Running task 119.0 in stage 16.0 (TID 1755)
15/08/16 12:52:05 INFO Executor: Finished task 109.0 in stage 16.0 (TID 1745). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 110.0 in stage 16.0 (TID 1746). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 121.0 in stage 16.0 (TID 1757, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Finished task 103.0 in stage 16.0 (TID 1739) in 43 ms on localhost (103/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 94.0 in stage 16.0 (TID 1730) in 56 ms on localhost (104/200)
15/08/16 12:52:05 INFO Executor: Running task 121.0 in stage 16.0 (TID 1757)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 104.0 in stage 16.0 (TID 1740) in 43 ms on localhost (105/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 102.0 in stage 16.0 (TID 1738) in 47 ms on localhost (106/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 108.0 in stage 16.0 (TID 1744). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 109.0 in stage 16.0 (TID 1745) in 34 ms on localhost (107/200)
15/08/16 12:52:05 INFO Executor: Finished task 114.0 in stage 16.0 (TID 1750). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 111.0 in stage 16.0 (TID 1747). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 122.0 in stage 16.0 (TID 1758, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 122.0 in stage 16.0 (TID 1758)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 123.0 in stage 16.0 (TID 1759, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 105.0 in stage 16.0 (TID 1741). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 124.0 in stage 16.0 (TID 1760, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 124.0 in stage 16.0 (TID 1760)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 125.0 in stage 16.0 (TID 1761, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 125.0 in stage 16.0 (TID 1761)
15/08/16 12:52:05 INFO Executor: Finished task 112.0 in stage 16.0 (TID 1748). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 126.0 in stage 16.0 (TID 1762, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 126.0 in stage 16.0 (TID 1762)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 127.0 in stage 16.0 (TID 1763, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 123.0 in stage 16.0 (TID 1759)
15/08/16 12:52:05 INFO Executor: Running task 127.0 in stage 16.0 (TID 1763)
15/08/16 12:52:05 INFO Executor: Finished task 116.0 in stage 16.0 (TID 1752). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 115.0 in stage 16.0 (TID 1751). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 113.0 in stage 16.0 (TID 1749). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 128.0 in stage 16.0 (TID 1764, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 128.0 in stage 16.0 (TID 1764)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 111.0 in stage 16.0 (TID 1747) in 36 ms on localhost (108/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 114.0 in stage 16.0 (TID 1750) in 21 ms on localhost (109/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 108.0 in stage 16.0 (TID 1744) in 43 ms on localhost (110/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 129.0 in stage 16.0 (TID 1765, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Running task 129.0 in stage 16.0 (TID 1765)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 130.0 in stage 16.0 (TID 1766, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 112.0 in stage 16.0 (TID 1748) in 27 ms on localhost (111/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 120.0 in stage 16.0 (TID 1756). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 105.0 in stage 16.0 (TID 1741) in 49 ms on localhost (112/200)
15/08/16 12:52:05 INFO Executor: Running task 130.0 in stage 16.0 (TID 1766)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 131.0 in stage 16.0 (TID 1767, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO Executor: Running task 131.0 in stage 16.0 (TID 1767)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 110.0 in stage 16.0 (TID 1746) in 40 ms on localhost (113/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 116.0 in stage 16.0 (TID 1752) in 23 ms on localhost (114/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 115.0 in stage 16.0 (TID 1751) in 25 ms on localhost (115/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 132.0 in stage 16.0 (TID 1768, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 121.0 in stage 16.0 (TID 1757). 896 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 132.0 in stage 16.0 (TID 1768)
15/08/16 12:52:05 INFO Executor: Finished task 107.0 in stage 16.0 (TID 1743). 1147 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 117.0 in stage 16.0 (TID 1753). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 113.0 in stage 16.0 (TID 1749) in 28 ms on localhost (116/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 133.0 in stage 16.0 (TID 1769, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 134.0 in stage 16.0 (TID 1770, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 135.0 in stage 16.0 (TID 1771, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 135.0 in stage 16.0 (TID 1771)
15/08/16 12:52:05 INFO Executor: Running task 133.0 in stage 16.0 (TID 1769)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 121.0 in stage 16.0 (TID 1757) in 20 ms on localhost (117/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 107.0 in stage 16.0 (TID 1743) in 52 ms on localhost (118/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 117.0 in stage 16.0 (TID 1753) in 29 ms on localhost (119/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 120.0 in stage 16.0 (TID 1756) in 26 ms on localhost (120/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 118.0 in stage 16.0 (TID 1754). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 136.0 in stage 16.0 (TID 1772, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 136.0 in stage 16.0 (TID 1772)
15/08/16 12:52:05 INFO Executor: Finished task 122.0 in stage 16.0 (TID 1758). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 118.0 in stage 16.0 (TID 1754) in 32 ms on localhost (121/200)
15/08/16 12:52:05 INFO Executor: Running task 134.0 in stage 16.0 (TID 1770)
15/08/16 12:52:05 INFO Executor: Finished task 124.0 in stage 16.0 (TID 1760). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 123.0 in stage 16.0 (TID 1759). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 137.0 in stage 16.0 (TID 1773, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 138.0 in stage 16.0 (TID 1774, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 137.0 in stage 16.0 (TID 1773)
15/08/16 12:52:05 INFO Executor: Finished task 126.0 in stage 16.0 (TID 1762). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Finished task 124.0 in stage 16.0 (TID 1760) in 24 ms on localhost (122/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 139.0 in stage 16.0 (TID 1775, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 138.0 in stage 16.0 (TID 1774)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 123.0 in stage 16.0 (TID 1759) in 25 ms on localhost (123/200)
15/08/16 12:52:05 INFO Executor: Running task 139.0 in stage 16.0 (TID 1775)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 140.0 in stage 16.0 (TID 1776, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 140.0 in stage 16.0 (TID 1776)
15/08/16 12:52:05 INFO Executor: Finished task 119.0 in stage 16.0 (TID 1755). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 122.0 in stage 16.0 (TID 1758) in 27 ms on localhost (124/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 141.0 in stage 16.0 (TID 1777, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 141.0 in stage 16.0 (TID 1777)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 126.0 in stage 16.0 (TID 1762) in 26 ms on localhost (125/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 119.0 in stage 16.0 (TID 1755) in 39 ms on localhost (126/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 132.0 in stage 16.0 (TID 1768). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 129.0 in stage 16.0 (TID 1765). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 132.0 in stage 16.0 (TID 1768) in 21 ms on localhost (127/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 142.0 in stage 16.0 (TID 1778, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 135.0 in stage 16.0 (TID 1771). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 130.0 in stage 16.0 (TID 1766). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 142.0 in stage 16.0 (TID 1778)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 143.0 in stage 16.0 (TID 1779, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 133.0 in stage 16.0 (TID 1769). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 143.0 in stage 16.0 (TID 1779)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 144.0 in stage 16.0 (TID 1780, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 128.0 in stage 16.0 (TID 1764). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Finished task 129.0 in stage 16.0 (TID 1765) in 28 ms on localhost (128/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 127.0 in stage 16.0 (TID 1763). 1147 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 144.0 in stage 16.0 (TID 1780)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 135.0 in stage 16.0 (TID 1771) in 22 ms on localhost (129/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 130.0 in stage 16.0 (TID 1766) in 29 ms on localhost (130/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 145.0 in stage 16.0 (TID 1781, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 145.0 in stage 16.0 (TID 1781)
15/08/16 12:52:05 INFO Executor: Finished task 131.0 in stage 16.0 (TID 1767). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 146.0 in stage 16.0 (TID 1782, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 133.0 in stage 16.0 (TID 1769) in 24 ms on localhost (131/200)
15/08/16 12:52:05 INFO Executor: Running task 146.0 in stage 16.0 (TID 1782)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 147.0 in stage 16.0 (TID 1783, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 125.0 in stage 16.0 (TID 1761). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 128.0 in stage 16.0 (TID 1764) in 36 ms on localhost (132/200)
15/08/16 12:52:05 INFO Executor: Running task 147.0 in stage 16.0 (TID 1783)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 148.0 in stage 16.0 (TID 1784, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 148.0 in stage 16.0 (TID 1784)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Starting task 149.0 in stage 16.0 (TID 1785, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 150.0 in stage 16.0 (TID 1786, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 150.0 in stage 16.0 (TID 1786)
15/08/16 12:52:05 INFO Executor: Running task 149.0 in stage 16.0 (TID 1785)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 127.0 in stage 16.0 (TID 1763) in 38 ms on localhost (133/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 131.0 in stage 16.0 (TID 1767) in 33 ms on localhost (134/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 125.0 in stage 16.0 (TID 1761) in 40 ms on localhost (135/200)
15/08/16 12:52:05 INFO Executor: Finished task 138.0 in stage 16.0 (TID 1774). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 151.0 in stage 16.0 (TID 1787, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 140.0 in stage 16.0 (TID 1776). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 151.0 in stage 16.0 (TID 1787)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 138.0 in stage 16.0 (TID 1774) in 20 ms on localhost (136/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 141.0 in stage 16.0 (TID 1777). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 152.0 in stage 16.0 (TID 1788, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 137.0 in stage 16.0 (TID 1773). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 142.0 in stage 16.0 (TID 1778). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 152.0 in stage 16.0 (TID 1788)
15/08/16 12:52:05 INFO Executor: Finished task 139.0 in stage 16.0 (TID 1775). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 134.0 in stage 16.0 (TID 1770). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 153.0 in stage 16.0 (TID 1789, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 140.0 in stage 16.0 (TID 1776) in 24 ms on localhost (137/200)
15/08/16 12:52:05 INFO Executor: Running task 153.0 in stage 16.0 (TID 1789)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 154.0 in stage 16.0 (TID 1790, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 136.0 in stage 16.0 (TID 1772). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 154.0 in stage 16.0 (TID 1790)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 141.0 in stage 16.0 (TID 1777) in 24 ms on localhost (138/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 137.0 in stage 16.0 (TID 1773) in 28 ms on localhost (139/200)
15/08/16 12:52:05 INFO Executor: Finished task 143.0 in stage 16.0 (TID 1779). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 155.0 in stage 16.0 (TID 1791, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 155.0 in stage 16.0 (TID 1791)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 142.0 in stage 16.0 (TID 1778) in 20 ms on localhost (140/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 156.0 in stage 16.0 (TID 1792, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 144.0 in stage 16.0 (TID 1780). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 157.0 in stage 16.0 (TID 1793, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 156.0 in stage 16.0 (TID 1792)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 158.0 in stage 16.0 (TID 1794, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 139.0 in stage 16.0 (TID 1775) in 29 ms on localhost (141/200)
15/08/16 12:52:05 INFO Executor: Running task 158.0 in stage 16.0 (TID 1794)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 134.0 in stage 16.0 (TID 1770) in 41 ms on localhost (142/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 136.0 in stage 16.0 (TID 1772) in 37 ms on localhost (143/200)
15/08/16 12:52:05 INFO Executor: Running task 157.0 in stage 16.0 (TID 1793)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 159.0 in stage 16.0 (TID 1795, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 159.0 in stage 16.0 (TID 1795)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 148.0 in stage 16.0 (TID 1784). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 149.0 in stage 16.0 (TID 1785). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 160.0 in stage 16.0 (TID 1796, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 160.0 in stage 16.0 (TID 1796)
15/08/16 12:52:05 INFO Executor: Finished task 150.0 in stage 16.0 (TID 1786). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 161.0 in stage 16.0 (TID 1797, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 161.0 in stage 16.0 (TID 1797)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 162.0 in stage 16.0 (TID 1798, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 163.0 in stage 16.0 (TID 1799, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 163.0 in stage 16.0 (TID 1799)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Finished task 144.0 in stage 16.0 (TID 1780) in 25 ms on localhost (144/200)
15/08/16 12:52:05 INFO Executor: Running task 162.0 in stage 16.0 (TID 1798)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 143.0 in stage 16.0 (TID 1779) in 28 ms on localhost (145/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 148.0 in stage 16.0 (TID 1784) in 21 ms on localhost (146/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 151.0 in stage 16.0 (TID 1787). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 150.0 in stage 16.0 (TID 1786) in 21 ms on localhost (147/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 149.0 in stage 16.0 (TID 1785) in 22 ms on localhost (148/200)
15/08/16 12:52:05 INFO Executor: Finished task 145.0 in stage 16.0 (TID 1781). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 164.0 in stage 16.0 (TID 1800, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 165.0 in stage 16.0 (TID 1801, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 146.0 in stage 16.0 (TID 1782). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 164.0 in stage 16.0 (TID 1800)
15/08/16 12:52:05 INFO Executor: Running task 165.0 in stage 16.0 (TID 1801)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Starting task 166.0 in stage 16.0 (TID 1802, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 145.0 in stage 16.0 (TID 1781) in 28 ms on localhost (149/200)
15/08/16 12:52:05 INFO Executor: Running task 166.0 in stage 16.0 (TID 1802)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 151.0 in stage 16.0 (TID 1787) in 23 ms on localhost (150/200)
15/08/16 12:52:05 INFO Executor: Finished task 152.0 in stage 16.0 (TID 1788). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO Executor: Finished task 154.0 in stage 16.0 (TID 1790). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 146.0 in stage 16.0 (TID 1782) in 29 ms on localhost (151/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Finished task 152.0 in stage 16.0 (TID 1788) in 23 ms on localhost (152/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 167.0 in stage 16.0 (TID 1803, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 168.0 in stage 16.0 (TID 1804, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 168.0 in stage 16.0 (TID 1804)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 154.0 in stage 16.0 (TID 1790) in 19 ms on localhost (153/200)
15/08/16 12:52:05 INFO Executor: Finished task 158.0 in stage 16.0 (TID 1794). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 167.0 in stage 16.0 (TID 1803)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 156.0 in stage 16.0 (TID 1792). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 147.0 in stage 16.0 (TID 1783). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 169.0 in stage 16.0 (TID 1805, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 170.0 in stage 16.0 (TID 1806, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 160.0 in stage 16.0 (TID 1796). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 171.0 in stage 16.0 (TID 1807, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 171.0 in stage 16.0 (TID 1807)
15/08/16 12:52:05 INFO Executor: Running task 170.0 in stage 16.0 (TID 1806)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 172.0 in stage 16.0 (TID 1808, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 156.0 in stage 16.0 (TID 1792) in 22 ms on localhost (154/200)
15/08/16 12:52:05 INFO Executor: Running task 172.0 in stage 16.0 (TID 1808)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 147.0 in stage 16.0 (TID 1783) in 37 ms on localhost (155/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 158.0 in stage 16.0 (TID 1794) in 21 ms on localhost (156/200)
15/08/16 12:52:05 INFO Executor: Running task 169.0 in stage 16.0 (TID 1805)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 160.0 in stage 16.0 (TID 1796) in 19 ms on localhost (157/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO Executor: Finished task 163.0 in stage 16.0 (TID 1799). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 161.0 in stage 16.0 (TID 1797). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 162.0 in stage 16.0 (TID 1798). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Starting task 173.0 in stage 16.0 (TID 1809, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 174.0 in stage 16.0 (TID 1810, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 174.0 in stage 16.0 (TID 1810)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 175.0 in stage 16.0 (TID 1811, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 175.0 in stage 16.0 (TID 1811)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 163.0 in stage 16.0 (TID 1799) in 20 ms on localhost (158/200)
15/08/16 12:52:05 INFO Executor: Finished task 153.0 in stage 16.0 (TID 1789). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 161.0 in stage 16.0 (TID 1797) in 22 ms on localhost (159/200)
15/08/16 12:52:05 INFO Executor: Running task 173.0 in stage 16.0 (TID 1809)
15/08/16 12:52:05 INFO Executor: Finished task 164.0 in stage 16.0 (TID 1800). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 159.0 in stage 16.0 (TID 1795). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 176.0 in stage 16.0 (TID 1812, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 162.0 in stage 16.0 (TID 1798) in 25 ms on localhost (160/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 153.0 in stage 16.0 (TID 1789) in 36 ms on localhost (161/200)
15/08/16 12:52:05 INFO Executor: Running task 176.0 in stage 16.0 (TID 1812)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 164.0 in stage 16.0 (TID 1800) in 22 ms on localhost (162/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 177.0 in stage 16.0 (TID 1813, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 177.0 in stage 16.0 (TID 1813)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 178.0 in stage 16.0 (TID 1814, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 178.0 in stage 16.0 (TID 1814)
15/08/16 12:52:05 INFO Executor: Finished task 165.0 in stage 16.0 (TID 1801). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 157.0 in stage 16.0 (TID 1793). 896 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 171.0 in stage 16.0 (TID 1807). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 159.0 in stage 16.0 (TID 1795) in 32 ms on localhost (163/200)
15/08/16 12:52:05 INFO Executor: Finished task 172.0 in stage 16.0 (TID 1808). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 165.0 in stage 16.0 (TID 1801) in 27 ms on localhost (164/200)
15/08/16 12:52:05 INFO Executor: Finished task 167.0 in stage 16.0 (TID 1803). 1147 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 155.0 in stage 16.0 (TID 1791). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 179.0 in stage 16.0 (TID 1815, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 179.0 in stage 16.0 (TID 1815)
15/08/16 12:52:05 INFO Executor: Finished task 168.0 in stage 16.0 (TID 1804). 896 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 157.0 in stage 16.0 (TID 1793) in 39 ms on localhost (165/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 180.0 in stage 16.0 (TID 1816, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 180.0 in stage 16.0 (TID 1816)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 181.0 in stage 16.0 (TID 1817, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 181.0 in stage 16.0 (TID 1817)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 182.0 in stage 16.0 (TID 1818, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 182.0 in stage 16.0 (TID 1818)
15/08/16 12:52:05 INFO Executor: Finished task 174.0 in stage 16.0 (TID 1810). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 183.0 in stage 16.0 (TID 1819, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 183.0 in stage 16.0 (TID 1819)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 184.0 in stage 16.0 (TID 1820, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 169.0 in stage 16.0 (TID 1805). 896 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 184.0 in stage 16.0 (TID 1820)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 171.0 in stage 16.0 (TID 1807) in 22 ms on localhost (166/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 172.0 in stage 16.0 (TID 1808) in 21 ms on localhost (167/200)
15/08/16 12:52:05 INFO Executor: Finished task 166.0 in stage 16.0 (TID 1802). 896 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO TaskSetManager: Starting task 185.0 in stage 16.0 (TID 1821, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 167.0 in stage 16.0 (TID 1803) in 29 ms on localhost (168/200)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 186.0 in stage 16.0 (TID 1822, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 175.0 in stage 16.0 (TID 1811). 1147 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Running task 186.0 in stage 16.0 (TID 1822)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 155.0 in stage 16.0 (TID 1791) in 45 ms on localhost (169/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 170.0 in stage 16.0 (TID 1806). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 168.0 in stage 16.0 (TID 1804) in 29 ms on localhost (170/200)
15/08/16 12:52:05 INFO Executor: Running task 185.0 in stage 16.0 (TID 1821)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 187.0 in stage 16.0 (TID 1823, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 187.0 in stage 16.0 (TID 1823)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 174.0 in stage 16.0 (TID 1810) in 21 ms on localhost (171/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Finished task 176.0 in stage 16.0 (TID 1812). 896 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 188.0 in stage 16.0 (TID 1824, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Running task 188.0 in stage 16.0 (TID 1824)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 189.0 in stage 16.0 (TID 1825, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Running task 189.0 in stage 16.0 (TID 1825)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 169.0 in stage 16.0 (TID 1805) in 29 ms on localhost (172/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 166.0 in stage 16.0 (TID 1802) in 37 ms on localhost (173/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 175.0 in stage 16.0 (TID 1811) in 24 ms on localhost (174/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 190.0 in stage 16.0 (TID 1826, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 190.0 in stage 16.0 (TID 1826)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 176.0 in stage 16.0 (TID 1812) in 25 ms on localhost (175/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Starting task 191.0 in stage 16.0 (TID 1827, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 173.0 in stage 16.0 (TID 1809). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 191.0 in stage 16.0 (TID 1827)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 192.0 in stage 16.0 (TID 1828, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Running task 192.0 in stage 16.0 (TID 1828)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 173.0 in stage 16.0 (TID 1809) in 31 ms on localhost (176/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 170.0 in stage 16.0 (TID 1806) in 36 ms on localhost (177/200)
15/08/16 12:52:05 INFO Executor: Finished task 177.0 in stage 16.0 (TID 1813). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 182.0 in stage 16.0 (TID 1818). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 193.0 in stage 16.0 (TID 1829, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 193.0 in stage 16.0 (TID 1829)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 177.0 in stage 16.0 (TID 1813) in 27 ms on localhost (178/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 182.0 in stage 16.0 (TID 1818) in 17 ms on localhost (179/200)
15/08/16 12:52:05 INFO Executor: Finished task 181.0 in stage 16.0 (TID 1817). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 194.0 in stage 16.0 (TID 1830, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:05 INFO Executor: Running task 194.0 in stage 16.0 (TID 1830)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 195.0 in stage 16.0 (TID 1831, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 184.0 in stage 16.0 (TID 1820). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 195.0 in stage 16.0 (TID 1831)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 183.0 in stage 16.0 (TID 1819). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 186.0 in stage 16.0 (TID 1822). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 180.0 in stage 16.0 (TID 1816). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 196.0 in stage 16.0 (TID 1832, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 196.0 in stage 16.0 (TID 1832)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 197.0 in stage 16.0 (TID 1833, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 198.0 in stage 16.0 (TID 1834, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Finished task 188.0 in stage 16.0 (TID 1824). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Running task 198.0 in stage 16.0 (TID 1834)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 199.0 in stage 16.0 (TID 1835, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:05 INFO Executor: Running task 197.0 in stage 16.0 (TID 1833)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 183.0 in stage 16.0 (TID 1819) in 24 ms on localhost (180/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Running task 199.0 in stage 16.0 (TID 1835)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 179.0 in stage 16.0 (TID 1815). 896 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 178.0 in stage 16.0 (TID 1814). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 181.0 in stage 16.0 (TID 1817) in 26 ms on localhost (181/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 186.0 in stage 16.0 (TID 1822) in 23 ms on localhost (182/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 184.0 in stage 16.0 (TID 1820) in 24 ms on localhost (183/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 180.0 in stage 16.0 (TID 1816) in 27 ms on localhost (184/200)
15/08/16 12:52:05 INFO Executor: Finished task 187.0 in stage 16.0 (TID 1823). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 188.0 in stage 16.0 (TID 1824) in 22 ms on localhost (185/200)
15/08/16 12:52:05 INFO Executor: Finished task 185.0 in stage 16.0 (TID 1821). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 179.0 in stage 16.0 (TID 1815) in 31 ms on localhost (186/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 178.0 in stage 16.0 (TID 1814) in 37 ms on localhost (187/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 187.0 in stage 16.0 (TID 1823) in 25 ms on localhost (188/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 185.0 in stage 16.0 (TID 1821) in 29 ms on localhost (189/200)
15/08/16 12:52:05 INFO Executor: Finished task 193.0 in stage 16.0 (TID 1829). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 194.0 in stage 16.0 (TID 1830). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 193.0 in stage 16.0 (TID 1829) in 17 ms on localhost (190/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 194.0 in stage 16.0 (TID 1830) in 17 ms on localhost (191/200)
15/08/16 12:52:05 INFO Executor: Finished task 191.0 in stage 16.0 (TID 1827). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 191.0 in stage 16.0 (TID 1827) in 25 ms on localhost (192/200)
15/08/16 12:52:05 INFO Executor: Finished task 189.0 in stage 16.0 (TID 1825). 896 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 196.0 in stage 16.0 (TID 1832). 1119 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 197.0 in stage 16.0 (TID 1833). 1032 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 195.0 in stage 16.0 (TID 1831). 896 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 189.0 in stage 16.0 (TID 1825) in 34 ms on localhost (193/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 196.0 in stage 16.0 (TID 1832) in 19 ms on localhost (194/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 197.0 in stage 16.0 (TID 1833) in 19 ms on localhost (195/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 195.0 in stage 16.0 (TID 1831) in 23 ms on localhost (196/200)
15/08/16 12:52:05 INFO Executor: Finished task 199.0 in stage 16.0 (TID 1835). 896 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 198.0 in stage 16.0 (TID 1834). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO Executor: Finished task 192.0 in stage 16.0 (TID 1828). 1060 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 199.0 in stage 16.0 (TID 1835) in 20 ms on localhost (197/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 198.0 in stage 16.0 (TID 1834) in 21 ms on localhost (198/200)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 192.0 in stage 16.0 (TID 1828) in 32 ms on localhost (199/200)
15/08/16 12:52:05 INFO Executor: Finished task 190.0 in stage 16.0 (TID 1826). 1088 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Finished task 190.0 in stage 16.0 (TID 1826) in 40 ms on localhost (200/200)
15/08/16 12:52:05 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
15/08/16 12:52:05 INFO DAGScheduler: ResultStage 16 (processCmd at CliDriver.java:423) finished in 0.362 s
15/08/16 12:52:05 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@338ecf48
15/08/16 12:52:05 INFO DAGScheduler: Job 7 finished: processCmd at CliDriver.java:423, took 25.352433 s
15/08/16 12:52:05 INFO StatsReportListener: task runtime:(count: 200, mean: 30.740000, stdev: 9.619376, max: 59.000000, min: 17.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	17.0 ms	20.0 ms	21.0 ms	23.0 ms	28.0 ms	37.0 ms	45.0 ms	50.0 ms	59.0 ms
15/08/16 12:52:05 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.160000, stdev: 0.429418, max: 2.000000, min: 0.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/16 12:52:05 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:52:05 INFO StatsReportListener: task result size:(count: 200, mean: 1047.100000, stdev: 66.557794, max: 1147.000000, min: 895.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	895.0 B	896.0 B	896.0 B	1032.0 B	1060.0 B	1088.0 B	1119.0 B	1146.0 B	1147.0 B
15/08/16 12:52:05 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 56.342478, stdev: 9.684266, max: 76.000000, min: 27.500000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	28 %	39 %	44 %	52 %	57 %	63 %	68 %	71 %	76 %
15/08/16 12:52:05 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.501382, stdev: 1.370033, max: 6.451613, min: 0.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 3 %	 5 %	 6 %
15/08/16 12:52:05 INFO StatsReportListener: other time pct: (count: 200, mean: 43.156140, stdev: 9.732214, max: 72.500000, min: 24.000000)
15/08/16 12:52:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:05 INFO StatsReportListener: 	24 %	30 %	31 %	36 %	43 %	48 %	56 %	61 %	73 %
15/08/16 12:52:05 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/16 12:52:05 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/16 12:52:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:52:05 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 1216 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 434 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 446 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 271 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 18616 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 1061 bytes
15/08/16 12:52:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 4639 bytes
15/08/16 12:52:05 INFO DAGScheduler: Registering RDD 81 (processCmd at CliDriver.java:423)
15/08/16 12:52:05 INFO DAGScheduler: Registering RDD 86 (processCmd at CliDriver.java:423)
15/08/16 12:52:05 INFO DAGScheduler: Got job 8 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/16 12:52:05 INFO DAGScheduler: Final stage: ResultStage 26(processCmd at CliDriver.java:423)
15/08/16 12:52:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
15/08/16 12:52:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 25)
15/08/16 12:52:05 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[81] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:52:05 INFO MemoryStore: ensureFreeSpace(24304) called with curMem=508754228, maxMem=3333968363
15/08/16 12:52:05 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 23.7 KB, free 2.6 GB)
15/08/16 12:52:05 INFO MemoryStore: ensureFreeSpace(10057) called with curMem=508778532, maxMem=3333968363
15/08/16 12:52:05 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 9.8 KB, free 2.6 GB)
15/08/16 12:52:05 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:36543 (size: 9.8 KB, free: 3.1 GB)
15/08/16 12:52:05 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:874
15/08/16 12:52:05 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[81] at processCmd at CliDriver.java:423)
15/08/16 12:52:05 INFO TaskSchedulerImpl: Adding task set 24.0 with 200 tasks
15/08/16 12:52:05 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 1836, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 1837, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 1838, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 1839, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 1840, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 5.0 in stage 24.0 (TID 1841, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 6.0 in stage 24.0 (TID 1842, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 7.0 in stage 24.0 (TID 1843, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 8.0 in stage 24.0 (TID 1844, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 9.0 in stage 24.0 (TID 1845, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 10.0 in stage 24.0 (TID 1846, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 11.0 in stage 24.0 (TID 1847, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 12.0 in stage 24.0 (TID 1848, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 13.0 in stage 24.0 (TID 1849, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 14.0 in stage 24.0 (TID 1850, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO TaskSetManager: Starting task 15.0 in stage 24.0 (TID 1851, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO Executor: Running task 0.0 in stage 24.0 (TID 1836)
15/08/16 12:52:05 INFO Executor: Running task 1.0 in stage 24.0 (TID 1837)
15/08/16 12:52:05 INFO Executor: Running task 4.0 in stage 24.0 (TID 1840)
15/08/16 12:52:05 INFO Executor: Running task 8.0 in stage 24.0 (TID 1844)
15/08/16 12:52:05 INFO Executor: Running task 2.0 in stage 24.0 (TID 1838)
15/08/16 12:52:05 INFO Executor: Running task 12.0 in stage 24.0 (TID 1848)
15/08/16 12:52:05 INFO Executor: Running task 11.0 in stage 24.0 (TID 1847)
15/08/16 12:52:05 INFO Executor: Running task 14.0 in stage 24.0 (TID 1850)
15/08/16 12:52:05 INFO Executor: Running task 5.0 in stage 24.0 (TID 1841)
15/08/16 12:52:05 INFO Executor: Running task 9.0 in stage 24.0 (TID 1845)
15/08/16 12:52:05 INFO Executor: Running task 3.0 in stage 24.0 (TID 1839)
15/08/16 12:52:05 INFO Executor: Running task 13.0 in stage 24.0 (TID 1849)
15/08/16 12:52:05 INFO Executor: Running task 15.0 in stage 24.0 (TID 1851)
15/08/16 12:52:05 INFO Executor: Running task 10.0 in stage 24.0 (TID 1846)
15/08/16 12:52:05 INFO Executor: Running task 7.0 in stage 24.0 (TID 1843)
15/08/16 12:52:05 INFO Executor: Running task 6.0 in stage 24.0 (TID 1842)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO Executor: Finished task 12.0 in stage 24.0 (TID 1848). 1219 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 16.0 in stage 24.0 (TID 1852, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO Executor: Running task 16.0 in stage 24.0 (TID 1852)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO TaskSetManager: Finished task 12.0 in stage 24.0 (TID 1848) in 42 ms on localhost (1/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:05 INFO Executor: Finished task 16.0 in stage 24.0 (TID 1852). 1219 bytes result sent to driver
15/08/16 12:52:05 INFO TaskSetManager: Starting task 17.0 in stage 24.0 (TID 1853, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:05 INFO Executor: Running task 17.0 in stage 24.0 (TID 1853)
15/08/16 12:52:05 INFO TaskSetManager: Finished task 16.0 in stage 24.0 (TID 1852) in 40 ms on localhost (2/200)
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 10.0 in stage 24.0 (TID 1846). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 18.0 in stage 24.0 (TID 1854, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 18.0 in stage 24.0 (TID 1854)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 10.0 in stage 24.0 (TID 1846) in 673 ms on localhost (3/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 5.0 in stage 24.0 (TID 1841). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO Executor: Finished task 13.0 in stage 24.0 (TID 1849). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 19.0 in stage 24.0 (TID 1855, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 19.0 in stage 24.0 (TID 1855)
15/08/16 12:52:06 INFO TaskSetManager: Starting task 20.0 in stage 24.0 (TID 1856, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 20.0 in stage 24.0 (TID 1856)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 5.0 in stage 24.0 (TID 1841) in 844 ms on localhost (4/200)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 13.0 in stage 24.0 (TID 1849) in 842 ms on localhost (5/200)
15/08/16 12:52:06 INFO Executor: Finished task 3.0 in stage 24.0 (TID 1839). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 21.0 in stage 24.0 (TID 1857, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 21.0 in stage 24.0 (TID 1857)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 1839) in 853 ms on localhost (6/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 15.0 in stage 24.0 (TID 1851). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 22.0 in stage 24.0 (TID 1858, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 22.0 in stage 24.0 (TID 1858)
15/08/16 12:52:06 INFO Executor: Finished task 2.0 in stage 24.0 (TID 1838). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Finished task 15.0 in stage 24.0 (TID 1851) in 862 ms on localhost (7/200)
15/08/16 12:52:06 INFO TaskSetManager: Starting task 23.0 in stage 24.0 (TID 1859, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 23.0 in stage 24.0 (TID 1859)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 1838) in 868 ms on localhost (8/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 20.0 in stage 24.0 (TID 1856). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO TaskSetManager: Starting task 24.0 in stage 24.0 (TID 1860, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 24.0 in stage 24.0 (TID 1860)
15/08/16 12:52:06 INFO Executor: Finished task 0.0 in stage 24.0 (TID 1836). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 25.0 in stage 24.0 (TID 1861, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 20.0 in stage 24.0 (TID 1856) in 34 ms on localhost (9/200)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 1836) in 880 ms on localhost (10/200)
15/08/16 12:52:06 INFO Executor: Running task 25.0 in stage 24.0 (TID 1861)
15/08/16 12:52:06 INFO Executor: Finished task 8.0 in stage 24.0 (TID 1844). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 26.0 in stage 24.0 (TID 1862, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 26.0 in stage 24.0 (TID 1862)
15/08/16 12:52:06 INFO Executor: Finished task 4.0 in stage 24.0 (TID 1840). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Finished task 8.0 in stage 24.0 (TID 1844) in 882 ms on localhost (11/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO TaskSetManager: Starting task 27.0 in stage 24.0 (TID 1863, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 27.0 in stage 24.0 (TID 1863)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 1840) in 886 ms on localhost (12/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 11.0 in stage 24.0 (TID 1847). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 28.0 in stage 24.0 (TID 1864, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 28.0 in stage 24.0 (TID 1864)
15/08/16 12:52:06 INFO Executor: Finished task 17.0 in stage 24.0 (TID 1853). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 29.0 in stage 24.0 (TID 1865, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 11.0 in stage 24.0 (TID 1847) in 910 ms on localhost (13/200)
15/08/16 12:52:06 INFO Executor: Finished task 7.0 in stage 24.0 (TID 1843). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO Executor: Running task 29.0 in stage 24.0 (TID 1865)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 17.0 in stage 24.0 (TID 1853) in 833 ms on localhost (14/200)
15/08/16 12:52:06 INFO Executor: Finished task 18.0 in stage 24.0 (TID 1854). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 30.0 in stage 24.0 (TID 1866, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 30.0 in stage 24.0 (TID 1866)
15/08/16 12:52:06 INFO TaskSetManager: Starting task 31.0 in stage 24.0 (TID 1867, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 7.0 in stage 24.0 (TID 1843) in 915 ms on localhost (15/200)
15/08/16 12:52:06 INFO Executor: Finished task 9.0 in stage 24.0 (TID 1845). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 32.0 in stage 24.0 (TID 1868, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 18.0 in stage 24.0 (TID 1854) in 248 ms on localhost (16/200)
15/08/16 12:52:06 INFO Executor: Running task 32.0 in stage 24.0 (TID 1868)
15/08/16 12:52:06 INFO Executor: Finished task 1.0 in stage 24.0 (TID 1837). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO Executor: Finished task 14.0 in stage 24.0 (TID 1850). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Finished task 9.0 in stage 24.0 (TID 1845) in 916 ms on localhost (17/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO Executor: Running task 31.0 in stage 24.0 (TID 1867)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO TaskSetManager: Starting task 33.0 in stage 24.0 (TID 1869, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Starting task 34.0 in stage 24.0 (TID 1870, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 1837) in 920 ms on localhost (18/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO Executor: Running task 34.0 in stage 24.0 (TID 1870)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO TaskSetManager: Finished task 14.0 in stage 24.0 (TID 1850) in 920 ms on localhost (19/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Running task 33.0 in stage 24.0 (TID 1869)
15/08/16 12:52:06 INFO Executor: Finished task 6.0 in stage 24.0 (TID 1842). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO TaskSetManager: Starting task 35.0 in stage 24.0 (TID 1871, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Running task 35.0 in stage 24.0 (TID 1871)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 6.0 in stage 24.0 (TID 1842) in 931 ms on localhost (20/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 34.0 in stage 24.0 (TID 1870). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 36.0 in stage 24.0 (TID 1872, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 36.0 in stage 24.0 (TID 1872)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 34.0 in stage 24.0 (TID 1870) in 39 ms on localhost (21/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 28.0 in stage 24.0 (TID 1864). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 37.0 in stage 24.0 (TID 1873, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 37.0 in stage 24.0 (TID 1873)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 28.0 in stage 24.0 (TID 1864) in 62 ms on localhost (22/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 19.0 in stage 24.0 (TID 1855). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 38.0 in stage 24.0 (TID 1874, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 38.0 in stage 24.0 (TID 1874)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 19.0 in stage 24.0 (TID 1855) in 149 ms on localhost (23/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 22.0 in stage 24.0 (TID 1858). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 39.0 in stage 24.0 (TID 1875, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 39.0 in stage 24.0 (TID 1875)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 22.0 in stage 24.0 (TID 1858) in 321 ms on localhost (24/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 23.0 in stage 24.0 (TID 1859). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 40.0 in stage 24.0 (TID 1876, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 40.0 in stage 24.0 (TID 1876)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 23.0 in stage 24.0 (TID 1859) in 343 ms on localhost (25/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 21.0 in stage 24.0 (TID 1857). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 41.0 in stage 24.0 (TID 1877, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 41.0 in stage 24.0 (TID 1877)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 21.0 in stage 24.0 (TID 1857) in 402 ms on localhost (26/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 27.0 in stage 24.0 (TID 1863). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 42.0 in stage 24.0 (TID 1878, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 42.0 in stage 24.0 (TID 1878)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 27.0 in stage 24.0 (TID 1863) in 474 ms on localhost (27/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 26.0 in stage 24.0 (TID 1862). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 43.0 in stage 24.0 (TID 1879, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 43.0 in stage 24.0 (TID 1879)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 26.0 in stage 24.0 (TID 1862) in 514 ms on localhost (28/200)
15/08/16 12:52:06 INFO Executor: Finished task 24.0 in stage 24.0 (TID 1860). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 44.0 in stage 24.0 (TID 1880, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 44.0 in stage 24.0 (TID 1880)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO TaskSetManager: Finished task 24.0 in stage 24.0 (TID 1860) in 529 ms on localhost (29/200)
15/08/16 12:52:06 INFO Executor: Finished task 25.0 in stage 24.0 (TID 1861). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 45.0 in stage 24.0 (TID 1881, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 45.0 in stage 24.0 (TID 1881)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 32.0 in stage 24.0 (TID 1868). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 46.0 in stage 24.0 (TID 1882, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 46.0 in stage 24.0 (TID 1882)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 25.0 in stage 24.0 (TID 1861) in 534 ms on localhost (30/200)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 32.0 in stage 24.0 (TID 1868) in 497 ms on localhost (31/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:52:06 INFO Executor: Finished task 29.0 in stage 24.0 (TID 1865). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 47.0 in stage 24.0 (TID 1883, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Finished task 31.0 in stage 24.0 (TID 1867). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO Executor: Running task 47.0 in stage 24.0 (TID 1883)
15/08/16 12:52:06 INFO Executor: Finished task 35.0 in stage 24.0 (TID 1871). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO Executor: Finished task 36.0 in stage 24.0 (TID 1872). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 48.0 in stage 24.0 (TID 1884, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO TaskSetManager: Starting task 49.0 in stage 24.0 (TID 1885, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO TaskSetManager: Starting task 50.0 in stage 24.0 (TID 1886, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 49.0 in stage 24.0 (TID 1885)
15/08/16 12:52:06 INFO Executor: Running task 50.0 in stage 24.0 (TID 1886)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 31.0 in stage 24.0 (TID 1867) in 530 ms on localhost (32/200)
15/08/16 12:52:06 INFO Executor: Running task 48.0 in stage 24.0 (TID 1884)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 29.0 in stage 24.0 (TID 1865) in 535 ms on localhost (33/200)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 35.0 in stage 24.0 (TID 1871) in 517 ms on localhost (34/200)
15/08/16 12:52:06 INFO Executor: Finished task 44.0 in stage 24.0 (TID 1880). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 51.0 in stage 24.0 (TID 1887, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 51.0 in stage 24.0 (TID 1887)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 36.0 in stage 24.0 (TID 1872) in 496 ms on localhost (35/200)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 44.0 in stage 24.0 (TID 1880) in 50 ms on localhost (36/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 30.0 in stage 24.0 (TID 1866). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 52.0 in stage 24.0 (TID 1888, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 52.0 in stage 24.0 (TID 1888)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 30.0 in stage 24.0 (TID 1866) in 548 ms on localhost (37/200)
15/08/16 12:52:06 INFO Executor: Finished task 33.0 in stage 24.0 (TID 1869). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 53.0 in stage 24.0 (TID 1889, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 53.0 in stage 24.0 (TID 1889)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 33.0 in stage 24.0 (TID 1869) in 547 ms on localhost (38/200)
15/08/16 12:52:06 INFO Executor: Finished task 37.0 in stage 24.0 (TID 1873). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 54.0 in stage 24.0 (TID 1890, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 54.0 in stage 24.0 (TID 1890)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO TaskSetManager: Finished task 37.0 in stage 24.0 (TID 1873) in 501 ms on localhost (39/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 38.0 in stage 24.0 (TID 1874). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 55.0 in stage 24.0 (TID 1891, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 55.0 in stage 24.0 (TID 1891)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO TaskSetManager: Finished task 38.0 in stage 24.0 (TID 1874) in 491 ms on localhost (40/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 39.0 in stage 24.0 (TID 1875). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 56.0 in stage 24.0 (TID 1892, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 56.0 in stage 24.0 (TID 1892)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 39.0 in stage 24.0 (TID 1875) in 357 ms on localhost (41/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:06 INFO Executor: Finished task 40.0 in stage 24.0 (TID 1876). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 57.0 in stage 24.0 (TID 1893, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 57.0 in stage 24.0 (TID 1893)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 40.0 in stage 24.0 (TID 1876) in 364 ms on localhost (42/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:06 INFO Executor: Finished task 41.0 in stage 24.0 (TID 1877). 1219 bytes result sent to driver
15/08/16 12:52:06 INFO TaskSetManager: Starting task 58.0 in stage 24.0 (TID 1894, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:06 INFO Executor: Running task 58.0 in stage 24.0 (TID 1894)
15/08/16 12:52:06 INFO TaskSetManager: Finished task 41.0 in stage 24.0 (TID 1877) in 364 ms on localhost (43/200)
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 42.0 in stage 24.0 (TID 1878). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 59.0 in stage 24.0 (TID 1895, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 59.0 in stage 24.0 (TID 1895)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 42.0 in stage 24.0 (TID 1878) in 379 ms on localhost (44/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 59.0 in stage 24.0 (TID 1895). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 60.0 in stage 24.0 (TID 1896, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 60.0 in stage 24.0 (TID 1896)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 59.0 in stage 24.0 (TID 1895) in 41 ms on localhost (45/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 43.0 in stage 24.0 (TID 1879). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 61.0 in stage 24.0 (TID 1897, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 61.0 in stage 24.0 (TID 1897)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 43.0 in stage 24.0 (TID 1879) in 420 ms on localhost (46/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 45.0 in stage 24.0 (TID 1881). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 62.0 in stage 24.0 (TID 1898, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 62.0 in stage 24.0 (TID 1898)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 45.0 in stage 24.0 (TID 1881) in 440 ms on localhost (47/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 46.0 in stage 24.0 (TID 1882). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 63.0 in stage 24.0 (TID 1899, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 63.0 in stage 24.0 (TID 1899)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 46.0 in stage 24.0 (TID 1882) in 487 ms on localhost (48/200)
15/08/16 12:52:07 INFO Executor: Finished task 51.0 in stage 24.0 (TID 1887). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 64.0 in stage 24.0 (TID 1900, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 64.0 in stage 24.0 (TID 1900)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 51.0 in stage 24.0 (TID 1887) in 451 ms on localhost (49/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 47.0 in stage 24.0 (TID 1883). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 65.0 in stage 24.0 (TID 1901, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 65.0 in stage 24.0 (TID 1901)
15/08/16 12:52:07 INFO Executor: Finished task 48.0 in stage 24.0 (TID 1884). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 66.0 in stage 24.0 (TID 1902, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 47.0 in stage 24.0 (TID 1883) in 474 ms on localhost (50/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 48.0 in stage 24.0 (TID 1884) in 468 ms on localhost (51/200)
15/08/16 12:52:07 INFO Executor: Running task 66.0 in stage 24.0 (TID 1902)
15/08/16 12:52:07 INFO Executor: Finished task 50.0 in stage 24.0 (TID 1886). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 67.0 in stage 24.0 (TID 1903, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 67.0 in stage 24.0 (TID 1903)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 50.0 in stage 24.0 (TID 1886) in 470 ms on localhost (52/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 49.0 in stage 24.0 (TID 1885). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 68.0 in stage 24.0 (TID 1904, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 68.0 in stage 24.0 (TID 1904)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 49.0 in stage 24.0 (TID 1885) in 497 ms on localhost (53/200)
15/08/16 12:52:07 INFO Executor: Finished task 53.0 in stage 24.0 (TID 1889). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 69.0 in stage 24.0 (TID 1905, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 69.0 in stage 24.0 (TID 1905)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 53.0 in stage 24.0 (TID 1889) in 483 ms on localhost (54/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 54.0 in stage 24.0 (TID 1890). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 70.0 in stage 24.0 (TID 1906, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 70.0 in stage 24.0 (TID 1906)
15/08/16 12:52:07 INFO Executor: Finished task 67.0 in stage 24.0 (TID 1903). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO Executor: Finished task 52.0 in stage 24.0 (TID 1888). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 71.0 in stage 24.0 (TID 1907, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 54.0 in stage 24.0 (TID 1890) in 484 ms on localhost (55/200)
15/08/16 12:52:07 INFO Executor: Running task 71.0 in stage 24.0 (TID 1907)
15/08/16 12:52:07 INFO TaskSetManager: Starting task 72.0 in stage 24.0 (TID 1908, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Running task 72.0 in stage 24.0 (TID 1908)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 67.0 in stage 24.0 (TID 1903) in 39 ms on localhost (56/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 52.0 in stage 24.0 (TID 1888) in 494 ms on localhost (57/200)
15/08/16 12:52:07 INFO Executor: Finished task 55.0 in stage 24.0 (TID 1891). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 73.0 in stage 24.0 (TID 1909, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 73.0 in stage 24.0 (TID 1909)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 55.0 in stage 24.0 (TID 1891) in 482 ms on localhost (58/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 56.0 in stage 24.0 (TID 1892). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 74.0 in stage 24.0 (TID 1910, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 74.0 in stage 24.0 (TID 1910)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 56.0 in stage 24.0 (TID 1892) in 447 ms on localhost (59/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 58.0 in stage 24.0 (TID 1894). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO Executor: Finished task 57.0 in stage 24.0 (TID 1893). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 75.0 in stage 24.0 (TID 1911, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 75.0 in stage 24.0 (TID 1911)
15/08/16 12:52:07 INFO TaskSetManager: Starting task 76.0 in stage 24.0 (TID 1912, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 76.0 in stage 24.0 (TID 1912)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 58.0 in stage 24.0 (TID 1894) in 388 ms on localhost (60/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 57.0 in stage 24.0 (TID 1893) in 433 ms on localhost (61/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 60.0 in stage 24.0 (TID 1896). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 77.0 in stage 24.0 (TID 1913, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 77.0 in stage 24.0 (TID 1913)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 60.0 in stage 24.0 (TID 1896) in 324 ms on localhost (62/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 61.0 in stage 24.0 (TID 1897). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 78.0 in stage 24.0 (TID 1914, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 78.0 in stage 24.0 (TID 1914)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 61.0 in stage 24.0 (TID 1897) in 313 ms on localhost (63/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 62.0 in stage 24.0 (TID 1898). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 79.0 in stage 24.0 (TID 1915, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 79.0 in stage 24.0 (TID 1915)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 62.0 in stage 24.0 (TID 1898) in 368 ms on localhost (64/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 64.0 in stage 24.0 (TID 1900). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 80.0 in stage 24.0 (TID 1916, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 80.0 in stage 24.0 (TID 1916)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 64.0 in stage 24.0 (TID 1900) in 406 ms on localhost (65/200)
15/08/16 12:52:07 INFO Executor: Finished task 63.0 in stage 24.0 (TID 1899). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 81.0 in stage 24.0 (TID 1917, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 81.0 in stage 24.0 (TID 1917)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO TaskSetManager: Finished task 63.0 in stage 24.0 (TID 1899) in 416 ms on localhost (66/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 65.0 in stage 24.0 (TID 1901). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 82.0 in stage 24.0 (TID 1918, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 82.0 in stage 24.0 (TID 1918)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 65.0 in stage 24.0 (TID 1901) in 511 ms on localhost (67/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 66.0 in stage 24.0 (TID 1902). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 83.0 in stage 24.0 (TID 1919, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 83.0 in stage 24.0 (TID 1919)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 66.0 in stage 24.0 (TID 1902) in 522 ms on localhost (68/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 69.0 in stage 24.0 (TID 1905). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 84.0 in stage 24.0 (TID 1920, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 84.0 in stage 24.0 (TID 1920)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 69.0 in stage 24.0 (TID 1905) in 495 ms on localhost (69/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 72.0 in stage 24.0 (TID 1908). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 85.0 in stage 24.0 (TID 1921, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 85.0 in stage 24.0 (TID 1921)
15/08/16 12:52:07 INFO Executor: Finished task 68.0 in stage 24.0 (TID 1904). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Finished task 72.0 in stage 24.0 (TID 1908) in 537 ms on localhost (70/200)
15/08/16 12:52:07 INFO TaskSetManager: Starting task 86.0 in stage 24.0 (TID 1922, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 86.0 in stage 24.0 (TID 1922)
15/08/16 12:52:07 INFO Executor: Finished task 73.0 in stage 24.0 (TID 1909). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 87.0 in stage 24.0 (TID 1923, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 87.0 in stage 24.0 (TID 1923)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 68.0 in stage 24.0 (TID 1904) in 555 ms on localhost (71/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 73.0 in stage 24.0 (TID 1909) in 539 ms on localhost (72/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 70.0 in stage 24.0 (TID 1906). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO TaskSetManager: Starting task 88.0 in stage 24.0 (TID 1924, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 88.0 in stage 24.0 (TID 1924)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 70.0 in stage 24.0 (TID 1906) in 551 ms on localhost (73/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 74.0 in stage 24.0 (TID 1910). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO Executor: Finished task 76.0 in stage 24.0 (TID 1912). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO Executor: Finished task 75.0 in stage 24.0 (TID 1911). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 89.0 in stage 24.0 (TID 1925, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 89.0 in stage 24.0 (TID 1925)
15/08/16 12:52:07 INFO TaskSetManager: Starting task 90.0 in stage 24.0 (TID 1926, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 90.0 in stage 24.0 (TID 1926)
15/08/16 12:52:07 INFO TaskSetManager: Starting task 91.0 in stage 24.0 (TID 1927, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 91.0 in stage 24.0 (TID 1927)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 74.0 in stage 24.0 (TID 1910) in 531 ms on localhost (74/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 76.0 in stage 24.0 (TID 1912) in 512 ms on localhost (75/200)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 75.0 in stage 24.0 (TID 1911) in 513 ms on localhost (76/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 71.0 in stage 24.0 (TID 1907). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 92.0 in stage 24.0 (TID 1928, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 92.0 in stage 24.0 (TID 1928)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 71.0 in stage 24.0 (TID 1907) in 566 ms on localhost (77/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO Executor: Finished task 77.0 in stage 24.0 (TID 1913). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 93.0 in stage 24.0 (TID 1929, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 93.0 in stage 24.0 (TID 1929)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 77.0 in stage 24.0 (TID 1913) in 435 ms on localhost (78/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 78.0 in stage 24.0 (TID 1914). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 94.0 in stage 24.0 (TID 1930, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 94.0 in stage 24.0 (TID 1930)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 78.0 in stage 24.0 (TID 1914) in 436 ms on localhost (79/200)
15/08/16 12:52:07 INFO Executor: Finished task 93.0 in stage 24.0 (TID 1929). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 95.0 in stage 24.0 (TID 1931, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 95.0 in stage 24.0 (TID 1931)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO TaskSetManager: Finished task 93.0 in stage 24.0 (TID 1929) in 31 ms on localhost (80/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:07 INFO Executor: Finished task 79.0 in stage 24.0 (TID 1915). 1219 bytes result sent to driver
15/08/16 12:52:07 INFO TaskSetManager: Starting task 96.0 in stage 24.0 (TID 1932, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:07 INFO Executor: Running task 96.0 in stage 24.0 (TID 1932)
15/08/16 12:52:07 INFO TaskSetManager: Finished task 79.0 in stage 24.0 (TID 1915) in 373 ms on localhost (81/200)
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 80.0 in stage 24.0 (TID 1916). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO Executor: Finished task 81.0 in stage 24.0 (TID 1917). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 97.0 in stage 24.0 (TID 1933, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 97.0 in stage 24.0 (TID 1933)
15/08/16 12:52:08 INFO TaskSetManager: Starting task 98.0 in stage 24.0 (TID 1934, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 98.0 in stage 24.0 (TID 1934)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 80.0 in stage 24.0 (TID 1916) in 365 ms on localhost (82/200)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 81.0 in stage 24.0 (TID 1917) in 357 ms on localhost (83/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 83.0 in stage 24.0 (TID 1919). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 99.0 in stage 24.0 (TID 1935, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 99.0 in stage 24.0 (TID 1935)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 83.0 in stage 24.0 (TID 1919) in 368 ms on localhost (84/200)
15/08/16 12:52:08 INFO Executor: Finished task 82.0 in stage 24.0 (TID 1918). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 100.0 in stage 24.0 (TID 1936, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 100.0 in stage 24.0 (TID 1936)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 82.0 in stage 24.0 (TID 1918) in 389 ms on localhost (85/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 84.0 in stage 24.0 (TID 1920). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 101.0 in stage 24.0 (TID 1937, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 101.0 in stage 24.0 (TID 1937)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 84.0 in stage 24.0 (TID 1920) in 417 ms on localhost (86/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 87.0 in stage 24.0 (TID 1923). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 102.0 in stage 24.0 (TID 1938, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 102.0 in stage 24.0 (TID 1938)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 87.0 in stage 24.0 (TID 1923) in 423 ms on localhost (87/200)
15/08/16 12:52:08 INFO Executor: Finished task 86.0 in stage 24.0 (TID 1922). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 103.0 in stage 24.0 (TID 1939, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 103.0 in stage 24.0 (TID 1939)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 86.0 in stage 24.0 (TID 1922) in 430 ms on localhost (88/200)
15/08/16 12:52:08 INFO Executor: Finished task 85.0 in stage 24.0 (TID 1921). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 104.0 in stage 24.0 (TID 1940, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 104.0 in stage 24.0 (TID 1940)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 85.0 in stage 24.0 (TID 1921) in 437 ms on localhost (89/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 90.0 in stage 24.0 (TID 1926). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO Executor: Finished task 88.0 in stage 24.0 (TID 1924). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 105.0 in stage 24.0 (TID 1941, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 105.0 in stage 24.0 (TID 1941)
15/08/16 12:52:08 INFO TaskSetManager: Starting task 106.0 in stage 24.0 (TID 1942, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 106.0 in stage 24.0 (TID 1942)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 88.0 in stage 24.0 (TID 1924) in 435 ms on localhost (90/200)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 90.0 in stage 24.0 (TID 1926) in 427 ms on localhost (91/200)
15/08/16 12:52:08 INFO Executor: Finished task 91.0 in stage 24.0 (TID 1927). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 107.0 in stage 24.0 (TID 1943, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Finished task 89.0 in stage 24.0 (TID 1925). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO Executor: Running task 107.0 in stage 24.0 (TID 1943)
15/08/16 12:52:08 INFO TaskSetManager: Starting task 108.0 in stage 24.0 (TID 1944, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 108.0 in stage 24.0 (TID 1944)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 91.0 in stage 24.0 (TID 1927) in 429 ms on localhost (92/200)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 89.0 in stage 24.0 (TID 1925) in 430 ms on localhost (93/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 92.0 in stage 24.0 (TID 1928). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 109.0 in stage 24.0 (TID 1945, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 109.0 in stage 24.0 (TID 1945)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 92.0 in stage 24.0 (TID 1928) in 441 ms on localhost (94/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 94.0 in stage 24.0 (TID 1930). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 110.0 in stage 24.0 (TID 1946, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 110.0 in stage 24.0 (TID 1946)
15/08/16 12:52:08 INFO Executor: Finished task 108.0 in stage 24.0 (TID 1944). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 111.0 in stage 24.0 (TID 1947, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 94.0 in stage 24.0 (TID 1930) in 425 ms on localhost (95/200)
15/08/16 12:52:08 INFO Executor: Running task 111.0 in stage 24.0 (TID 1947)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 108.0 in stage 24.0 (TID 1944) in 35 ms on localhost (96/200)
15/08/16 12:52:08 INFO Executor: Finished task 95.0 in stage 24.0 (TID 1931). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 112.0 in stage 24.0 (TID 1948, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 112.0 in stage 24.0 (TID 1948)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO TaskSetManager: Finished task 95.0 in stage 24.0 (TID 1931) in 680 ms on localhost (97/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 96.0 in stage 24.0 (TID 1932). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 113.0 in stage 24.0 (TID 1949, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 113.0 in stage 24.0 (TID 1949)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 96.0 in stage 24.0 (TID 1932) in 665 ms on localhost (98/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 112.0 in stage 24.0 (TID 1948). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 114.0 in stage 24.0 (TID 1950, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 114.0 in stage 24.0 (TID 1950)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 112.0 in stage 24.0 (TID 1948) in 36 ms on localhost (99/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 97.0 in stage 24.0 (TID 1933). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO Executor: Finished task 98.0 in stage 24.0 (TID 1934). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 115.0 in stage 24.0 (TID 1951, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 115.0 in stage 24.0 (TID 1951)
15/08/16 12:52:08 INFO TaskSetManager: Starting task 116.0 in stage 24.0 (TID 1952, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 116.0 in stage 24.0 (TID 1952)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 97.0 in stage 24.0 (TID 1933) in 628 ms on localhost (100/200)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 98.0 in stage 24.0 (TID 1934) in 628 ms on localhost (101/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 100.0 in stage 24.0 (TID 1936). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 117.0 in stage 24.0 (TID 1953, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 117.0 in stage 24.0 (TID 1953)
15/08/16 12:52:08 INFO Executor: Finished task 99.0 in stage 24.0 (TID 1935). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 118.0 in stage 24.0 (TID 1954, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 118.0 in stage 24.0 (TID 1954)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 100.0 in stage 24.0 (TID 1936) in 627 ms on localhost (102/200)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 99.0 in stage 24.0 (TID 1935) in 635 ms on localhost (103/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:08 INFO Executor: Finished task 101.0 in stage 24.0 (TID 1937). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 119.0 in stage 24.0 (TID 1955, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 119.0 in stage 24.0 (TID 1955)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 101.0 in stage 24.0 (TID 1937) in 628 ms on localhost (104/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 102.0 in stage 24.0 (TID 1938). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 120.0 in stage 24.0 (TID 1956, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 120.0 in stage 24.0 (TID 1956)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 102.0 in stage 24.0 (TID 1938) in 685 ms on localhost (105/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO Executor: Finished task 103.0 in stage 24.0 (TID 1939). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 121.0 in stage 24.0 (TID 1957, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 121.0 in stage 24.0 (TID 1957)
15/08/16 12:52:08 INFO TaskSetManager: Finished task 103.0 in stage 24.0 (TID 1939) in 694 ms on localhost (106/200)
15/08/16 12:52:08 INFO Executor: Finished task 104.0 in stage 24.0 (TID 1940). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 122.0 in stage 24.0 (TID 1958, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 122.0 in stage 24.0 (TID 1958)
15/08/16 12:52:08 INFO Executor: Finished task 105.0 in stage 24.0 (TID 1941). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Finished task 104.0 in stage 24.0 (TID 1940) in 693 ms on localhost (107/200)
15/08/16 12:52:08 INFO TaskSetManager: Starting task 123.0 in stage 24.0 (TID 1959, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 123.0 in stage 24.0 (TID 1959)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO TaskSetManager: Finished task 105.0 in stage 24.0 (TID 1941) in 684 ms on localhost (108/200)
15/08/16 12:52:08 INFO Executor: Finished task 106.0 in stage 24.0 (TID 1942). 1219 bytes result sent to driver
15/08/16 12:52:08 INFO TaskSetManager: Starting task 124.0 in stage 24.0 (TID 1960, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:08 INFO Executor: Running task 124.0 in stage 24.0 (TID 1960)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO TaskSetManager: Finished task 106.0 in stage 24.0 (TID 1942) in 690 ms on localhost (109/200)
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 121.0 in stage 24.0 (TID 1957). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 125.0 in stage 24.0 (TID 1961, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 125.0 in stage 24.0 (TID 1961)
15/08/16 12:52:09 INFO Executor: Finished task 107.0 in stage 24.0 (TID 1943). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 126.0 in stage 24.0 (TID 1962, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 126.0 in stage 24.0 (TID 1962)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 121.0 in stage 24.0 (TID 1957) in 41 ms on localhost (110/200)
15/08/16 12:52:09 INFO Executor: Finished task 123.0 in stage 24.0 (TID 1959). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Finished task 107.0 in stage 24.0 (TID 1943) in 715 ms on localhost (111/200)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 127.0 in stage 24.0 (TID 1963, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 127.0 in stage 24.0 (TID 1963)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 123.0 in stage 24.0 (TID 1959) in 39 ms on localhost (112/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 110.0 in stage 24.0 (TID 1946). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO Executor: Finished task 111.0 in stage 24.0 (TID 1947). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 128.0 in stage 24.0 (TID 1964, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Finished task 109.0 in stage 24.0 (TID 1945). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO Executor: Running task 128.0 in stage 24.0 (TID 1964)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 129.0 in stage 24.0 (TID 1965, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 129.0 in stage 24.0 (TID 1965)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 110.0 in stage 24.0 (TID 1946) in 702 ms on localhost (113/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 109.0 in stage 24.0 (TID 1945) in 717 ms on localhost (114/200)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 130.0 in stage 24.0 (TID 1966, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 130.0 in stage 24.0 (TID 1966)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 111.0 in stage 24.0 (TID 1947) in 702 ms on localhost (115/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 114.0 in stage 24.0 (TID 1950). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 131.0 in stage 24.0 (TID 1967, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 131.0 in stage 24.0 (TID 1967)
15/08/16 12:52:09 INFO Executor: Finished task 113.0 in stage 24.0 (TID 1949). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Finished task 114.0 in stage 24.0 (TID 1950) in 427 ms on localhost (116/200)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 132.0 in stage 24.0 (TID 1968, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 132.0 in stage 24.0 (TID 1968)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 113.0 in stage 24.0 (TID 1949) in 453 ms on localhost (117/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 115.0 in stage 24.0 (TID 1951). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 133.0 in stage 24.0 (TID 1969, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 133.0 in stage 24.0 (TID 1969)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 115.0 in stage 24.0 (TID 1951) in 422 ms on localhost (118/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 116.0 in stage 24.0 (TID 1952). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 134.0 in stage 24.0 (TID 1970, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 134.0 in stage 24.0 (TID 1970)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 116.0 in stage 24.0 (TID 1952) in 434 ms on localhost (119/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 118.0 in stage 24.0 (TID 1954). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO Executor: Finished task 117.0 in stage 24.0 (TID 1953). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 135.0 in stage 24.0 (TID 1971, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 135.0 in stage 24.0 (TID 1971)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 136.0 in stage 24.0 (TID 1972, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 136.0 in stage 24.0 (TID 1972)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 118.0 in stage 24.0 (TID 1954) in 397 ms on localhost (120/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 117.0 in stage 24.0 (TID 1953) in 399 ms on localhost (121/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 119.0 in stage 24.0 (TID 1955). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 137.0 in stage 24.0 (TID 1973, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 137.0 in stage 24.0 (TID 1973)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 119.0 in stage 24.0 (TID 1955) in 381 ms on localhost (122/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 120.0 in stage 24.0 (TID 1956). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 138.0 in stage 24.0 (TID 1974, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 138.0 in stage 24.0 (TID 1974)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 120.0 in stage 24.0 (TID 1956) in 411 ms on localhost (123/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 124.0 in stage 24.0 (TID 1960). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 139.0 in stage 24.0 (TID 1975, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 139.0 in stage 24.0 (TID 1975)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 124.0 in stage 24.0 (TID 1960) in 411 ms on localhost (124/200)
15/08/16 12:52:09 INFO Executor: Finished task 122.0 in stage 24.0 (TID 1958). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 140.0 in stage 24.0 (TID 1976, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 140.0 in stage 24.0 (TID 1976)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 122.0 in stage 24.0 (TID 1958) in 425 ms on localhost (125/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 126.0 in stage 24.0 (TID 1962). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 141.0 in stage 24.0 (TID 1977, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 141.0 in stage 24.0 (TID 1977)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 126.0 in stage 24.0 (TID 1962) in 434 ms on localhost (126/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 125.0 in stage 24.0 (TID 1961). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 142.0 in stage 24.0 (TID 1978, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 142.0 in stage 24.0 (TID 1978)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 125.0 in stage 24.0 (TID 1961) in 448 ms on localhost (127/200)
15/08/16 12:52:09 INFO Executor: Finished task 127.0 in stage 24.0 (TID 1963). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO TaskSetManager: Starting task 143.0 in stage 24.0 (TID 1979, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 143.0 in stage 24.0 (TID 1979)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 127.0 in stage 24.0 (TID 1963) in 455 ms on localhost (128/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 128.0 in stage 24.0 (TID 1964). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 144.0 in stage 24.0 (TID 1980, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 144.0 in stage 24.0 (TID 1980)
15/08/16 12:52:09 INFO Executor: Finished task 129.0 in stage 24.0 (TID 1965). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 145.0 in stage 24.0 (TID 1981, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 145.0 in stage 24.0 (TID 1981)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 128.0 in stage 24.0 (TID 1964) in 453 ms on localhost (129/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 129.0 in stage 24.0 (TID 1965) in 449 ms on localhost (130/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 130.0 in stage 24.0 (TID 1966). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 146.0 in stage 24.0 (TID 1982, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO TaskSetManager: Finished task 130.0 in stage 24.0 (TID 1966) in 452 ms on localhost (131/200)
15/08/16 12:52:09 INFO Executor: Running task 146.0 in stage 24.0 (TID 1982)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 143.0 in stage 24.0 (TID 1979). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 147.0 in stage 24.0 (TID 1983, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 147.0 in stage 24.0 (TID 1983)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 143.0 in stage 24.0 (TID 1979) in 33 ms on localhost (132/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 132.0 in stage 24.0 (TID 1968). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 148.0 in stage 24.0 (TID 1984, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 148.0 in stage 24.0 (TID 1984)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 132.0 in stage 24.0 (TID 1968) in 459 ms on localhost (133/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO Executor: Finished task 131.0 in stage 24.0 (TID 1967). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO TaskSetManager: Starting task 149.0 in stage 24.0 (TID 1985, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 149.0 in stage 24.0 (TID 1985)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 131.0 in stage 24.0 (TID 1967) in 470 ms on localhost (134/200)
15/08/16 12:52:09 INFO Executor: Finished task 133.0 in stage 24.0 (TID 1969). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 150.0 in stage 24.0 (TID 1986, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 150.0 in stage 24.0 (TID 1986)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 133.0 in stage 24.0 (TID 1969) in 455 ms on localhost (135/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 134.0 in stage 24.0 (TID 1970). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 151.0 in stage 24.0 (TID 1987, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 151.0 in stage 24.0 (TID 1987)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 134.0 in stage 24.0 (TID 1970) in 454 ms on localhost (136/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 135.0 in stage 24.0 (TID 1971). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 152.0 in stage 24.0 (TID 1988, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 152.0 in stage 24.0 (TID 1988)
15/08/16 12:52:09 INFO Executor: Finished task 136.0 in stage 24.0 (TID 1972). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 153.0 in stage 24.0 (TID 1989, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 153.0 in stage 24.0 (TID 1989)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 135.0 in stage 24.0 (TID 1971) in 440 ms on localhost (137/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 136.0 in stage 24.0 (TID 1972) in 441 ms on localhost (138/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 137.0 in stage 24.0 (TID 1973). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 154.0 in stage 24.0 (TID 1990, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 154.0 in stage 24.0 (TID 1990)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 137.0 in stage 24.0 (TID 1973) in 435 ms on localhost (139/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 154.0 in stage 24.0 (TID 1990). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 155.0 in stage 24.0 (TID 1991, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 155.0 in stage 24.0 (TID 1991)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 154.0 in stage 24.0 (TID 1990) in 42 ms on localhost (140/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 155.0 in stage 24.0 (TID 1991). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 156.0 in stage 24.0 (TID 1992, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 156.0 in stage 24.0 (TID 1992)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 155.0 in stage 24.0 (TID 1991) in 38 ms on localhost (141/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 138.0 in stage 24.0 (TID 1974). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 157.0 in stage 24.0 (TID 1993, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 157.0 in stage 24.0 (TID 1993)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 138.0 in stage 24.0 (TID 1974) in 405 ms on localhost (142/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 140.0 in stage 24.0 (TID 1976). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 158.0 in stage 24.0 (TID 1994, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 158.0 in stage 24.0 (TID 1994)
15/08/16 12:52:09 INFO Executor: Finished task 139.0 in stage 24.0 (TID 1975). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 159.0 in stage 24.0 (TID 1995, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 140.0 in stage 24.0 (TID 1976) in 407 ms on localhost (143/200)
15/08/16 12:52:09 INFO Executor: Running task 159.0 in stage 24.0 (TID 1995)
15/08/16 12:52:09 INFO Executor: Finished task 157.0 in stage 24.0 (TID 1993). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 160.0 in stage 24.0 (TID 1996, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 160.0 in stage 24.0 (TID 1996)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 139.0 in stage 24.0 (TID 1975) in 417 ms on localhost (144/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 157.0 in stage 24.0 (TID 1993) in 38 ms on localhost (145/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 141.0 in stage 24.0 (TID 1977). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 161.0 in stage 24.0 (TID 1997, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 161.0 in stage 24.0 (TID 1997)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 141.0 in stage 24.0 (TID 1977) in 454 ms on localhost (146/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:09 INFO Executor: Finished task 144.0 in stage 24.0 (TID 1980). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO Executor: Finished task 142.0 in stage 24.0 (TID 1978). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO Executor: Finished task 145.0 in stage 24.0 (TID 1981). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 162.0 in stage 24.0 (TID 1998, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 162.0 in stage 24.0 (TID 1998)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 163.0 in stage 24.0 (TID 1999, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 163.0 in stage 24.0 (TID 1999)
15/08/16 12:52:09 INFO TaskSetManager: Starting task 164.0 in stage 24.0 (TID 2000, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 164.0 in stage 24.0 (TID 2000)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 144.0 in stage 24.0 (TID 1980) in 462 ms on localhost (147/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 145.0 in stage 24.0 (TID 1981) in 459 ms on localhost (148/200)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 142.0 in stage 24.0 (TID 1978) in 483 ms on localhost (149/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:09 INFO Executor: Finished task 146.0 in stage 24.0 (TID 1982). 1219 bytes result sent to driver
15/08/16 12:52:09 INFO TaskSetManager: Starting task 165.0 in stage 24.0 (TID 2001, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:09 INFO Executor: Running task 165.0 in stage 24.0 (TID 2001)
15/08/16 12:52:09 INFO TaskSetManager: Finished task 146.0 in stage 24.0 (TID 1982) in 487 ms on localhost (150/200)
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 150.0 in stage 24.0 (TID 1986). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 166.0 in stage 24.0 (TID 2002, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 166.0 in stage 24.0 (TID 2002)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 150.0 in stage 24.0 (TID 1986) in 490 ms on localhost (151/200)
15/08/16 12:52:10 INFO Executor: Finished task 147.0 in stage 24.0 (TID 1983). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 167.0 in stage 24.0 (TID 2003, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 147.0 in stage 24.0 (TID 1983) in 525 ms on localhost (152/200)
15/08/16 12:52:10 INFO Executor: Running task 167.0 in stage 24.0 (TID 2003)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 148.0 in stage 24.0 (TID 1984). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 168.0 in stage 24.0 (TID 2004, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 168.0 in stage 24.0 (TID 2004)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 148.0 in stage 24.0 (TID 1984) in 511 ms on localhost (153/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 149.0 in stage 24.0 (TID 1985). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 169.0 in stage 24.0 (TID 2005, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 169.0 in stage 24.0 (TID 2005)
15/08/16 12:52:10 INFO Executor: Finished task 151.0 in stage 24.0 (TID 1987). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 170.0 in stage 24.0 (TID 2006, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 170.0 in stage 24.0 (TID 2006)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 149.0 in stage 24.0 (TID 1985) in 509 ms on localhost (154/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO TaskSetManager: Finished task 151.0 in stage 24.0 (TID 1987) in 498 ms on localhost (155/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 166.0 in stage 24.0 (TID 2002). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 171.0 in stage 24.0 (TID 2007, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 171.0 in stage 24.0 (TID 2007)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 166.0 in stage 24.0 (TID 2002) in 38 ms on localhost (156/200)
15/08/16 12:52:10 INFO Executor: Finished task 168.0 in stage 24.0 (TID 2004). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 172.0 in stage 24.0 (TID 2008, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 172.0 in stage 24.0 (TID 2008)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO TaskSetManager: Finished task 168.0 in stage 24.0 (TID 2004) in 32 ms on localhost (157/200)
15/08/16 12:52:10 INFO Executor: Finished task 169.0 in stage 24.0 (TID 2005). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 173.0 in stage 24.0 (TID 2009, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 169.0 in stage 24.0 (TID 2005) in 33 ms on localhost (158/200)
15/08/16 12:52:10 INFO Executor: Running task 173.0 in stage 24.0 (TID 2009)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 153.0 in stage 24.0 (TID 1989). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 174.0 in stage 24.0 (TID 2010, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 174.0 in stage 24.0 (TID 2010)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 153.0 in stage 24.0 (TID 1989) in 460 ms on localhost (159/200)
15/08/16 12:52:10 INFO Executor: Finished task 172.0 in stage 24.0 (TID 2008). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 152.0 in stage 24.0 (TID 1988). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 175.0 in stage 24.0 (TID 2011, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 175.0 in stage 24.0 (TID 2011)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 176.0 in stage 24.0 (TID 2012, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 176.0 in stage 24.0 (TID 2012)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 172.0 in stage 24.0 (TID 2008) in 34 ms on localhost (160/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 152.0 in stage 24.0 (TID 1988) in 466 ms on localhost (161/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 176.0 in stage 24.0 (TID 2012). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 177.0 in stage 24.0 (TID 2013, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 177.0 in stage 24.0 (TID 2013)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 176.0 in stage 24.0 (TID 2012) in 41 ms on localhost (162/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 156.0 in stage 24.0 (TID 1992). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 178.0 in stage 24.0 (TID 2014, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 178.0 in stage 24.0 (TID 2014)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 156.0 in stage 24.0 (TID 1992) in 452 ms on localhost (163/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 159.0 in stage 24.0 (TID 1995). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 179.0 in stage 24.0 (TID 2015, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 179.0 in stage 24.0 (TID 2015)
15/08/16 12:52:10 INFO Executor: Finished task 160.0 in stage 24.0 (TID 1996). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 180.0 in stage 24.0 (TID 2016, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 180.0 in stage 24.0 (TID 2016)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 159.0 in stage 24.0 (TID 1995) in 418 ms on localhost (164/200)
15/08/16 12:52:10 INFO Executor: Finished task 158.0 in stage 24.0 (TID 1994). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 181.0 in stage 24.0 (TID 2017, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 181.0 in stage 24.0 (TID 2017)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 160.0 in stage 24.0 (TID 1996) in 419 ms on localhost (165/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 158.0 in stage 24.0 (TID 1994) in 425 ms on localhost (166/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 179.0 in stage 24.0 (TID 2015). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 182.0 in stage 24.0 (TID 2018, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 182.0 in stage 24.0 (TID 2018)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 179.0 in stage 24.0 (TID 2015) in 44 ms on localhost (167/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 161.0 in stage 24.0 (TID 1997). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 183.0 in stage 24.0 (TID 2019, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 183.0 in stage 24.0 (TID 2019)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 161.0 in stage 24.0 (TID 1997) in 417 ms on localhost (168/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 163.0 in stage 24.0 (TID 1999). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 184.0 in stage 24.0 (TID 2020, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 184.0 in stage 24.0 (TID 2020)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 163.0 in stage 24.0 (TID 1999) in 423 ms on localhost (169/200)
15/08/16 12:52:10 INFO Executor: Finished task 162.0 in stage 24.0 (TID 1998). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 185.0 in stage 24.0 (TID 2021, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 185.0 in stage 24.0 (TID 2021)
15/08/16 12:52:10 INFO Executor: Finished task 164.0 in stage 24.0 (TID 2000). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 186.0 in stage 24.0 (TID 2022, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 186.0 in stage 24.0 (TID 2022)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 162.0 in stage 24.0 (TID 1998) in 429 ms on localhost (170/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 164.0 in stage 24.0 (TID 2000) in 428 ms on localhost (171/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 165.0 in stage 24.0 (TID 2001). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 187.0 in stage 24.0 (TID 2023, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 165.0 in stage 24.0 (TID 2001) in 437 ms on localhost (172/200)
15/08/16 12:52:10 INFO Executor: Running task 187.0 in stage 24.0 (TID 2023)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 167.0 in stage 24.0 (TID 2003). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 188.0 in stage 24.0 (TID 2024, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 188.0 in stage 24.0 (TID 2024)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 167.0 in stage 24.0 (TID 2003) in 399 ms on localhost (173/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 170.0 in stage 24.0 (TID 2006). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 189.0 in stage 24.0 (TID 2025, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 189.0 in stage 24.0 (TID 2025)
15/08/16 12:52:10 INFO Executor: Finished task 171.0 in stage 24.0 (TID 2007). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 170.0 in stage 24.0 (TID 2006) in 432 ms on localhost (174/200)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 190.0 in stage 24.0 (TID 2026, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 190.0 in stage 24.0 (TID 2026)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 171.0 in stage 24.0 (TID 2007) in 417 ms on localhost (175/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO Executor: Finished task 173.0 in stage 24.0 (TID 2009). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO TaskSetManager: Starting task 191.0 in stage 24.0 (TID 2027, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 191.0 in stage 24.0 (TID 2027)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 173.0 in stage 24.0 (TID 2009) in 419 ms on localhost (176/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 189.0 in stage 24.0 (TID 2025). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 192.0 in stage 24.0 (TID 2028, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 192.0 in stage 24.0 (TID 2028)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 189.0 in stage 24.0 (TID 2025) in 54 ms on localhost (177/200)
15/08/16 12:52:10 INFO Executor: Finished task 174.0 in stage 24.0 (TID 2010). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 193.0 in stage 24.0 (TID 2029, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 193.0 in stage 24.0 (TID 2029)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 174.0 in stage 24.0 (TID 2010) in 434 ms on localhost (178/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 175.0 in stage 24.0 (TID 2011). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 194.0 in stage 24.0 (TID 2030, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 194.0 in stage 24.0 (TID 2030)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 175.0 in stage 24.0 (TID 2011) in 439 ms on localhost (179/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 177.0 in stage 24.0 (TID 2013). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 195.0 in stage 24.0 (TID 2031, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 195.0 in stage 24.0 (TID 2031)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 177.0 in stage 24.0 (TID 2013) in 437 ms on localhost (180/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO Executor: Finished task 195.0 in stage 24.0 (TID 2031). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 178.0 in stage 24.0 (TID 2014). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 196.0 in stage 24.0 (TID 2032, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 196.0 in stage 24.0 (TID 2032)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 197.0 in stage 24.0 (TID 2033, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 197.0 in stage 24.0 (TID 2033)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 178.0 in stage 24.0 (TID 2014) in 428 ms on localhost (181/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 195.0 in stage 24.0 (TID 2031) in 43 ms on localhost (182/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 180.0 in stage 24.0 (TID 2016). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 198.0 in stage 24.0 (TID 2034, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 198.0 in stage 24.0 (TID 2034)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 180.0 in stage 24.0 (TID 2016) in 455 ms on localhost (183/200)
15/08/16 12:52:10 INFO Executor: Finished task 181.0 in stage 24.0 (TID 2017). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 199.0 in stage 24.0 (TID 2035, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 199.0 in stage 24.0 (TID 2035)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 181.0 in stage 24.0 (TID 2017) in 460 ms on localhost (184/200)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO Executor: Finished task 199.0 in stage 24.0 (TID 2035). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 199.0 in stage 24.0 (TID 2035) in 32 ms on localhost (185/200)
15/08/16 12:52:10 INFO Executor: Finished task 182.0 in stage 24.0 (TID 2018). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 182.0 in stage 24.0 (TID 2018) in 457 ms on localhost (186/200)
15/08/16 12:52:10 INFO Executor: Finished task 183.0 in stage 24.0 (TID 2019). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 183.0 in stage 24.0 (TID 2019) in 480 ms on localhost (187/200)
15/08/16 12:52:10 INFO Executor: Finished task 184.0 in stage 24.0 (TID 2020). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 184.0 in stage 24.0 (TID 2020) in 466 ms on localhost (188/200)
15/08/16 12:52:10 INFO Executor: Finished task 185.0 in stage 24.0 (TID 2021). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 186.0 in stage 24.0 (TID 2022). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 185.0 in stage 24.0 (TID 2021) in 472 ms on localhost (189/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 186.0 in stage 24.0 (TID 2022) in 472 ms on localhost (190/200)
15/08/16 12:52:10 INFO Executor: Finished task 187.0 in stage 24.0 (TID 2023). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 187.0 in stage 24.0 (TID 2023) in 459 ms on localhost (191/200)
15/08/16 12:52:10 INFO Executor: Finished task 188.0 in stage 24.0 (TID 2024). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 188.0 in stage 24.0 (TID 2024) in 450 ms on localhost (192/200)
15/08/16 12:52:10 INFO Executor: Finished task 191.0 in stage 24.0 (TID 2027). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 191.0 in stage 24.0 (TID 2027) in 414 ms on localhost (193/200)
15/08/16 12:52:10 INFO Executor: Finished task 190.0 in stage 24.0 (TID 2026). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 190.0 in stage 24.0 (TID 2026) in 427 ms on localhost (194/200)
15/08/16 12:52:10 INFO Executor: Finished task 192.0 in stage 24.0 (TID 2028). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 192.0 in stage 24.0 (TID 2028) in 397 ms on localhost (195/200)
15/08/16 12:52:10 INFO Executor: Finished task 193.0 in stage 24.0 (TID 2029). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 194.0 in stage 24.0 (TID 2030). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 193.0 in stage 24.0 (TID 2029) in 394 ms on localhost (196/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 194.0 in stage 24.0 (TID 2030) in 386 ms on localhost (197/200)
15/08/16 12:52:10 INFO Executor: Finished task 197.0 in stage 24.0 (TID 2033). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 197.0 in stage 24.0 (TID 2033) in 318 ms on localhost (198/200)
15/08/16 12:52:10 INFO Executor: Finished task 196.0 in stage 24.0 (TID 2032). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 198.0 in stage 24.0 (TID 2034). 1219 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Finished task 196.0 in stage 24.0 (TID 2032) in 322 ms on localhost (199/200)
15/08/16 12:52:10 INFO TaskSetManager: Finished task 198.0 in stage 24.0 (TID 2034) in 248 ms on localhost (200/200)
15/08/16 12:52:10 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
15/08/16 12:52:10 INFO DAGScheduler: ShuffleMapStage 24 (processCmd at CliDriver.java:423) finished in 5.562 s
15/08/16 12:52:10 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:52:10 INFO DAGScheduler: running: Set()
15/08/16 12:52:10 INFO DAGScheduler: waiting: Set(ShuffleMapStage 25, ResultStage 26)
15/08/16 12:52:10 INFO DAGScheduler: failed: Set()
15/08/16 12:52:10 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@22efa076
15/08/16 12:52:10 INFO StatsReportListener: task runtime:(count: 200, mean: 441.705000, stdev: 207.847656, max: 931.000000, min: 31.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	31.0 ms	38.0 ms	42.0 ms	397.0 ms	441.0 ms	512.0 ms	694.0 ms	868.0 ms	931.0 ms
15/08/16 12:52:10 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 2858.995000, stdev: 1105.905667, max: 3385.000000, min: 0.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	3.2 KB	3.2 KB	3.2 KB	3.3 KB	3.3 KB	3.3 KB
15/08/16 12:52:10 INFO DAGScheduler: Missing parents for ShuffleMapStage 25: List()
15/08/16 12:52:10 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.175000, stdev: 0.440880, max: 2.000000, min: 0.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/16 12:52:10 INFO DAGScheduler: Missing parents for ResultStage 26: List(ShuffleMapStage 25)
15/08/16 12:52:10 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[86] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:52:10 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:52:10 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/16 12:52:10 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 91.773793, stdev: 12.059873, max: 98.412698, min: 40.476190)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	40 %	62 %	68 %	95 %	97 %	97 %	98 %	98 %	98 %
15/08/16 12:52:10 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.090016, stdev: 0.484416, max: 5.714286, min: 0.000000)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 6 %
15/08/16 12:52:10 INFO StatsReportListener: other time pct: (count: 200, mean: 8.136191, stdev: 11.966530, max: 59.523810, min: 1.538462)
15/08/16 12:52:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:10 INFO StatsReportListener: 	 2 %	 2 %	 2 %	 3 %	 3 %	 5 %	32 %	39 %	60 %
15/08/16 12:52:10 INFO MemoryStore: ensureFreeSpace(25512) called with curMem=508788589, maxMem=3333968363
15/08/16 12:52:10 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 24.9 KB, free 2.6 GB)
15/08/16 12:52:10 INFO MemoryStore: ensureFreeSpace(10671) called with curMem=508814101, maxMem=3333968363
15/08/16 12:52:10 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 10.4 KB, free 2.6 GB)
15/08/16 12:52:10 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:36543 (size: 10.4 KB, free: 3.1 GB)
15/08/16 12:52:10 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:874
15/08/16 12:52:10 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[86] at processCmd at CliDriver.java:423)
15/08/16 12:52:10 INFO TaskSchedulerImpl: Adding task set 25.0 with 200 tasks
15/08/16 12:52:10 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 2036, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 2037, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 2.0 in stage 25.0 (TID 2038, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 3.0 in stage 25.0 (TID 2039, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 4.0 in stage 25.0 (TID 2040, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 5.0 in stage 25.0 (TID 2041, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 6.0 in stage 25.0 (TID 2042, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 7.0 in stage 25.0 (TID 2043, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 8.0 in stage 25.0 (TID 2044, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 9.0 in stage 25.0 (TID 2045, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 10.0 in stage 25.0 (TID 2046, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 11.0 in stage 25.0 (TID 2047, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 12.0 in stage 25.0 (TID 2048, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 13.0 in stage 25.0 (TID 2049, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 14.0 in stage 25.0 (TID 2050, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 15.0 in stage 25.0 (TID 2051, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Running task 1.0 in stage 25.0 (TID 2037)
15/08/16 12:52:10 INFO Executor: Running task 9.0 in stage 25.0 (TID 2045)
15/08/16 12:52:10 INFO Executor: Running task 8.0 in stage 25.0 (TID 2044)
15/08/16 12:52:10 INFO Executor: Running task 6.0 in stage 25.0 (TID 2042)
15/08/16 12:52:10 INFO Executor: Running task 14.0 in stage 25.0 (TID 2050)
15/08/16 12:52:10 INFO Executor: Running task 12.0 in stage 25.0 (TID 2048)
15/08/16 12:52:10 INFO Executor: Running task 10.0 in stage 25.0 (TID 2046)
15/08/16 12:52:10 INFO Executor: Running task 13.0 in stage 25.0 (TID 2049)
15/08/16 12:52:10 INFO Executor: Running task 4.0 in stage 25.0 (TID 2040)
15/08/16 12:52:10 INFO Executor: Running task 5.0 in stage 25.0 (TID 2041)
15/08/16 12:52:10 INFO Executor: Running task 3.0 in stage 25.0 (TID 2039)
15/08/16 12:52:10 INFO Executor: Running task 0.0 in stage 25.0 (TID 2036)
15/08/16 12:52:10 INFO Executor: Running task 11.0 in stage 25.0 (TID 2047)
15/08/16 12:52:10 INFO Executor: Running task 15.0 in stage 25.0 (TID 2051)
15/08/16 12:52:10 INFO Executor: Running task 2.0 in stage 25.0 (TID 2038)
15/08/16 12:52:10 INFO Executor: Running task 7.0 in stage 25.0 (TID 2043)
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:10 INFO Executor: Finished task 11.0 in stage 25.0 (TID 2047). 1019 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 9.0 in stage 25.0 (TID 2045). 1019 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Finished task 8.0 in stage 25.0 (TID 2044). 1019 bytes result sent to driver
15/08/16 12:52:10 INFO TaskSetManager: Starting task 16.0 in stage 25.0 (TID 2052, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO TaskSetManager: Starting task 17.0 in stage 25.0 (TID 2053, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:10 INFO Executor: Finished task 1.0 in stage 25.0 (TID 2037). 1019 bytes result sent to driver
15/08/16 12:52:10 INFO Executor: Running task 17.0 in stage 25.0 (TID 2053)
15/08/16 12:52:10 INFO Executor: Running task 16.0 in stage 25.0 (TID 2052)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 18.0 in stage 25.0 (TID 2054, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 11.0 in stage 25.0 (TID 2047) in 59 ms on localhost (1/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 8.0 in stage 25.0 (TID 2044) in 61 ms on localhost (2/200)
15/08/16 12:52:11 INFO Executor: Running task 18.0 in stage 25.0 (TID 2054)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 9.0 in stage 25.0 (TID 2045) in 60 ms on localhost (3/200)
15/08/16 12:52:11 INFO Executor: Finished task 0.0 in stage 25.0 (TID 2036). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 6.0 in stage 25.0 (TID 2042). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 19.0 in stage 25.0 (TID 2055, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 19.0 in stage 25.0 (TID 2055)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 20.0 in stage 25.0 (TID 2056, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 21.0 in stage 25.0 (TID 2057, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 5.0 in stage 25.0 (TID 2041). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 4.0 in stage 25.0 (TID 2040). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 21.0 in stage 25.0 (TID 2057)
15/08/16 12:52:11 INFO Executor: Running task 20.0 in stage 25.0 (TID 2056)
15/08/16 12:52:11 INFO Executor: Finished task 2.0 in stage 25.0 (TID 2038). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 22.0 in stage 25.0 (TID 2058, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 22.0 in stage 25.0 (TID 2058)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 23.0 in stage 25.0 (TID 2059, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 2036) in 340 ms on localhost (4/200)
15/08/16 12:52:11 INFO Executor: Running task 23.0 in stage 25.0 (TID 2059)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 6.0 in stage 25.0 (TID 2042) in 340 ms on localhost (5/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 2037) in 342 ms on localhost (6/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 24.0 in stage 25.0 (TID 2060, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 5.0 in stage 25.0 (TID 2041) in 342 ms on localhost (7/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 4.0 in stage 25.0 (TID 2040) in 342 ms on localhost (8/200)
15/08/16 12:52:11 INFO Executor: Running task 24.0 in stage 25.0 (TID 2060)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 2.0 in stage 25.0 (TID 2038) in 347 ms on localhost (9/200)
15/08/16 12:52:11 INFO Executor: Finished task 10.0 in stage 25.0 (TID 2046). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 25.0 in stage 25.0 (TID 2061, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 25.0 in stage 25.0 (TID 2061)
15/08/16 12:52:11 INFO Executor: Finished task 3.0 in stage 25.0 (TID 2039). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 7.0 in stage 25.0 (TID 2043). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 14.0 in stage 25.0 (TID 2050). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 26.0 in stage 25.0 (TID 2062, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 12.0 in stage 25.0 (TID 2048). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 26.0 in stage 25.0 (TID 2062)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 27.0 in stage 25.0 (TID 2063, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 28.0 in stage 25.0 (TID 2064, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 29.0 in stage 25.0 (TID 2065, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 29.0 in stage 25.0 (TID 2065)
15/08/16 12:52:11 INFO Executor: Running task 28.0 in stage 25.0 (TID 2064)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 10.0 in stage 25.0 (TID 2046) in 351 ms on localhost (10/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 7.0 in stage 25.0 (TID 2043) in 352 ms on localhost (11/200)
15/08/16 12:52:11 INFO Executor: Finished task 13.0 in stage 25.0 (TID 2049). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 30.0 in stage 25.0 (TID 2066, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 3.0 in stage 25.0 (TID 2039) in 354 ms on localhost (12/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 12.0 in stage 25.0 (TID 2048) in 352 ms on localhost (13/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 14.0 in stage 25.0 (TID 2050) in 352 ms on localhost (14/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 30.0 in stage 25.0 (TID 2066)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 13.0 in stage 25.0 (TID 2049) in 354 ms on localhost (15/200)
15/08/16 12:52:11 INFO Executor: Running task 27.0 in stage 25.0 (TID 2063)
15/08/16 12:52:11 INFO Executor: Finished task 15.0 in stage 25.0 (TID 2051). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 31.0 in stage 25.0 (TID 2067, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Finished task 15.0 in stage 25.0 (TID 2051) in 360 ms on localhost (16/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 31.0 in stage 25.0 (TID 2067)
15/08/16 12:52:11 INFO Executor: Finished task 20.0 in stage 25.0 (TID 2056). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 32.0 in stage 25.0 (TID 2068, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 32.0 in stage 25.0 (TID 2068)
15/08/16 12:52:11 INFO Executor: Finished task 23.0 in stage 25.0 (TID 2059). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 33.0 in stage 25.0 (TID 2069, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 33.0 in stage 25.0 (TID 2069)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 23.0 in stage 25.0 (TID 2059) in 34 ms on localhost (17/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 20.0 in stage 25.0 (TID 2056) in 35 ms on localhost (18/200)
15/08/16 12:52:11 INFO Executor: Finished task 19.0 in stage 25.0 (TID 2055). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 17.0 in stage 25.0 (TID 2053). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 24.0 in stage 25.0 (TID 2060). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 34.0 in stage 25.0 (TID 2070, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 35.0 in stage 25.0 (TID 2071, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 36.0 in stage 25.0 (TID 2072, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 36.0 in stage 25.0 (TID 2072)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 19.0 in stage 25.0 (TID 2055) in 46 ms on localhost (19/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 17.0 in stage 25.0 (TID 2053) in 332 ms on localhost (20/200)
15/08/16 12:52:11 INFO Executor: Finished task 25.0 in stage 25.0 (TID 2061). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 24.0 in stage 25.0 (TID 2060) in 43 ms on localhost (21/200)
15/08/16 12:52:11 INFO Executor: Running task 35.0 in stage 25.0 (TID 2071)
15/08/16 12:52:11 INFO Executor: Running task 34.0 in stage 25.0 (TID 2070)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 37.0 in stage 25.0 (TID 2073, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 37.0 in stage 25.0 (TID 2073)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 25.0 in stage 25.0 (TID 2061) in 40 ms on localhost (22/200)
15/08/16 12:52:11 INFO Executor: Finished task 29.0 in stage 25.0 (TID 2065). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 38.0 in stage 25.0 (TID 2074, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 38.0 in stage 25.0 (TID 2074)
15/08/16 12:52:11 INFO Executor: Finished task 22.0 in stage 25.0 (TID 2058). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 16.0 in stage 25.0 (TID 2052). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 39.0 in stage 25.0 (TID 2075, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 40.0 in stage 25.0 (TID 2076, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 39.0 in stage 25.0 (TID 2075)
15/08/16 12:52:11 INFO Executor: Running task 40.0 in stage 25.0 (TID 2076)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 22.0 in stage 25.0 (TID 2058) in 53 ms on localhost (23/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 16.0 in stage 25.0 (TID 2052) in 342 ms on localhost (24/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 18.0 in stage 25.0 (TID 2054). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 41.0 in stage 25.0 (TID 2077, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 41.0 in stage 25.0 (TID 2077)
15/08/16 12:52:11 INFO Executor: Finished task 21.0 in stage 25.0 (TID 2057). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 42.0 in stage 25.0 (TID 2078, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 42.0 in stage 25.0 (TID 2078)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 18.0 in stage 25.0 (TID 2054) in 344 ms on localhost (25/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 21.0 in stage 25.0 (TID 2057) in 58 ms on localhost (26/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:52:11 INFO Executor: Finished task 27.0 in stage 25.0 (TID 2063). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 43.0 in stage 25.0 (TID 2079, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 29.0 in stage 25.0 (TID 2065) in 49 ms on localhost (27/200)
15/08/16 12:52:11 INFO Executor: Running task 43.0 in stage 25.0 (TID 2079)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 27.0 in stage 25.0 (TID 2063) in 52 ms on localhost (28/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 32.0 in stage 25.0 (TID 2068). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 44.0 in stage 25.0 (TID 2080, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 44.0 in stage 25.0 (TID 2080)
15/08/16 12:52:11 INFO Executor: Finished task 33.0 in stage 25.0 (TID 2069). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 32.0 in stage 25.0 (TID 2068) in 35 ms on localhost (29/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 45.0 in stage 25.0 (TID 2081, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 45.0 in stage 25.0 (TID 2081)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 33.0 in stage 25.0 (TID 2069) in 35 ms on localhost (30/200)
15/08/16 12:52:11 INFO Executor: Finished task 30.0 in stage 25.0 (TID 2066). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 46.0 in stage 25.0 (TID 2082, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 46.0 in stage 25.0 (TID 2082)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 30.0 in stage 25.0 (TID 2066) in 56 ms on localhost (31/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 36.0 in stage 25.0 (TID 2072). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 31.0 in stage 25.0 (TID 2067). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 47.0 in stage 25.0 (TID 2083, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 47.0 in stage 25.0 (TID 2083)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 48.0 in stage 25.0 (TID 2084, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 48.0 in stage 25.0 (TID 2084)
15/08/16 12:52:11 INFO Executor: Finished task 38.0 in stage 25.0 (TID 2074). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 34.0 in stage 25.0 (TID 2070). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 49.0 in stage 25.0 (TID 2085, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 36.0 in stage 25.0 (TID 2072) in 31 ms on localhost (32/200)
15/08/16 12:52:11 INFO Executor: Finished task 40.0 in stage 25.0 (TID 2076). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 49.0 in stage 25.0 (TID 2085)
15/08/16 12:52:11 INFO Executor: Finished task 39.0 in stage 25.0 (TID 2075). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 50.0 in stage 25.0 (TID 2086, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 31.0 in stage 25.0 (TID 2067) in 55 ms on localhost (33/200)
15/08/16 12:52:11 INFO Executor: Running task 50.0 in stage 25.0 (TID 2086)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 51.0 in stage 25.0 (TID 2087, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 51.0 in stage 25.0 (TID 2087)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 52.0 in stage 25.0 (TID 2088, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 38.0 in stage 25.0 (TID 2074) in 29 ms on localhost (34/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 34.0 in stage 25.0 (TID 2070) in 35 ms on localhost (35/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 40.0 in stage 25.0 (TID 2076) in 28 ms on localhost (36/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 39.0 in stage 25.0 (TID 2075) in 29 ms on localhost (37/200)
15/08/16 12:52:11 INFO Executor: Finished task 26.0 in stage 25.0 (TID 2062). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 53.0 in stage 25.0 (TID 2089, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 37.0 in stage 25.0 (TID 2073). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 53.0 in stage 25.0 (TID 2089)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 52.0 in stage 25.0 (TID 2088)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 54.0 in stage 25.0 (TID 2090, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Running task 54.0 in stage 25.0 (TID 2090)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 26.0 in stage 25.0 (TID 2062) in 73 ms on localhost (38/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 37.0 in stage 25.0 (TID 2073) in 40 ms on localhost (39/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 28.0 in stage 25.0 (TID 2064). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 41.0 in stage 25.0 (TID 2077). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 55.0 in stage 25.0 (TID 2091, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 55.0 in stage 25.0 (TID 2091)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 35.0 in stage 25.0 (TID 2071). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 43.0 in stage 25.0 (TID 2079). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 56.0 in stage 25.0 (TID 2092, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 28.0 in stage 25.0 (TID 2064) in 81 ms on localhost (40/200)
15/08/16 12:52:11 INFO Executor: Running task 56.0 in stage 25.0 (TID 2092)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 57.0 in stage 25.0 (TID 2093, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 42.0 in stage 25.0 (TID 2078). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 58.0 in stage 25.0 (TID 2094, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 59.0 in stage 25.0 (TID 2095, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 41.0 in stage 25.0 (TID 2077) in 39 ms on localhost (41/200)
15/08/16 12:52:11 INFO Executor: Running task 59.0 in stage 25.0 (TID 2095)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 35.0 in stage 25.0 (TID 2071) in 50 ms on localhost (42/200)
15/08/16 12:52:11 INFO Executor: Running task 58.0 in stage 25.0 (TID 2094)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 43.0 in stage 25.0 (TID 2079) in 34 ms on localhost (43/200)
15/08/16 12:52:11 INFO Executor: Running task 57.0 in stage 25.0 (TID 2093)
15/08/16 12:52:11 INFO Executor: Finished task 45.0 in stage 25.0 (TID 2081). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 42.0 in stage 25.0 (TID 2078) in 39 ms on localhost (44/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 60.0 in stage 25.0 (TID 2096, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 60.0 in stage 25.0 (TID 2096)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 47.0 in stage 25.0 (TID 2083). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 61.0 in stage 25.0 (TID 2097, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 61.0 in stage 25.0 (TID 2097)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 45.0 in stage 25.0 (TID 2081) in 36 ms on localhost (45/200)
15/08/16 12:52:11 INFO Executor: Finished task 48.0 in stage 25.0 (TID 2084). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 62.0 in stage 25.0 (TID 2098, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 62.0 in stage 25.0 (TID 2098)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 44.0 in stage 25.0 (TID 2080). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 63.0 in stage 25.0 (TID 2099, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 44.0 in stage 25.0 (TID 2080) in 41 ms on localhost (46/200)
15/08/16 12:52:11 INFO Executor: Finished task 46.0 in stage 25.0 (TID 2082). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 48.0 in stage 25.0 (TID 2084) in 30 ms on localhost (47/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 64.0 in stage 25.0 (TID 2100, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 64.0 in stage 25.0 (TID 2100)
15/08/16 12:52:11 INFO Executor: Running task 63.0 in stage 25.0 (TID 2099)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 47.0 in stage 25.0 (TID 2083) in 34 ms on localhost (48/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 51.0 in stage 25.0 (TID 2087). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 65.0 in stage 25.0 (TID 2101, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 65.0 in stage 25.0 (TID 2101)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 46.0 in stage 25.0 (TID 2082) in 50 ms on localhost (49/200)
15/08/16 12:52:11 INFO Executor: Finished task 49.0 in stage 25.0 (TID 2085). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 66.0 in stage 25.0 (TID 2102, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 66.0 in stage 25.0 (TID 2102)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 51.0 in stage 25.0 (TID 2087) in 44 ms on localhost (50/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 49.0 in stage 25.0 (TID 2085) in 45 ms on localhost (51/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 52.0 in stage 25.0 (TID 2088). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 67.0 in stage 25.0 (TID 2103, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 67.0 in stage 25.0 (TID 2103)
15/08/16 12:52:11 INFO Executor: Finished task 50.0 in stage 25.0 (TID 2086). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 53.0 in stage 25.0 (TID 2089). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 68.0 in stage 25.0 (TID 2104, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 69.0 in stage 25.0 (TID 2105, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 52.0 in stage 25.0 (TID 2088) in 51 ms on localhost (52/200)
15/08/16 12:52:11 INFO Executor: Running task 69.0 in stage 25.0 (TID 2105)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 50.0 in stage 25.0 (TID 2086) in 51 ms on localhost (53/200)
15/08/16 12:52:11 INFO Executor: Finished task 59.0 in stage 25.0 (TID 2095). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 68.0 in stage 25.0 (TID 2104)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 70.0 in stage 25.0 (TID 2106, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 53.0 in stage 25.0 (TID 2089) in 48 ms on localhost (54/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 59.0 in stage 25.0 (TID 2095) in 36 ms on localhost (55/200)
15/08/16 12:52:11 INFO Executor: Finished task 56.0 in stage 25.0 (TID 2092). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 71.0 in stage 25.0 (TID 2107, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 71.0 in stage 25.0 (TID 2107)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Running task 70.0 in stage 25.0 (TID 2106)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 56.0 in stage 25.0 (TID 2092) in 43 ms on localhost (56/200)
15/08/16 12:52:11 INFO Executor: Finished task 55.0 in stage 25.0 (TID 2091). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 57.0 in stage 25.0 (TID 2093). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 72.0 in stage 25.0 (TID 2108, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 73.0 in stage 25.0 (TID 2109, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Running task 73.0 in stage 25.0 (TID 2109)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 55.0 in stage 25.0 (TID 2091) in 46 ms on localhost (57/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 57.0 in stage 25.0 (TID 2093) in 44 ms on localhost (58/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 72.0 in stage 25.0 (TID 2108)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 54.0 in stage 25.0 (TID 2090). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 74.0 in stage 25.0 (TID 2110, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 74.0 in stage 25.0 (TID 2110)
15/08/16 12:52:11 INFO Executor: Finished task 62.0 in stage 25.0 (TID 2098). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 75.0 in stage 25.0 (TID 2111, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 66.0 in stage 25.0 (TID 2102). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 54.0 in stage 25.0 (TID 2090) in 60 ms on localhost (59/200)
15/08/16 12:52:11 INFO Executor: Running task 75.0 in stage 25.0 (TID 2111)
15/08/16 12:52:11 INFO Executor: Finished task 60.0 in stage 25.0 (TID 2096). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 76.0 in stage 25.0 (TID 2112, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 62.0 in stage 25.0 (TID 2098) in 42 ms on localhost (60/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 77.0 in stage 25.0 (TID 2113, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 77.0 in stage 25.0 (TID 2113)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 76.0 in stage 25.0 (TID 2112)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 60.0 in stage 25.0 (TID 2096) in 49 ms on localhost (61/200)
15/08/16 12:52:11 INFO Executor: Finished task 63.0 in stage 25.0 (TID 2099). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 78.0 in stage 25.0 (TID 2114, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 78.0 in stage 25.0 (TID 2114)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 63.0 in stage 25.0 (TID 2099) in 44 ms on localhost (62/200)
15/08/16 12:52:11 INFO Executor: Finished task 61.0 in stage 25.0 (TID 2097). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 79.0 in stage 25.0 (TID 2115, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 64.0 in stage 25.0 (TID 2100). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 79.0 in stage 25.0 (TID 2115)
15/08/16 12:52:11 INFO Executor: Finished task 58.0 in stage 25.0 (TID 2094). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 80.0 in stage 25.0 (TID 2116, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 81.0 in stage 25.0 (TID 2117, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 61.0 in stage 25.0 (TID 2097) in 50 ms on localhost (63/200)
15/08/16 12:52:11 INFO Executor: Running task 81.0 in stage 25.0 (TID 2117)
15/08/16 12:52:11 INFO Executor: Running task 80.0 in stage 25.0 (TID 2116)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 64.0 in stage 25.0 (TID 2100) in 46 ms on localhost (64/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 66.0 in stage 25.0 (TID 2102) in 33 ms on localhost (65/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 58.0 in stage 25.0 (TID 2094) in 61 ms on localhost (66/200)
15/08/16 12:52:11 INFO Executor: Finished task 65.0 in stage 25.0 (TID 2101). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 67.0 in stage 25.0 (TID 2103). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 82.0 in stage 25.0 (TID 2118, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 82.0 in stage 25.0 (TID 2118)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 83.0 in stage 25.0 (TID 2119, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 65.0 in stage 25.0 (TID 2101) in 40 ms on localhost (67/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 67.0 in stage 25.0 (TID 2103) in 31 ms on localhost (68/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 68.0 in stage 25.0 (TID 2104). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 83.0 in stage 25.0 (TID 2119)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 84.0 in stage 25.0 (TID 2120, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 84.0 in stage 25.0 (TID 2120)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 68.0 in stage 25.0 (TID 2104) in 33 ms on localhost (69/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 72.0 in stage 25.0 (TID 2108). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 85.0 in stage 25.0 (TID 2121, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 85.0 in stage 25.0 (TID 2121)
15/08/16 12:52:11 INFO Executor: Finished task 70.0 in stage 25.0 (TID 2106). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 72.0 in stage 25.0 (TID 2108) in 29 ms on localhost (70/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 86.0 in stage 25.0 (TID 2122, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 71.0 in stage 25.0 (TID 2107). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 87.0 in stage 25.0 (TID 2123, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 87.0 in stage 25.0 (TID 2123)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 70.0 in stage 25.0 (TID 2106) in 38 ms on localhost (71/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 71.0 in stage 25.0 (TID 2107) in 37 ms on localhost (72/200)
15/08/16 12:52:11 INFO Executor: Finished task 77.0 in stage 25.0 (TID 2113). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 88.0 in stage 25.0 (TID 2124, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 77.0 in stage 25.0 (TID 2113) in 27 ms on localhost (73/200)
15/08/16 12:52:11 INFO Executor: Finished task 75.0 in stage 25.0 (TID 2111). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 73.0 in stage 25.0 (TID 2109). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 69.0 in stage 25.0 (TID 2105). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 89.0 in stage 25.0 (TID 2125, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 88.0 in stage 25.0 (TID 2124)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 90.0 in stage 25.0 (TID 2126, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 74.0 in stage 25.0 (TID 2110). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 90.0 in stage 25.0 (TID 2126)
15/08/16 12:52:11 INFO Executor: Running task 89.0 in stage 25.0 (TID 2125)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 91.0 in stage 25.0 (TID 2127, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 92.0 in stage 25.0 (TID 2128, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 91.0 in stage 25.0 (TID 2127)
15/08/16 12:52:11 INFO Executor: Running task 92.0 in stage 25.0 (TID 2128)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 75.0 in stage 25.0 (TID 2111) in 32 ms on localhost (74/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 69.0 in stage 25.0 (TID 2105) in 46 ms on localhost (75/200)
15/08/16 12:52:11 INFO Executor: Running task 86.0 in stage 25.0 (TID 2122)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 74.0 in stage 25.0 (TID 2110) in 34 ms on localhost (76/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 73.0 in stage 25.0 (TID 2109) in 38 ms on localhost (77/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 76.0 in stage 25.0 (TID 2112). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 78.0 in stage 25.0 (TID 2114). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 81.0 in stage 25.0 (TID 2117). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 93.0 in stage 25.0 (TID 2129, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 93.0 in stage 25.0 (TID 2129)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 94.0 in stage 25.0 (TID 2130, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 95.0 in stage 25.0 (TID 2131, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 76.0 in stage 25.0 (TID 2112) in 36 ms on localhost (78/200)
15/08/16 12:52:11 INFO Executor: Running task 95.0 in stage 25.0 (TID 2131)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 81.0 in stage 25.0 (TID 2117) in 30 ms on localhost (79/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 78.0 in stage 25.0 (TID 2114) in 34 ms on localhost (80/200)
15/08/16 12:52:11 INFO Executor: Running task 94.0 in stage 25.0 (TID 2130)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 79.0 in stage 25.0 (TID 2115). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 80.0 in stage 25.0 (TID 2116). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 96.0 in stage 25.0 (TID 2132, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 97.0 in stage 25.0 (TID 2133, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Running task 97.0 in stage 25.0 (TID 2133)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 79.0 in stage 25.0 (TID 2115) in 36 ms on localhost (81/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 80.0 in stage 25.0 (TID 2116) in 36 ms on localhost (82/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Running task 96.0 in stage 25.0 (TID 2132)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:52:11 INFO Executor: Finished task 84.0 in stage 25.0 (TID 2120). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 98.0 in stage 25.0 (TID 2134, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 83.0 in stage 25.0 (TID 2119). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 85.0 in stage 25.0 (TID 2121). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 99.0 in stage 25.0 (TID 2135, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 99.0 in stage 25.0 (TID 2135)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 100.0 in stage 25.0 (TID 2136, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 87.0 in stage 25.0 (TID 2123). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 86.0 in stage 25.0 (TID 2122). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 101.0 in stage 25.0 (TID 2137, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 102.0 in stage 25.0 (TID 2138, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 102.0 in stage 25.0 (TID 2138)
15/08/16 12:52:11 INFO Executor: Finished task 82.0 in stage 25.0 (TID 2118). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 92.0 in stage 25.0 (TID 2128). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/16 12:52:11 INFO Executor: Running task 100.0 in stage 25.0 (TID 2136)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 84.0 in stage 25.0 (TID 2120) in 39 ms on localhost (83/200)
15/08/16 12:52:11 INFO Executor: Running task 101.0 in stage 25.0 (TID 2137)
15/08/16 12:52:11 INFO Executor: Running task 98.0 in stage 25.0 (TID 2134)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 103.0 in stage 25.0 (TID 2139, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/16 12:52:11 INFO Executor: Running task 103.0 in stage 25.0 (TID 2139)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 85.0 in stage 25.0 (TID 2121) in 36 ms on localhost (84/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 83.0 in stage 25.0 (TID 2119) in 43 ms on localhost (85/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 104.0 in stage 25.0 (TID 2140, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 104.0 in stage 25.0 (TID 2140)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 82.0 in stage 25.0 (TID 2118) in 46 ms on localhost (86/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 87.0 in stage 25.0 (TID 2123) in 33 ms on localhost (87/200)
15/08/16 12:52:11 INFO Executor: Finished task 93.0 in stage 25.0 (TID 2129). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 105.0 in stage 25.0 (TID 2141, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 92.0 in stage 25.0 (TID 2128) in 29 ms on localhost (88/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Running task 105.0 in stage 25.0 (TID 2141)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 93.0 in stage 25.0 (TID 2129) in 25 ms on localhost (89/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 86.0 in stage 25.0 (TID 2122) in 37 ms on localhost (90/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 97.0 in stage 25.0 (TID 2133). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 94.0 in stage 25.0 (TID 2130). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 106.0 in stage 25.0 (TID 2142, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 107.0 in stage 25.0 (TID 2143, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 106.0 in stage 25.0 (TID 2142)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 94.0 in stage 25.0 (TID 2130) in 36 ms on localhost (91/200)
15/08/16 12:52:11 INFO Executor: Running task 107.0 in stage 25.0 (TID 2143)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 97.0 in stage 25.0 (TID 2133) in 32 ms on localhost (92/200)
15/08/16 12:52:11 INFO Executor: Finished task 88.0 in stage 25.0 (TID 2124). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 108.0 in stage 25.0 (TID 2144, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 91.0 in stage 25.0 (TID 2127). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 109.0 in stage 25.0 (TID 2145, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 109.0 in stage 25.0 (TID 2145)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 88.0 in stage 25.0 (TID 2124) in 60 ms on localhost (93/200)
15/08/16 12:52:11 INFO Executor: Finished task 96.0 in stage 25.0 (TID 2132). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 110.0 in stage 25.0 (TID 2146, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 108.0 in stage 25.0 (TID 2144)
15/08/16 12:52:11 INFO Executor: Running task 110.0 in stage 25.0 (TID 2146)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 91.0 in stage 25.0 (TID 2127) in 58 ms on localhost (94/200)
15/08/16 12:52:11 INFO Executor: Finished task 102.0 in stage 25.0 (TID 2138). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 111.0 in stage 25.0 (TID 2147, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 95.0 in stage 25.0 (TID 2131). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 96.0 in stage 25.0 (TID 2132) in 50 ms on localhost (95/200)
15/08/16 12:52:11 INFO Executor: Running task 111.0 in stage 25.0 (TID 2147)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 112.0 in stage 25.0 (TID 2148, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 112.0 in stage 25.0 (TID 2148)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 95.0 in stage 25.0 (TID 2131) in 57 ms on localhost (96/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 104.0 in stage 25.0 (TID 2140). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 99.0 in stage 25.0 (TID 2135). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 113.0 in stage 25.0 (TID 2149, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 114.0 in stage 25.0 (TID 2150, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 113.0 in stage 25.0 (TID 2149)
15/08/16 12:52:11 INFO Executor: Running task 114.0 in stage 25.0 (TID 2150)
15/08/16 12:52:11 INFO Executor: Finished task 90.0 in stage 25.0 (TID 2126). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 104.0 in stage 25.0 (TID 2140) in 42 ms on localhost (97/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 115.0 in stage 25.0 (TID 2151, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 105.0 in stage 25.0 (TID 2141). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 115.0 in stage 25.0 (TID 2151)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 116.0 in stage 25.0 (TID 2152, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 103.0 in stage 25.0 (TID 2139). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 98.0 in stage 25.0 (TID 2134). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 90.0 in stage 25.0 (TID 2126) in 70 ms on localhost (98/200)
15/08/16 12:52:11 INFO Executor: Running task 116.0 in stage 25.0 (TID 2152)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 102.0 in stage 25.0 (TID 2138) in 48 ms on localhost (99/200)
15/08/16 12:52:11 INFO Executor: Finished task 100.0 in stage 25.0 (TID 2136). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 105.0 in stage 25.0 (TID 2141) in 43 ms on localhost (100/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 117.0 in stage 25.0 (TID 2153, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 117.0 in stage 25.0 (TID 2153)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 118.0 in stage 25.0 (TID 2154, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 118.0 in stage 25.0 (TID 2154)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 119.0 in stage 25.0 (TID 2155, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 119.0 in stage 25.0 (TID 2155)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Finished task 103.0 in stage 25.0 (TID 2139) in 47 ms on localhost (101/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 99.0 in stage 25.0 (TID 2135) in 52 ms on localhost (102/200)
15/08/16 12:52:11 INFO Executor: Finished task 89.0 in stage 25.0 (TID 2125). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 120.0 in stage 25.0 (TID 2156, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 120.0 in stage 25.0 (TID 2156)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 98.0 in stage 25.0 (TID 2134) in 57 ms on localhost (103/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 100.0 in stage 25.0 (TID 2136) in 56 ms on localhost (104/200)
15/08/16 12:52:11 INFO Executor: Finished task 101.0 in stage 25.0 (TID 2137). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 121.0 in stage 25.0 (TID 2157, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 121.0 in stage 25.0 (TID 2157)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 101.0 in stage 25.0 (TID 2137) in 59 ms on localhost (105/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 89.0 in stage 25.0 (TID 2125) in 86 ms on localhost (106/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 107.0 in stage 25.0 (TID 2143). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 122.0 in stage 25.0 (TID 2158, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 122.0 in stage 25.0 (TID 2158)
15/08/16 12:52:11 INFO Executor: Finished task 106.0 in stage 25.0 (TID 2142). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 108.0 in stage 25.0 (TID 2144). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 110.0 in stage 25.0 (TID 2146). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 113.0 in stage 25.0 (TID 2149). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 123.0 in stage 25.0 (TID 2159, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 107.0 in stage 25.0 (TID 2143) in 53 ms on localhost (107/200)
15/08/16 12:52:11 INFO Executor: Finished task 111.0 in stage 25.0 (TID 2147). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 123.0 in stage 25.0 (TID 2159)
15/08/16 12:52:11 INFO Executor: Finished task 109.0 in stage 25.0 (TID 2145). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 124.0 in stage 25.0 (TID 2160, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 106.0 in stage 25.0 (TID 2142) in 56 ms on localhost (108/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 125.0 in stage 25.0 (TID 2161, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 124.0 in stage 25.0 (TID 2160)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 126.0 in stage 25.0 (TID 2162, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 116.0 in stage 25.0 (TID 2152). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 110.0 in stage 25.0 (TID 2146) in 38 ms on localhost (109/200)
15/08/16 12:52:11 INFO Executor: Running task 126.0 in stage 25.0 (TID 2162)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 127.0 in stage 25.0 (TID 2163, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 113.0 in stage 25.0 (TID 2149) in 29 ms on localhost (110/200)
15/08/16 12:52:11 INFO Executor: Running task 125.0 in stage 25.0 (TID 2161)
15/08/16 12:52:11 INFO Executor: Running task 127.0 in stage 25.0 (TID 2163)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 128.0 in stage 25.0 (TID 2164, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 111.0 in stage 25.0 (TID 2147) in 37 ms on localhost (111/200)
15/08/16 12:52:11 INFO Executor: Running task 128.0 in stage 25.0 (TID 2164)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 108.0 in stage 25.0 (TID 2144) in 44 ms on localhost (112/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 109.0 in stage 25.0 (TID 2145) in 44 ms on localhost (113/200)
15/08/16 12:52:11 INFO Executor: Finished task 115.0 in stage 25.0 (TID 2151). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 114.0 in stage 25.0 (TID 2150). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 129.0 in stage 25.0 (TID 2165, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 130.0 in stage 25.0 (TID 2166, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 119.0 in stage 25.0 (TID 2155). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 129.0 in stage 25.0 (TID 2165)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 131.0 in stage 25.0 (TID 2167, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 118.0 in stage 25.0 (TID 2154). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 132.0 in stage 25.0 (TID 2168, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 132.0 in stage 25.0 (TID 2168)
15/08/16 12:52:11 INFO Executor: Running task 130.0 in stage 25.0 (TID 2166)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 133.0 in stage 25.0 (TID 2169, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 116.0 in stage 25.0 (TID 2152) in 33 ms on localhost (114/200)
15/08/16 12:52:11 INFO Executor: Running task 133.0 in stage 25.0 (TID 2169)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 115.0 in stage 25.0 (TID 2151) in 34 ms on localhost (115/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 114.0 in stage 25.0 (TID 2150) in 36 ms on localhost (116/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 119.0 in stage 25.0 (TID 2155) in 32 ms on localhost (117/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 118.0 in stage 25.0 (TID 2154) in 33 ms on localhost (118/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 131.0 in stage 25.0 (TID 2167)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 112.0 in stage 25.0 (TID 2148). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 134.0 in stage 25.0 (TID 2170, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 112.0 in stage 25.0 (TID 2148) in 49 ms on localhost (119/200)
15/08/16 12:52:11 INFO Executor: Running task 134.0 in stage 25.0 (TID 2170)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 121.0 in stage 25.0 (TID 2157). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 135.0 in stage 25.0 (TID 2171, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 135.0 in stage 25.0 (TID 2171)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 121.0 in stage 25.0 (TID 2157) in 41 ms on localhost (120/200)
15/08/16 12:52:11 INFO Executor: Finished task 117.0 in stage 25.0 (TID 2153). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 136.0 in stage 25.0 (TID 2172, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 122.0 in stage 25.0 (TID 2158). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 117.0 in stage 25.0 (TID 2153) in 54 ms on localhost (121/200)
15/08/16 12:52:11 INFO Executor: Running task 136.0 in stage 25.0 (TID 2172)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 137.0 in stage 25.0 (TID 2173, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 137.0 in stage 25.0 (TID 2173)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 122.0 in stage 25.0 (TID 2158) in 40 ms on localhost (122/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 133.0 in stage 25.0 (TID 2169). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 138.0 in stage 25.0 (TID 2174, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 138.0 in stage 25.0 (TID 2174)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 133.0 in stage 25.0 (TID 2169) in 28 ms on localhost (123/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 127.0 in stage 25.0 (TID 2163). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 139.0 in stage 25.0 (TID 2175, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 120.0 in stage 25.0 (TID 2156). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 140.0 in stage 25.0 (TID 2176, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 139.0 in stage 25.0 (TID 2175)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 127.0 in stage 25.0 (TID 2163) in 39 ms on localhost (124/200)
15/08/16 12:52:11 INFO Executor: Running task 140.0 in stage 25.0 (TID 2176)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 120.0 in stage 25.0 (TID 2156) in 60 ms on localhost (125/200)
15/08/16 12:52:11 INFO Executor: Finished task 129.0 in stage 25.0 (TID 2165). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 141.0 in stage 25.0 (TID 2177, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 141.0 in stage 25.0 (TID 2177)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 129.0 in stage 25.0 (TID 2165) in 39 ms on localhost (126/200)
15/08/16 12:52:11 INFO Executor: Finished task 126.0 in stage 25.0 (TID 2162). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 142.0 in stage 25.0 (TID 2178, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 142.0 in stage 25.0 (TID 2178)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 126.0 in stage 25.0 (TID 2162) in 48 ms on localhost (127/200)
15/08/16 12:52:11 INFO Executor: Finished task 135.0 in stage 25.0 (TID 2171). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 125.0 in stage 25.0 (TID 2161). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 124.0 in stage 25.0 (TID 2160). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 143.0 in stage 25.0 (TID 2179, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 143.0 in stage 25.0 (TID 2179)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 144.0 in stage 25.0 (TID 2180, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 144.0 in stage 25.0 (TID 2180)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 145.0 in stage 25.0 (TID 2181, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 131.0 in stage 25.0 (TID 2167). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 135.0 in stage 25.0 (TID 2171) in 26 ms on localhost (128/200)
15/08/16 12:52:11 INFO Executor: Finished task 130.0 in stage 25.0 (TID 2166). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 146.0 in stage 25.0 (TID 2182, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 147.0 in stage 25.0 (TID 2183, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 145.0 in stage 25.0 (TID 2181)
15/08/16 12:52:11 INFO Executor: Running task 147.0 in stage 25.0 (TID 2183)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 131.0 in stage 25.0 (TID 2167) in 46 ms on localhost (129/200)
15/08/16 12:52:11 INFO Executor: Running task 146.0 in stage 25.0 (TID 2182)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 124.0 in stage 25.0 (TID 2160) in 55 ms on localhost (130/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 125.0 in stage 25.0 (TID 2161) in 54 ms on localhost (131/200)
15/08/16 12:52:11 INFO Executor: Finished task 137.0 in stage 25.0 (TID 2173). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 130.0 in stage 25.0 (TID 2166) in 47 ms on localhost (132/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 148.0 in stage 25.0 (TID 2184, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 128.0 in stage 25.0 (TID 2164). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 149.0 in stage 25.0 (TID 2185, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 128.0 in stage 25.0 (TID 2164) in 55 ms on localhost (133/200)
15/08/16 12:52:11 INFO Executor: Running task 149.0 in stage 25.0 (TID 2185)
15/08/16 12:52:11 INFO Executor: Running task 148.0 in stage 25.0 (TID 2184)
15/08/16 12:52:11 INFO Executor: Finished task 134.0 in stage 25.0 (TID 2170). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 150.0 in stage 25.0 (TID 2186, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 132.0 in stage 25.0 (TID 2168). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 123.0 in stage 25.0 (TID 2159). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 140.0 in stage 25.0 (TID 2176). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 138.0 in stage 25.0 (TID 2174). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:11 INFO Executor: Finished task 136.0 in stage 25.0 (TID 2172). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Finished task 137.0 in stage 25.0 (TID 2173) in 29 ms on localhost (134/200)
15/08/16 12:52:11 INFO Executor: Running task 150.0 in stage 25.0 (TID 2186)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 134.0 in stage 25.0 (TID 2170) in 51 ms on localhost (135/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 151.0 in stage 25.0 (TID 2187, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 152.0 in stage 25.0 (TID 2188, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 153.0 in stage 25.0 (TID 2189, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 132.0 in stage 25.0 (TID 2168) in 58 ms on localhost (136/200)
15/08/16 12:52:11 INFO Executor: Running task 153.0 in stage 25.0 (TID 2189)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Finished task 123.0 in stage 25.0 (TID 2159) in 72 ms on localhost (137/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 140.0 in stage 25.0 (TID 2176) in 29 ms on localhost (138/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 154.0 in stage 25.0 (TID 2190, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 155.0 in stage 25.0 (TID 2191, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 141.0 in stage 25.0 (TID 2177). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 156.0 in stage 25.0 (TID 2192, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 156.0 in stage 25.0 (TID 2192)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 136.0 in stage 25.0 (TID 2172) in 40 ms on localhost (139/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 138.0 in stage 25.0 (TID 2174) in 35 ms on localhost (140/200)
15/08/16 12:52:11 INFO Executor: Running task 154.0 in stage 25.0 (TID 2190)
15/08/16 12:52:11 INFO Executor: Running task 155.0 in stage 25.0 (TID 2191)
15/08/16 12:52:11 INFO Executor: Running task 152.0 in stage 25.0 (TID 2188)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 141.0 in stage 25.0 (TID 2177) in 27 ms on localhost (141/200)
15/08/16 12:52:11 INFO Executor: Running task 151.0 in stage 25.0 (TID 2187)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/16 12:52:11 INFO Executor: Finished task 139.0 in stage 25.0 (TID 2175). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 157.0 in stage 25.0 (TID 2193, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 157.0 in stage 25.0 (TID 2193)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 139.0 in stage 25.0 (TID 2175) in 37 ms on localhost (142/200)
15/08/16 12:52:11 INFO Executor: Finished task 142.0 in stage 25.0 (TID 2178). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 158.0 in stage 25.0 (TID 2194, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 158.0 in stage 25.0 (TID 2194)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 142.0 in stage 25.0 (TID 2178) in 34 ms on localhost (143/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 145.0 in stage 25.0 (TID 2181). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 159.0 in stage 25.0 (TID 2195, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 159.0 in stage 25.0 (TID 2195)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 145.0 in stage 25.0 (TID 2181) in 36 ms on localhost (144/200)
15/08/16 12:52:11 INFO Executor: Finished task 147.0 in stage 25.0 (TID 2183). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 160.0 in stage 25.0 (TID 2196, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 160.0 in stage 25.0 (TID 2196)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 147.0 in stage 25.0 (TID 2183) in 38 ms on localhost (145/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 143.0 in stage 25.0 (TID 2179). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 161.0 in stage 25.0 (TID 2197, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Running task 161.0 in stage 25.0 (TID 2197)
15/08/16 12:52:11 INFO Executor: Finished task 149.0 in stage 25.0 (TID 2185). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 153.0 in stage 25.0 (TID 2189). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 162.0 in stage 25.0 (TID 2198, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 148.0 in stage 25.0 (TID 2184). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 146.0 in stage 25.0 (TID 2182). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 162.0 in stage 25.0 (TID 2198)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 143.0 in stage 25.0 (TID 2179) in 46 ms on localhost (146/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 149.0 in stage 25.0 (TID 2185) in 40 ms on localhost (147/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 163.0 in stage 25.0 (TID 2199, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 163.0 in stage 25.0 (TID 2199)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 164.0 in stage 25.0 (TID 2200, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 144.0 in stage 25.0 (TID 2180). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 164.0 in stage 25.0 (TID 2200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 165.0 in stage 25.0 (TID 2201, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 166.0 in stage 25.0 (TID 2202, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 153.0 in stage 25.0 (TID 2189) in 32 ms on localhost (148/200)
15/08/16 12:52:11 INFO Executor: Finished task 154.0 in stage 25.0 (TID 2190). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 146.0 in stage 25.0 (TID 2182) in 47 ms on localhost (149/200)
15/08/16 12:52:11 INFO Executor: Running task 166.0 in stage 25.0 (TID 2202)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 148.0 in stage 25.0 (TID 2184) in 45 ms on localhost (150/200)
15/08/16 12:52:11 INFO Executor: Running task 165.0 in stage 25.0 (TID 2201)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 167.0 in stage 25.0 (TID 2203, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Finished task 144.0 in stage 25.0 (TID 2180) in 49 ms on localhost (151/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 154.0 in stage 25.0 (TID 2190) in 32 ms on localhost (152/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 150.0 in stage 25.0 (TID 2186). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 168.0 in stage 25.0 (TID 2204, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 157.0 in stage 25.0 (TID 2193). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 169.0 in stage 25.0 (TID 2205, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 169.0 in stage 25.0 (TID 2205)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 167.0 in stage 25.0 (TID 2203)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 150.0 in stage 25.0 (TID 2186) in 45 ms on localhost (153/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 157.0 in stage 25.0 (TID 2193) in 33 ms on localhost (154/200)
15/08/16 12:52:11 INFO Executor: Running task 168.0 in stage 25.0 (TID 2204)
15/08/16 12:52:11 INFO Executor: Finished task 151.0 in stage 25.0 (TID 2187). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 170.0 in stage 25.0 (TID 2206, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Running task 170.0 in stage 25.0 (TID 2206)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 151.0 in stage 25.0 (TID 2187) in 48 ms on localhost (155/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 161.0 in stage 25.0 (TID 2197). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 171.0 in stage 25.0 (TID 2207, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 171.0 in stage 25.0 (TID 2207)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 161.0 in stage 25.0 (TID 2197) in 25 ms on localhost (156/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/16 12:52:11 INFO Executor: Finished task 152.0 in stage 25.0 (TID 2188). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 172.0 in stage 25.0 (TID 2208, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 172.0 in stage 25.0 (TID 2208)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 152.0 in stage 25.0 (TID 2188) in 58 ms on localhost (157/200)
15/08/16 12:52:11 INFO Executor: Finished task 155.0 in stage 25.0 (TID 2191). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 158.0 in stage 25.0 (TID 2194). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 164.0 in stage 25.0 (TID 2200). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 173.0 in stage 25.0 (TID 2209, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 173.0 in stage 25.0 (TID 2209)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 174.0 in stage 25.0 (TID 2210, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 156.0 in stage 25.0 (TID 2192). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Finished task 162.0 in stage 25.0 (TID 2198). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 174.0 in stage 25.0 (TID 2210)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 175.0 in stage 25.0 (TID 2211, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 158.0 in stage 25.0 (TID 2194) in 51 ms on localhost (158/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 155.0 in stage 25.0 (TID 2191) in 60 ms on localhost (159/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 176.0 in stage 25.0 (TID 2212, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 166.0 in stage 25.0 (TID 2202). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 175.0 in stage 25.0 (TID 2211)
15/08/16 12:52:11 INFO Executor: Running task 176.0 in stage 25.0 (TID 2212)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 177.0 in stage 25.0 (TID 2213, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 164.0 in stage 25.0 (TID 2200) in 33 ms on localhost (160/200)
15/08/16 12:52:11 INFO Executor: Running task 177.0 in stage 25.0 (TID 2213)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 178.0 in stage 25.0 (TID 2214, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO Executor: Running task 178.0 in stage 25.0 (TID 2214)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 156.0 in stage 25.0 (TID 2192) in 61 ms on localhost (161/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 162.0 in stage 25.0 (TID 2198) in 37 ms on localhost (162/200)
15/08/16 12:52:11 INFO Executor: Finished task 160.0 in stage 25.0 (TID 2196). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 179.0 in stage 25.0 (TID 2215, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 160.0 in stage 25.0 (TID 2196) in 45 ms on localhost (163/200)
15/08/16 12:52:11 INFO Executor: Running task 179.0 in stage 25.0 (TID 2215)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 169.0 in stage 25.0 (TID 2205). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 180.0 in stage 25.0 (TID 2216, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 180.0 in stage 25.0 (TID 2216)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 166.0 in stage 25.0 (TID 2202) in 39 ms on localhost (164/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 169.0 in stage 25.0 (TID 2205) in 34 ms on localhost (165/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 159.0 in stage 25.0 (TID 2195). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 181.0 in stage 25.0 (TID 2217, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 181.0 in stage 25.0 (TID 2217)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 159.0 in stage 25.0 (TID 2195) in 60 ms on localhost (166/200)
15/08/16 12:52:11 INFO Executor: Finished task 168.0 in stage 25.0 (TID 2204). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 182.0 in stage 25.0 (TID 2218, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 182.0 in stage 25.0 (TID 2218)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 168.0 in stage 25.0 (TID 2204) in 43 ms on localhost (167/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 165.0 in stage 25.0 (TID 2201). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 174.0 in stage 25.0 (TID 2210). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 167.0 in stage 25.0 (TID 2203). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 163.0 in stage 25.0 (TID 2199). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 183.0 in stage 25.0 (TID 2219, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 171.0 in stage 25.0 (TID 2207). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 184.0 in stage 25.0 (TID 2220, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 184.0 in stage 25.0 (TID 2220)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 185.0 in stage 25.0 (TID 2221, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 186.0 in stage 25.0 (TID 2222, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 178.0 in stage 25.0 (TID 2214). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 183.0 in stage 25.0 (TID 2219)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 165.0 in stage 25.0 (TID 2201) in 56 ms on localhost (168/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 174.0 in stage 25.0 (TID 2210) in 29 ms on localhost (169/200)
15/08/16 12:52:11 INFO Executor: Running task 185.0 in stage 25.0 (TID 2221)
15/08/16 12:52:11 INFO Executor: Running task 186.0 in stage 25.0 (TID 2222)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 187.0 in stage 25.0 (TID 2223, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 177.0 in stage 25.0 (TID 2213). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 187.0 in stage 25.0 (TID 2223)
15/08/16 12:52:11 INFO Executor: Finished task 170.0 in stage 25.0 (TID 2206). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 188.0 in stage 25.0 (TID 2224, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 188.0 in stage 25.0 (TID 2224)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 189.0 in stage 25.0 (TID 2225, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 189.0 in stage 25.0 (TID 2225)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 167.0 in stage 25.0 (TID 2203) in 56 ms on localhost (170/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 163.0 in stage 25.0 (TID 2199) in 60 ms on localhost (171/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 190.0 in stage 25.0 (TID 2226, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 190.0 in stage 25.0 (TID 2226)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 170.0 in stage 25.0 (TID 2206) in 46 ms on localhost (172/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 178.0 in stage 25.0 (TID 2214) in 27 ms on localhost (173/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 177.0 in stage 25.0 (TID 2213) in 30 ms on localhost (174/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 171.0 in stage 25.0 (TID 2207) in 43 ms on localhost (175/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 173.0 in stage 25.0 (TID 2209). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 191.0 in stage 25.0 (TID 2227, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 191.0 in stage 25.0 (TID 2227)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 173.0 in stage 25.0 (TID 2209) in 39 ms on localhost (176/200)
15/08/16 12:52:11 INFO Executor: Finished task 181.0 in stage 25.0 (TID 2217). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 192.0 in stage 25.0 (TID 2228, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 175.0 in stage 25.0 (TID 2211). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 180.0 in stage 25.0 (TID 2216). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 193.0 in stage 25.0 (TID 2229, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 192.0 in stage 25.0 (TID 2228)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 194.0 in stage 25.0 (TID 2230, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 181.0 in stage 25.0 (TID 2217) in 28 ms on localhost (177/200)
15/08/16 12:52:11 INFO Executor: Finished task 172.0 in stage 25.0 (TID 2208). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Starting task 195.0 in stage 25.0 (TID 2231, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 175.0 in stage 25.0 (TID 2211) in 46 ms on localhost (178/200)
15/08/16 12:52:11 INFO Executor: Running task 193.0 in stage 25.0 (TID 2229)
15/08/16 12:52:11 INFO Executor: Running task 195.0 in stage 25.0 (TID 2231)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 172.0 in stage 25.0 (TID 2208) in 50 ms on localhost (179/200)
15/08/16 12:52:11 INFO Executor: Running task 194.0 in stage 25.0 (TID 2230)
15/08/16 12:52:11 INFO Executor: Finished task 179.0 in stage 25.0 (TID 2215). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO Executor: Finished task 176.0 in stage 25.0 (TID 2212). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 179.0 in stage 25.0 (TID 2215) in 46 ms on localhost (180/200)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 196.0 in stage 25.0 (TID 2232, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 196.0 in stage 25.0 (TID 2232)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 197.0 in stage 25.0 (TID 2233, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Running task 197.0 in stage 25.0 (TID 2233)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 180.0 in stage 25.0 (TID 2216) in 42 ms on localhost (181/200)
15/08/16 12:52:11 INFO Executor: Finished task 183.0 in stage 25.0 (TID 2219). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Starting task 198.0 in stage 25.0 (TID 2234, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 176.0 in stage 25.0 (TID 2212) in 50 ms on localhost (182/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 183.0 in stage 25.0 (TID 2219) in 30 ms on localhost (183/200)
15/08/16 12:52:11 INFO Executor: Finished task 185.0 in stage 25.0 (TID 2221). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Running task 198.0 in stage 25.0 (TID 2234)
15/08/16 12:52:11 INFO TaskSetManager: Starting task 199.0 in stage 25.0 (TID 2235, localhost, PROCESS_LOCAL, 1154 bytes)
15/08/16 12:52:11 INFO Executor: Finished task 182.0 in stage 25.0 (TID 2218). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 184.0 in stage 25.0 (TID 2220). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 189.0 in stage 25.0 (TID 2225). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 182.0 in stage 25.0 (TID 2218) in 38 ms on localhost (184/200)
15/08/16 12:52:11 INFO Executor: Running task 199.0 in stage 25.0 (TID 2235)
15/08/16 12:52:11 INFO Executor: Finished task 188.0 in stage 25.0 (TID 2224). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 185.0 in stage 25.0 (TID 2221) in 32 ms on localhost (185/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 184.0 in stage 25.0 (TID 2220) in 34 ms on localhost (186/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 189.0 in stage 25.0 (TID 2225) in 31 ms on localhost (187/200)
15/08/16 12:52:11 INFO Executor: Finished task 190.0 in stage 25.0 (TID 2226). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 188.0 in stage 25.0 (TID 2224) in 32 ms on localhost (188/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/16 12:52:11 INFO TaskSetManager: Finished task 190.0 in stage 25.0 (TID 2226) in 32 ms on localhost (189/200)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 174 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/16 12:52:11 INFO Executor: Finished task 187.0 in stage 25.0 (TID 2223). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 191.0 in stage 25.0 (TID 2227). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 187.0 in stage 25.0 (TID 2223) in 37 ms on localhost (190/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 191.0 in stage 25.0 (TID 2227) in 30 ms on localhost (191/200)
15/08/16 12:52:11 INFO Executor: Finished task 192.0 in stage 25.0 (TID 2228). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 192.0 in stage 25.0 (TID 2228) in 31 ms on localhost (192/200)
15/08/16 12:52:11 INFO Executor: Finished task 193.0 in stage 25.0 (TID 2229). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 193.0 in stage 25.0 (TID 2229) in 46 ms on localhost (193/200)
15/08/16 12:52:11 INFO Executor: Finished task 194.0 in stage 25.0 (TID 2230). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 186.0 in stage 25.0 (TID 2222). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 194.0 in stage 25.0 (TID 2230) in 47 ms on localhost (194/200)
15/08/16 12:52:11 INFO Executor: Finished task 196.0 in stage 25.0 (TID 2232). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 197.0 in stage 25.0 (TID 2233). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO Executor: Finished task 199.0 in stage 25.0 (TID 2235). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 186.0 in stage 25.0 (TID 2222) in 63 ms on localhost (195/200)
15/08/16 12:52:11 INFO Executor: Finished task 195.0 in stage 25.0 (TID 2231). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 196.0 in stage 25.0 (TID 2232) in 41 ms on localhost (196/200)
15/08/16 12:52:11 INFO Executor: Finished task 198.0 in stage 25.0 (TID 2234). 1019 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 197.0 in stage 25.0 (TID 2233) in 40 ms on localhost (197/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 199.0 in stage 25.0 (TID 2235) in 38 ms on localhost (198/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 195.0 in stage 25.0 (TID 2231) in 49 ms on localhost (199/200)
15/08/16 12:52:11 INFO TaskSetManager: Finished task 198.0 in stage 25.0 (TID 2234) in 41 ms on localhost (200/200)
15/08/16 12:52:11 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
15/08/16 12:52:11 INFO DAGScheduler: ShuffleMapStage 25 (processCmd at CliDriver.java:423) finished in 0.828 s
15/08/16 12:52:11 INFO DAGScheduler: looking for newly runnable stages
15/08/16 12:52:11 INFO DAGScheduler: running: Set()
15/08/16 12:52:11 INFO DAGScheduler: waiting: Set(ResultStage 26)
15/08/16 12:52:11 INFO DAGScheduler: failed: Set()
15/08/16 12:52:11 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@64e7c28f
15/08/16 12:52:11 INFO DAGScheduler: Missing parents for ResultStage 26: List()
15/08/16 12:52:11 INFO StatsReportListener: task runtime:(count: 200, mean: 67.230000, stdev: 83.106059, max: 360.000000, min: 25.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	25.0 ms	29.0 ms	30.0 ms	35.0 ms	43.0 ms	54.0 ms	72.0 ms	342.0 ms	360.0 ms
15/08/16 12:52:11 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[88] at processCmd at CliDriver.java:423), which is now runnable
15/08/16 12:52:11 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 121.525000, stdev: 2.139947, max: 137.000000, min: 119.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	119.0 B	120.0 B	120.0 B	121.0 B	121.0 B	122.0 B	122.0 B	122.0 B	137.0 B
15/08/16 12:52:11 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.210000, stdev: 0.784793, max: 8.000000, min: 0.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	8.0 ms
15/08/16 12:52:11 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/16 12:52:11 INFO StatsReportListener: task result size:(count: 200, mean: 1019.000000, stdev: 0.000000, max: 1019.000000, min: 1019.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	1019.0 B	1019.0 B	1019.0 B	1019.0 B	1019.0 B	1019.0 B	1019.0 B	1019.0 B	1019.0 B
15/08/16 12:52:11 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 61.511931, stdev: 14.604883, max: 94.034091, min: 7.309942)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	 7 %	41 %	48 %	54 %	63 %	69 %	77 %	83 %	94 %
15/08/16 12:52:11 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.448822, stdev: 1.745330, max: 16.000000, min: 0.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %	 3 %	16 %
15/08/16 12:52:11 INFO StatsReportListener: other time pct: (count: 200, mean: 38.039246, stdev: 14.634866, max: 92.690058, min: 5.698006)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	 6 %	18 %	23 %	30 %	37 %	45 %	52 %	59 %	93 %
15/08/16 12:52:11 INFO MemoryStore: ensureFreeSpace(87400) called with curMem=508824772, maxMem=3333968363
15/08/16 12:52:11 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 85.4 KB, free 2.6 GB)
15/08/16 12:52:11 INFO MemoryStore: ensureFreeSpace(33659) called with curMem=508912172, maxMem=3333968363
15/08/16 12:52:11 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 32.9 KB, free 2.6 GB)
15/08/16 12:52:11 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:36543 (size: 32.9 KB, free: 3.1 GB)
15/08/16 12:52:11 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:874
15/08/16 12:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[88] at processCmd at CliDriver.java:423)
15/08/16 12:52:11 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
15/08/16 12:52:11 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 2236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/16 12:52:11 INFO Executor: Running task 0.0 in stage 26.0 (TID 2236)
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/16 12:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/16 12:52:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/16 12:52:11 INFO CodecConfig: Compression: GZIP
15/08/16 12:52:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/16 12:52:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/16 12:52:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/16 12:52:11 INFO ParquetOutputFormat: Dictionary is on
15/08/16 12:52:11 INFO ParquetOutputFormat: Validation is off
15/08/16 12:52:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/16 12:52:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/16 12:52:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,312
15/08/16 12:52:11 INFO ColumnChunkPageWriteStore: written 445B for [s_name] BINARY: 100 values, 2,207B raw, 381B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/16 12:52:11 INFO ColumnChunkPageWriteStore: written 79B for [numwait] INT32: 100 values, 25B raw, 45B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 9 entries, 36B raw, 9B comp}
15/08/16 12:52:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508161252_0026_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_suppliers_who_kept_orders_waiting_par/_temporary/0/task_201508161252_0026_m_000000
15/08/16 12:52:11 INFO SparkHadoopMapRedUtil: attempt_201508161252_0026_m_000000_0: Committed
15/08/16 12:52:11 INFO Executor: Finished task 0.0 in stage 26.0 (TID 2236). 577 bytes result sent to driver
15/08/16 12:52:11 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 2236) in 134 ms on localhost (1/1)
15/08/16 12:52:11 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
15/08/16 12:52:11 INFO DAGScheduler: ResultStage 26 (processCmd at CliDriver.java:423) finished in 0.134 s
15/08/16 12:52:11 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@689c0fe1
15/08/16 12:52:11 INFO StatsReportListener: task runtime:(count: 1, mean: 134.000000, stdev: 0.000000, max: 134.000000, min: 134.000000)
15/08/16 12:52:11 INFO DAGScheduler: Job 8 finished: processCmd at CliDriver.java:423, took 6.653959 s
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	134.0 ms	134.0 ms	134.0 ms	134.0 ms	134.0 ms	134.0 ms	134.0 ms	134.0 ms	134.0 ms
15/08/16 12:52:11 INFO StatsReportListener: task result size:(count: 1, mean: 577.000000, stdev: 0.000000, max: 577.000000, min: 577.000000)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B
15/08/16 12:52:11 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 73.880597, stdev: 0.000000, max: 73.880597, min: 73.880597)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %
15/08/16 12:52:11 INFO StatsReportListener: other time pct: (count: 1, mean: 26.119403, stdev: 0.000000, max: 26.119403, min: 26.119403)
15/08/16 12:52:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:11 INFO StatsReportListener: 	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %
15/08/16 12:52:11 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:52:11 INFO DefaultWriterContainer: Job job_201508161252_0000 committed.
15/08/16 12:52:11 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/16 12:52:11 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q21_suppliers_who_kept_orders_waiting_par/_common_metadata
15/08/16 12:52:12 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/16 12:52:12 INFO DAGScheduler: Got job 9 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/16 12:52:12 INFO DAGScheduler: Final stage: ResultStage 27(processCmd at CliDriver.java:423)
15/08/16 12:52:12 INFO DAGScheduler: Parents of final stage: List()
15/08/16 12:52:12 INFO DAGScheduler: Missing parents: List()
15/08/16 12:52:12 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[90] at processCmd at CliDriver.java:423), which has no missing parents
15/08/16 12:52:12 INFO MemoryStore: ensureFreeSpace(2952) called with curMem=508945831, maxMem=3333968363
15/08/16 12:52:12 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 2.9 KB, free 2.6 GB)
15/08/16 12:52:12 INFO MemoryStore: ensureFreeSpace(1771) called with curMem=508948783, maxMem=3333968363
15/08/16 12:52:12 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1771.0 B, free 2.6 GB)
15/08/16 12:52:12 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:36543 (size: 1771.0 B, free: 3.1 GB)
15/08/16 12:52:12 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:874
15/08/16 12:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[90] at processCmd at CliDriver.java:423)
15/08/16 12:52:12 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
15/08/16 12:52:12 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 2237, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/16 12:52:12 INFO Executor: Running task 0.0 in stage 27.0 (TID 2237)
15/08/16 12:52:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 2237). 606 bytes result sent to driver
15/08/16 12:52:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 2237) in 8 ms on localhost (1/1)
15/08/16 12:52:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
15/08/16 12:52:12 INFO DAGScheduler: ResultStage 27 (processCmd at CliDriver.java:423) finished in 0.009 s
15/08/16 12:52:12 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@15c6c819
15/08/16 12:52:12 INFO DAGScheduler: Job 9 finished: processCmd at CliDriver.java:423, took 0.018916 s
15/08/16 12:52:12 INFO StatsReportListener: task runtime:(count: 1, mean: 8.000000, stdev: 0.000000, max: 8.000000, min: 8.000000)
15/08/16 12:52:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:12 INFO StatsReportListener: 	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms
15/08/16 12:52:12 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
Time taken: 39.985 seconds15/08/16 12:52:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:12 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B

15/08/16 12:52:12 INFO CliDriver: Time taken: 39.985 seconds
15/08/16 12:52:12 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/16 12:52:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:12 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/16 12:52:12 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/16 12:52:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/16 12:52:12 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/16 12:52:12 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/16 12:52:12 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/16 12:52:12 INFO DAGScheduler: Stopping DAGScheduler
15/08/16 12:52:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/16 12:52:12 INFO Utils: path = /tmp/spark-3f2fd1ef-07da-4991-a44b-f8b0f5341524/blockmgr-c8900d73-f3eb-417b-93a2-5c47174a33f4, already present as root for deletion.
15/08/16 12:52:12 INFO MemoryStore: MemoryStore cleared
15/08/16 12:52:12 INFO BlockManager: BlockManager stopped
15/08/16 12:52:12 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/16 12:52:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/16 12:52:12 INFO SparkContext: Successfully stopped SparkContext
15/08/16 12:52:12 INFO Utils: Shutdown hook called
15/08/16 12:52:12 INFO Utils: Deleting directory /tmp/spark-96f02786-7ec9-41d7-94f0-ffd1f16fc897
15/08/16 12:52:12 INFO Utils: Deleting directory /tmp/spark-3f2fd1ef-07da-4991-a44b-f8b0f5341524
15/08/16 12:52:12 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/16 12:52:12 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/16 12:52:12 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/16 12:52:12 INFO Utils: Deleting directory /tmp/spark-d2498e9a-8fb3-4049-ad8b-5b11629c5ef4
