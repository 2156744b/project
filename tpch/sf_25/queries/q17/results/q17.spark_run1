 -- the query
insert into table lineitem_tmp
select 
  l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
from 
  lineitem
group by l_partkey;

insert into table q17_small_quantity_order_revenue
select
  sum(l_extendedprice) / 7.0 as avg_yearly
from
  (select l_quantity, l_extendedprice, t_avg_quantity from
   lineitem_tmp t join
     (select
        l_quantity, l_partkey, l_extendedprice
      from
        part p join lineitem l
        on
          p.p_partkey = l.l_partkey
          and p.p_brand = 'Brand#54'
          and p.p_container = 'SM BAG'
      ) l1 on l1.l_partkey = t.t_partkey
   ) a
where l_quantity < t_avg_quantity;
15/08/18 03:16:53 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/18 03:16:53 INFO metastore: Connected to metastore.
15/08/18 03:16:54 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/18 03:16:54 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:54 INFO SparkContext: Running Spark version 1.4.1
15/08/18 03:16:54 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:54 INFO SecurityManager: Changing view acls to: hive
15/08/18 03:16:54 INFO SecurityManager: Changing modify acls to: hive
15/08/18 03:16:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/18 03:16:55 INFO Slf4jLogger: Slf4jLogger started
15/08/18 03:16:55 INFO Remoting: Starting remoting
15/08/18 03:16:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:58379]
15/08/18 03:16:56 INFO Utils: Successfully started service 'sparkDriver' on port 58379.
15/08/18 03:16:56 INFO SparkEnv: Registering MapOutputTracker
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 INFO SparkEnv: Registering BlockManagerMaster
15/08/18 03:16:56 INFO DiskBlockManager: Created local directory at /tmp/spark-a2254311-8b5d-454a-b85a-3f290e9d6e39/blockmgr-0c6acbd6-d73d-450a-9be3-5f218b949fc2
15/08/18 03:16:56 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a2254311-8b5d-454a-b85a-3f290e9d6e39/httpd-117afc49-6a2b-4d39-8580-bd7f8ef21db0
15/08/18 03:16:56 INFO HttpServer: Starting HTTP Server
15/08/18 03:16:56 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/18 03:16:56 INFO AbstractConnector: Started SocketConnector@0.0.0.0:45715
15/08/18 03:16:56 INFO Utils: Successfully started service 'HTTP file server' on port 45715.
15/08/18 03:16:56 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/18 03:16:56 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/18 03:16:56 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/18 03:16:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/18 03:16:56 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:56 INFO Executor: Starting executor ID driver on host localhost
15/08/18 03:16:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47403.
15/08/18 03:16:56 INFO NettyBlockTransferService: Server created on 47403
15/08/18 03:16:56 INFO BlockManagerMaster: Trying to register BlockManager
15/08/18 03:16:56 INFO BlockManagerMasterEndpoint: Registering block manager localhost:47403 with 20.7 GB RAM, BlockManagerId(driver, localhost, 47403)
15/08/18 03:16:56 INFO BlockManagerMaster: Registered BlockManager
15/08/18 03:16:57 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/18 03:16:57 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/18 03:16:57 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/18 03:16:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/18 03:16:58 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/18 03:16:58 INFO metastore: Connected to metastore.
15/08/18 03:16:59 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/18 03:16:59 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/18 03:16:59 INFO ParseDriver: Parsing command: -- the query
insert into table lineitem_tmp
select 
  l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
from 
  lineitem
group by l_partkey
15/08/18 03:16:59 INFO ParseDriver: Parse Completed
15/08/18 03:17:00 INFO MemoryStore: ensureFreeSpace(592856) called with curMem=0, maxMem=22226833244
15/08/18 03:17:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 579.0 KB, free 20.7 GB)
15/08/18 03:17:01 INFO MemoryStore: ensureFreeSpace(39335) called with curMem=592856, maxMem=22226833244
15/08/18 03:17:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.4 KB, free 20.7 GB)
15/08/18 03:17:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:47403 (size: 38.4 KB, free: 20.7 GB)
15/08/18 03:17:01 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/18 03:17:01 INFO Exchange: Using SparkSqlSerializer2.
15/08/18 03:17:01 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/18 03:17:01 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/18 03:17:01 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/18 03:17:01 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/18 03:17:01 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/18 03:17:02 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/18 03:17:02 INFO PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
15/08/18 03:17:02 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
15/08/18 03:17:03 INFO OrcInputFormat: FooterCacheHitRatio: 0/21
15/08/18 03:17:03 INFO PerfLogger: </PERFLOG method=OrcGetSplits start=1439867822326 end=1439867823056 duration=730 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
15/08/18 03:17:03 INFO DAGScheduler: Registering RDD 4 (processCmd at CliDriver.java:423)
15/08/18 03:17:03 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/18 03:17:03 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/18 03:17:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/18 03:17:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/18 03:17:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at processCmd at CliDriver.java:423), which has no missing parents
15/08/18 03:17:03 INFO MemoryStore: ensureFreeSpace(14736) called with curMem=632191, maxMem=22226833244
15/08/18 03:17:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.4 KB, free 20.7 GB)
15/08/18 03:17:03 INFO MemoryStore: ensureFreeSpace(7116) called with curMem=646927, maxMem=22226833244
15/08/18 03:17:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KB, free 20.7 GB)
15/08/18 03:17:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:47403 (size: 6.9 KB, free: 20.7 GB)
15/08/18 03:17:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/18 03:17:03 INFO DAGScheduler: Submitting 22 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at processCmd at CliDriver.java:423)
15/08/18 03:17:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 22 tasks
15/08/18 03:17:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, ANY, 1454 bytes)
15/08/18 03:17:03 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/18 03:17:03 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/18 03:17:03 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/18 03:17:03 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/18 03:17:03 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/08/18 03:17:03 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/18 03:17:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/18 03:17:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/18 03:17:03 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/18 03:17:03 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/18 03:17:03 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
15/08/18 03:17:03 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
15/08/18 03:17:03 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
15/08/18 03:17:03 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
15/08/18 03:17:03 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
15/08/18 03:17:03 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000004_0:3+274835458
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000000_0:276205445+660142
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000001_0:3+275566914
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000006_0:3+274845117
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000005_0:3+274843815
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000020_0:0+252413137
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000003_0:3+274810222
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000002_0:3+274840179
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000012_0:3+274819528
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000000_0:3+276205442
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000009_0:3+274837283
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000007_0:3+274852701
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000015_0:3+272733084
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000008_0:3+274828317
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000010_0:3+274813487
15/08/18 03:17:03 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000017_0:3+272754271
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000004_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274835458}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000010_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274813487}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000006_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274845117}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000000_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 276205442}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000015_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 272733084}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000000_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 276205445, length: 660142}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000005_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274843815}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000012_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274819528}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000001_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 275566914}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000003_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274810222}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000007_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274852701}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000020_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 0, length: 252413137}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000009_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274837283}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000017_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 272754271}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000008_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274828317}
15/08/18 03:17:03 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000002_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274840179}
15/08/18 03:17:05 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/18 03:17:05 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, ANY, 1454 bytes)
15/08/18 03:17:05 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
15/08/18 03:17:05 INFO HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000011_0:3+274844459
15/08/18 03:17:05 INFO ReaderImpl: Reading ORC rows from hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem/000011_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], offset: 3, length: 274844459}
15/08/18 03:17:05 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1982 ms on localhost (1/22)
15/08/18 03:27:38 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 132360 ms exceeds timeout 120000 ms
15/08/18 03:27:38 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 132360 ms
15/08/18 03:27:38 INFO TaskSetManager: Re-queueing tasks for driver from TaskSet 0.0
15/08/18 03:27:38 INFO DAGScheduler: Resubmitted ShuffleMapTask(0, 3), so marking it as still running
15/08/18 03:27:38 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 ERROR TaskSetManager: Task 8 in stage 0.0 failed 1 times; aborting job
15/08/18 03:27:38 WARN TaskSetManager: Lost task 11.0 in stage 0.0 (TID 11, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 14.0 in stage 0.0 (TID 14, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 13.0 in stage 0.0 (TID 13, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 16.0 in stage 0.0 (TID 16, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 10.0 in stage 0.0 (TID 10, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 9.0 in stage 0.0 (TID 9, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 12.0 in stage 0.0 (TID 12, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 15.0 in stage 0.0 (TID 15, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 6.0 in stage 0.0 (TID 6, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): ExecutorLostFailure (executor driver lost)
15/08/18 03:27:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/18 03:27:38 INFO TaskSchedulerImpl: Cancelling stage 0
15/08/18 03:27:38 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) failed in 635.671 s
15/08/18 03:27:38 INFO DAGScheduler: Job 0 failed: processCmd at CliDriver.java:423, took 636.747464 s
15/08/18 03:27:38 ERROR SparkSQLDriver: Failed in [ -- the query
insert into table lineitem_tmp
select 
  l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
from 
  lineitem
group by l_partkey]
org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver lost)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/08/18 03:27:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4b56cf26
org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver lost)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

15/08/18 03:27:38 ERROR CliDriver: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver lost)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

15/08/18 03:27:38 INFO DAGScheduler: Executor lost: driver (epoch 0)
15/08/18 03:27:38 INFO BlockManagerMasterEndpoint: Trying to remove executor driver from BlockManagerMaster.
15/08/18 03:27:38 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(driver, localhost, 47403)
15/08/18 03:27:38 INFO BlockManagerMaster: Removed driver successfully in removeExecutor
15/08/18 03:27:38 WARN Executor: Told to re-register on heartbeat
15/08/18 03:27:38 INFO BlockManager: BlockManager re-registering with master
15/08/18 03:27:38 INFO BlockManagerMaster: Trying to register BlockManager
15/08/18 03:27:38 INFO DAGScheduler: Host added was in lost list earlier: localhost
15/08/18 03:27:38 INFO StatsReportListener: task runtime:(count: 1, mean: 1982.000000, stdev: 0.000000, max: 1982.000000, min: 1982.000000)
15/08/18 03:27:38 INFO BlockManagerMasterEndpoint: Registering block manager localhost:47403 with 20.7 GB RAM, BlockManagerId(driver, localhost, 47403)
15/08/18 03:27:38 INFO BlockManagerMaster: Registered BlockManager
15/08/18 03:27:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/18 03:27:39 INFO StatsReportListener: 	2.0 s	2.0 s	2.0 s	2.0 s	2.0 s	2.0 s	2.0 s	2.0 s	2.0 s
15/08/18 03:27:39 INFO BlockManager: Reporting 4 blocks to the master.
15/08/18 03:27:39 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 235189.000000, stdev: 0.000000, max: 235189.000000, min: 235189.000000)
15/08/18 03:27:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/18 03:27:39 INFO StatsReportListener: 	229.7 KB	229.7 KB	229.7 KB	229.7 KB	229.7 KB	229.7 KB	229.7 KB	229.7 KB	229.7 KB
15/08/18 03:27:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:47403 (size: 38.4 KB, free: 20.7 GB)
15/08/18 03:27:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:47403 (size: 6.9 KB, free: 20.7 GB)
15/08/18 03:27:39 INFO StatsReportListener: task result size:(count: 1, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/18 03:27:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/18 03:27:39 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/18 03:27:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 90.110999, stdev: 0.000000, max: 90.110999, min: 90.110999)
15/08/18 03:27:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/18 03:27:39 INFO StatsReportListener: 	90 %	90 %	90 %	90 %	90 %	90 %	90 %	90 %	90 %
15/08/18 03:27:39 INFO StatsReportListener: other time pct: (count: 1, mean: 9.889001, stdev: 0.000000, max: 9.889001, min: 9.889001)
15/08/18 03:27:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/18 03:27:39 INFO StatsReportListener: 	10 %	10 %	10 %	10 %	10 %	10 %	10 %	10 %	10 %
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/18 03:27:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/18 03:27:39 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/18 03:31:14 INFO DAGScheduler: Stopping DAGScheduler
15/08/18 03:31:14 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 215168 ms exceeds timeout 120000 ms
15/08/18 03:31:14 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 215168 ms
15/08/18 03:31:14 WARN AkkaRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7b76952d,BlockManagerId(driver, localhost, 47403))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#-537631076]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:444)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/08/18 03:34:45 WARN AkkaRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7b76952d,BlockManagerId(driver, localhost, 47403))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#-537631076]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:444)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/08/18 03:34:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/18 03:34:45 WARN AkkaRpcEndpointRef: Error sending message [message = StopMapOutputTracker] in 1 attempts
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://sparkDriver/user/MapOutputTracker#-862877880]] after [120000 ms]
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)
	at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
15/08/18 03:37:40 WARN AkkaRpcEndpointRef: Error sending message [message = StopMapOutputTracker] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/MapOutputTracker#-862877880]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:109)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:119)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:324)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:93)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1659)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anonfun$main$1.apply$mcV$sp(SparkSQLCLIDriver.scala:112)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
15/08/18 03:37:40 WARN AkkaRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7b76952d,BlockManagerId(driver, localhost, 47403))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#-537631076]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:444)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/08/18 03:41:01 WARN AkkaRpcEndpointRef: Error sending message [message = StopMapOutputTracker] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/MapOutputTracker#-862877880]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:109)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:119)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:324)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:93)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1659)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anonfun$main$1.apply$mcV$sp(SparkSQLCLIDriver.scala:112)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
15/08/18 03:41:01 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7b76952d,BlockManagerId(driver, localhost, 47403))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:116)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:444)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#-537631076]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 14 more
15/08/18 03:44:40 ERROR MapOutputTrackerMaster: Error communicating with MapOutputTracker
org.apache.spark.SparkException: Error sending message [message = StopMapOutputTracker]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:116)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:109)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:119)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:324)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:93)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1659)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anonfun$main$1.apply$mcV$sp(SparkSQLCLIDriver.scala:112)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/MapOutputTracker#-862877880]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 20 more
15/08/18 03:44:41 ERROR Utils: Uncaught exception in thread Thread-0
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:113)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:119)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:324)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:93)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1659)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anonfun$main$1.apply$mcV$sp(SparkSQLCLIDriver.scala:112)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: org.apache.spark.SparkException: Error sending message [message = StopMapOutputTracker]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:116)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:109)
	... 18 more
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/MapOutputTracker#-862877880]] had already been terminated.
	at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:132)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.ask(AkkaRpcEnv.scala:299)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 20 more
15/08/18 03:44:41 INFO DiskBlockManager: Shutdown hook called
15/08/18 03:44:41 INFO Utils: path = /tmp/spark-a2254311-8b5d-454a-b85a-3f290e9d6e39/blockmgr-0c6acbd6-d73d-450a-9be3-5f218b949fc2, already present as root for deletion.
15/08/18 03:44:41 INFO Utils: Shutdown hook called
15/08/18 03:44:41 INFO Utils: Deleting directory /tmp/spark-a2254311-8b5d-454a-b85a-3f290e9d6e39/userFiles-ab35a5b7-1bc5-484d-bc98-253d8b63b839
15/08/18 03:44:41 INFO Utils: Deleting directory /tmp/spark-a2254311-8b5d-454a-b85a-3f290e9d6e39
15/08/18 03:44:41 INFO Utils: Deleting directory /tmp/spark-3a605a69-f435-4296-8202-9d48db54e01b
15/08/18 03:44:41 INFO Utils: Deleting directory /tmp/spark-8f70af0a-2b95-4dde-95f4-99ce62a591d5
